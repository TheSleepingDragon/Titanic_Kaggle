{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4348df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b9c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
      "       'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "PassengerId                                                                \n",
       "1                   0       3    male  22.0      1      0   7.2500        S\n",
       "2                   1       1  female  38.0      1      0  71.2833        C\n",
       "3                   1       3  female  26.0      0      0   7.9250        S\n",
       "4                   1       1  female  35.0      1      0  53.1000        S\n",
       "5                   0       3    male  35.0      0      0   8.0500        S\n",
       "...               ...     ...     ...   ...    ...    ...      ...      ...\n",
       "887                 0       2    male  27.0      0      0  13.0000        S\n",
       "888                 1       1  female  19.0      0      0  30.0000        S\n",
       "889                 0       3  female   NaN      1      2  23.4500        S\n",
       "890                 1       1    male  26.0      0      0  30.0000        C\n",
       "891                 0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "print(df.columns)\n",
    "df = df.drop([\"Cabin\",\"Name\",\"Ticket\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a2060a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare Embarked  Sex_M\n",
       "PassengerId                                                               \n",
       "1                   0       3  22.0      1      0   7.2500        S      1\n",
       "2                   1       1  38.0      1      0  71.2833        C      0\n",
       "3                   1       3  26.0      0      0   7.9250        S      0\n",
       "4                   1       1  35.0      1      0  53.1000        S      0\n",
       "5                   0       3  35.0      0      0   8.0500        S      1\n",
       "...               ...     ...   ...    ...    ...      ...      ...    ...\n",
       "887                 0       2  27.0      0      0  13.0000        S      1\n",
       "888                 1       1  19.0      0      0  30.0000        S      0\n",
       "889                 0       3   NaN      1      2  23.4500        S      0\n",
       "890                 1       1  26.0      0      0  30.0000        C      1\n",
       "891                 0       3  32.0      0      0   7.7500        Q      1\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[\"Sex\"]\n",
    "df_dummy = pd.get_dummies(df1,drop_first=True)\n",
    "df[\"Sex_M\"] = df_dummy #Male is one\n",
    "df = df.drop(columns=[\"Sex\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610af723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      2\n",
       "Sex_M         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d424dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare  Embarked  Sex_M\n",
       "PassengerId                                                                \n",
       "1                   0       3  22.0      1      0   7.2500         2      1\n",
       "2                   1       1  38.0      1      0  71.2833         0      0\n",
       "3                   1       3  26.0      0      0   7.9250         2      0\n",
       "4                   1       1  35.0      1      0  53.1000         2      0\n",
       "5                   0       3  35.0      0      0   8.0500         2      1\n",
       "6                   0       3   NaN      0      0   8.4583         1      1\n",
       "7                   0       1  54.0      0      0  51.8625         2      1\n",
       "8                   0       3   2.0      3      1  21.0750         2      1\n",
       "9                   1       3  27.0      0      2  11.1333         2      0\n",
       "10                  1       2  14.0      1      0  30.0708         0      0\n",
       "11                  1       3   4.0      1      1  16.7000         2      0\n",
       "12                  1       1  58.0      0      0  26.5500         2      0\n",
       "13                  0       3  20.0      0      0   8.0500         2      1\n",
       "14                  0       3  39.0      1      5  31.2750         2      1\n",
       "15                  0       3  14.0      0      0   7.8542         2      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = df.apply(lambda col: LabelEncoder().fit_transform(df[\"Embarked\"]) if col.dtype == \"object\" else col)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c965c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      0\n",
       "Sex_M         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337f683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 177\n",
      "Missing: 0\n",
      "Mean Accuracy: 1.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation transform for the horse colic dataset\n",
    "from numpy import isnan, mean, std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "# load dataset\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 7]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "#define modeling pipeline\n",
    "## model = svm.SVC(kernel='rbf', degree =3, gamma = 0.01, C = 100, max_iter=-1, random_state = 262)\n",
    "model = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='gbtree', eta=0.3, max_depth=4, min_child_weight=20,\n",
    "                    max_delta_step=0.5, subsample=0.6, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"approx\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=0, n_estimators=10, rate_drop=\"0.01\", skip_drop=\"0.8\",\n",
    "                    random_state = 262)\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "df = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(df).flatten()))\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN imputer\n",
    "from numpy import isnan\n",
    "from sklearn.impute import KNNImputer\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 7]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "df = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(df).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5f04ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.388746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass        Age  SibSp  Parch     Fare  Embarked  Sex_M\n",
       "0         0.0     3.0  22.000000    1.0    0.0   7.2500       2.0    1.0\n",
       "1         1.0     1.0  38.000000    1.0    0.0  71.2833       0.0    0.0\n",
       "2         1.0     3.0  26.000000    0.0    0.0   7.9250       2.0    0.0\n",
       "3         1.0     1.0  35.000000    1.0    0.0  53.1000       2.0    0.0\n",
       "4         0.0     3.0  35.000000    0.0    0.0   8.0500       2.0    1.0\n",
       "..        ...     ...        ...    ...    ...      ...       ...    ...\n",
       "886       0.0     2.0  27.000000    0.0    0.0  13.0000       2.0    1.0\n",
       "887       1.0     1.0  19.000000    0.0    0.0  30.0000       2.0    0.0\n",
       "888       0.0     3.0  23.388746    1.0    2.0  23.4500       2.0    0.0\n",
       "889       1.0     1.0  26.000000    0.0    0.0  30.0000       0.0    1.0\n",
       "890       0.0     3.0  32.000000    0.0    0.0   7.7500       1.0    1.0\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = [\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Sex_M\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74522df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Pclass      0\n",
       "Age         0\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Embarked    0\n",
       "Sex_M       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d5199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.388746</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age     Fare  Embarked  Sex_M\n",
       "0       3.0  22.000000   7.2500       2.0    1.0\n",
       "1       1.0  38.000000  71.2833       0.0    0.0\n",
       "2       3.0  26.000000   7.9250       2.0    0.0\n",
       "3       1.0  35.000000  53.1000       2.0    0.0\n",
       "4       3.0  35.000000   8.0500       2.0    1.0\n",
       "..      ...        ...      ...       ...    ...\n",
       "886     2.0  27.000000  13.0000       2.0    1.0\n",
       "887     1.0  19.000000  30.0000       2.0    0.0\n",
       "888     3.0  23.388746  23.4500       2.0    0.0\n",
       "889     1.0  26.000000  30.0000       0.0    1.0\n",
       "890     3.0  32.000000   7.7500       1.0    1.0\n",
       "\n",
       "[891 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[\"Pclass\",\"Age\",\"Fare\",\"Embarked\",\"Sex_M\"]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed7d2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "0         0.0\n",
       "1         1.0\n",
       "2         1.0\n",
       "3         1.0\n",
       "4         0.0\n",
       "..        ...\n",
       "886       0.0\n",
       "887       1.0\n",
       "888       0.0\n",
       "889       1.0\n",
       "890       0.0\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[[\"Survived\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6073d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.9000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>66.6000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>79.6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age     Fare  Embarked  Sex_M\n",
       "259     2.0  50.0  26.0000       2.0    0.0\n",
       "123     2.0  32.5  13.0000       2.0    0.0\n",
       "452     1.0  30.0  27.7500       0.0    1.0\n",
       "642     3.0   2.0  27.9000       2.0    0.0\n",
       "336     1.0  29.0  66.6000       2.0    1.0\n",
       "..      ...   ...      ...       ...    ...\n",
       "339     1.0  45.0  35.5000       2.0    1.0\n",
       "591     1.0  52.0  78.2667       0.0    0.0\n",
       "558     1.0  39.0  79.6500       2.0    0.0\n",
       "854     2.0  44.0  26.0000       2.0    0.0\n",
       "717     2.0  27.0  10.5000       2.0    0.0\n",
       "\n",
       "[757 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split dataset into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=262)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668dc0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 0.51575524, 0.03093714, 1.33333333, 0.        ],\n",
       "       [2.        , 0.4921958 , 0.0359796 , 1.33333333, 2.        ],\n",
       "       [2.        , 0.72779021, 0.05642544, 0.        , 2.        ],\n",
       "       [2.        , 0.78538307, 0.06050797, 0.66666667, 2.        ],\n",
       "       [2.        , 0.        , 0.27150512, 1.33333333, 2.        ],\n",
       "       [1.        , 0.65711189, 0.04098927, 1.33333333, 2.        ],\n",
       "       [2.        , 0.68067133, 0.06285021, 1.33333333, 2.        ],\n",
       "       [0.        , 1.29321678, 0.11208614, 0.        , 0.        ],\n",
       "       [2.        , 0.65711189, 0.036012  , 1.33333333, 2.        ],\n",
       "       [2.        , 0.80492188, 0.02947324, 1.33333333, 2.        ],\n",
       "       [1.        , 1.1283007 , 0.10247318, 1.33333333, 2.        ],\n",
       "       [2.        , 0.20948252, 0.12208947, 1.33333333, 2.        ],\n",
       "       [2.        , 0.79846853, 0.03074195, 1.33333333, 2.        ],\n",
       "       [0.        , 1.10474126, 0.20299448, 1.33333333, 2.        ],\n",
       "       [2.        , 0.56287413, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 0.89270629, 0.08012426, 1.33333333, 2.        ],\n",
       "       [2.        , 0.78739991, 0.03020519, 0.66666667, 2.        ],\n",
       "       [2.        , 0.        , 0.27150512, 1.33333333, 2.        ],\n",
       "       [2.        , 0.16236364, 0.04084288, 1.33333333, 0.        ],\n",
       "       [1.        , 1.29321678, 0.05074862, 1.33333333, 2.        ],\n",
       "       [1.        , 0.53931469, 0.05074862, 1.33333333, 2.        ],\n",
       "       [0.        , 1.16751688, 0.10120446, 1.33333333, 2.        ],\n",
       "       [1.        , 1.38745455, 0.10149724, 1.33333333, 2.        ],\n",
       "       [1.        , 0.91626573, 0.04098927, 1.33333333, 0.        ],\n",
       "       [0.        , 0.93982517, 0.20299448, 1.33333333, 0.        ],\n",
       "       [0.        , 0.93982517, 0.3258647 , 1.33333333, 0.        ],\n",
       "       [1.        , 0.42151748, 0.07612293, 1.33333333, 0.        ],\n",
       "       [0.        , 1.19897902, 0.30917621, 0.        , 2.        ],\n",
       "       [0.        , 0.98694406, 0.20728859, 1.33333333, 2.        ],\n",
       "       [1.        , 0.68067133, 0.04098927, 1.33333333, 2.        ],\n",
       "       [2.        , 0.79846853, 0.03035158, 1.33333333, 2.        ],\n",
       "       [2.        , 0.82202797, 0.03708553, 1.33333333, 2.        ],\n",
       "       [2.        , 0.53931469, 0.03035158, 1.33333333, 0.        ],\n",
       "       [2.        , 1.31677622, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 0.77005872, 0.02822092, 0.        , 2.        ],\n",
       "       [2.        , 0.59821329, 0.02830212, 1.33333333, 2.        ],\n",
       "       [1.        , 1.03406294, 0.05074862, 1.33333333, 2.        ],\n",
       "       [1.        , 1.57593007, 0.04098927, 1.33333333, 2.        ],\n",
       "       [1.        , 1.50525175, 0.05270049, 1.33333333, 2.        ],\n",
       "       [2.        , 0.80479197, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 0.64576153, 0.22054492, 1.33333333, 2.        ],\n",
       "       [2.        , 0.77005981, 0.02820452, 0.        , 2.        ],\n",
       "       [2.        , 0.4921958 , 0.15492968, 1.33333333, 2.        ],\n",
       "       [2.        , 0.7093109 , 0.06285021, 1.33333333, 2.        ],\n",
       "       [2.        , 0.69199354, 0.06050797, 0.66666667, 2.        ],\n",
       "       [2.        , 0.53931469, 0.03649997, 1.33333333, 0.        ],\n",
       "       [2.        , 0.78739666, 0.03025399, 0.66666667, 2.        ],\n",
       "       [0.        , 1.05762238, 0.5990777 , 1.33333333, 0.        ],\n",
       "       [2.        , 0.72779021, 0.06285021, 1.33333333, 0.        ],\n",
       "       [2.        , 0.68067133, 0.02926165, 1.33333333, 2.        ],\n",
       "       [2.        , 0.84558741, 0.03035158, 1.33333333, 2.        ],\n",
       "       [1.        , 0.91626573, 0.08197854, 1.33333333, 2.        ],\n",
       "       [2.        , 0.2566014 , 0.04869916, 1.33333333, 2.        ],\n",
       "       [2.        , 1.17541958, 0.10891435, 1.33333333, 0.        ],\n",
       "       [0.        , 1.22253846, 0.15029399, 1.33333333, 2.        ],\n",
       "       [0.        , 1.1674974 , 0.10149724, 1.33333333, 2.        ],\n",
       "       [0.        , 0.96338462, 0.15663757, 0.        , 2.        ],\n",
       "       [1.        , 0.63355245, 0.16231439, 0.        , 0.        ],\n",
       "       [1.        , 0.30372028, 0.14346245, 1.33333333, 2.        ],\n",
       "       [2.        , 0.63355245, 0.02830212, 1.33333333, 2.        ],\n",
       "       [2.        , 1.17541958, 0.05642544, 0.        , 0.        ],\n",
       "       [2.        , 0.45071325, 0.09076196, 0.66666667, 0.        ],\n",
       "       [2.        , 0.44507692, 0.03066075, 1.33333333, 0.        ],\n",
       "       [2.        , 1.0694021 , 0.03025399, 0.66666667, 2.        ],\n",
       "       [2.        , 0.1388042 , 0.08031945, 1.33333333, 2.        ],\n",
       "       [2.        , 0.80479197, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 0.80483203, 0.03082315, 1.33333333, 2.        ],\n",
       "       [1.        , 0.86914685, 0.10149724, 1.33333333, 2.        ],\n",
       "       [0.        , 1.78796503, 0.13528099, 0.        , 2.        ],\n",
       "       [0.        , 0.82202797, 0.2222368 , 0.        , 0.        ],\n",
       "       [2.        , 0.86914685, 0.22054492, 1.33333333, 2.        ],\n",
       "       [0.        , 1.16126178, 0.195187  , 1.33333333, 2.        ],\n",
       "       [2.        , 0.77005872, 0.02822092, 0.        , 2.        ],\n",
       "       [0.        , 1.55237063, 0.126172  , 1.33333333, 2.        ],\n",
       "       [2.        , 0.32727972, 0.13419106, 1.33333333, 0.        ],\n",
       "       [2.        , 0.72779021, 0.03082315, 1.33333333, 2.        ],\n",
       "       [1.        , 0.75134965, 0.0540992 , 0.        , 0.        ],\n",
       "       [1.        , 0.18592308, 0.16231439, 0.        , 0.        ],\n",
       "       [2.        , 0.84558741, 0.07026732, 1.33333333, 0.        ],\n",
       "       [2.        , 0.60999301, 0.03292141, 1.33333333, 2.        ],\n",
       "       [0.        , 1.0105035 , 0.3122992 , 2.        , 0.        ],\n",
       "       [2.        , 0.91626573, 0.03142511, 1.33333333, 2.        ],\n",
       "       [0.        , 1.10474126, 0.88819845, 0.        , 0.        ],\n",
       "       [2.        , 0.89270629, 0.03035158, 1.33333333, 2.        ],\n",
       "       [0.        , 1.48169231, 0.11594108, 0.        , 2.        ],\n",
       "       [0.        , 1.1283007 , 0.82500666, 1.33333333, 0.        ],\n",
       "       [2.        , 0.53931469, 0.07890435, 1.33333333, 2.        ],\n",
       "       [1.        , 0.77490909, 0.05074862, 1.33333333, 2.        ],\n",
       "       [2.        , 0.60999301, 0.03093714, 1.33333333, 2.        ],\n",
       "       [2.        , 1.05762238, 0.03082315, 1.33333333, 2.        ],\n",
       "       [2.        , 0.80479197, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 1.03406294, 0.09427532, 1.33333333, 2.        ],\n",
       "       [2.        , 0.86914685, 0.03264503, 1.33333333, 2.        ],\n",
       "       [1.        , 0.68067133, 0.05660423, 1.33333333, 0.        ],\n",
       "       [0.        , 1.22253846, 0.20515793, 1.33333333, 0.        ],\n",
       "       [1.        , 0.56287413, 0.10149724, 1.33333333, 0.        ],\n",
       "       [2.        , 1.0105035 , 0.12252864, 1.33333333, 0.        ],\n",
       "       [1.        , 0.91626573, 0.05074862, 1.33333333, 2.        ],\n",
       "       [2.        , 0.53931469, 0.05642544, 0.        , 0.        ],\n",
       "       [2.        , 0.64151949, 0.03025399, 0.66666667, 0.        ],\n",
       "       [2.        , 0.66889161, 0.02822092, 0.        , 2.        ],\n",
       "       [2.        , 0.89270629, 0.06187428, 1.33333333, 0.        ],\n",
       "       [1.        , 1.34033566, 0.05074862, 1.33333333, 2.        ],\n",
       "       [2.        , 0.78737608, 0.03056316, 0.66666667, 2.        ],\n",
       "       [2.        , 0.20948252, 0.06519246, 1.33333333, 0.        ],\n",
       "       [0.        , 1.10474126, 0.10261957, 1.33333333, 2.        ],\n",
       "       [2.        , 0.76818047, 0.05644144, 0.        , 2.        ],\n",
       "       [2.        , 0.67535714, 0.05644144, 0.        , 0.        ],\n",
       "       [2.        , 0.77490909, 0.03093714, 1.33333333, 2.        ],\n",
       "       [1.        , 0.53931469, 0.04489301, 1.33333333, 2.        ],\n",
       "       [2.        , 0.63355245, 0.03025399, 1.33333333, 0.        ],\n",
       "       [2.        , 0.58643357, 0.03093714, 1.33333333, 2.        ],\n",
       "       [2.        , 0.93982517, 0.07905074, 1.33333333, 0.        ],\n",
       "       [0.        , 0.77490909, 0.32077344, 0.        , 2.        ],\n",
       "       [2.        , 0.60999301, 0.03142511, 1.33333333, 2.        ],\n",
       "       [2.        , 0.64151949, 0.03025399, 0.66666667, 0.        ],\n",
       "       [1.        , 0.82202797, 0.04098927, 1.33333333, 2.        ],\n",
       "       [2.        , 0.49981595, 0.09941538, 1.33333333, 0.        ],\n",
       "       [2.        , 0.46863636, 0.02820452, 0.        , 0.        ],\n",
       "       [1.        , 0.51575524, 0.04684488, 0.        , 0.        ],\n",
       "       [1.        , 1.29321678, 0.04098927, 1.33333333, 0.        ],\n",
       "       [2.        , 0.82202797, 0.03142511, 1.33333333, 2.        ],\n",
       "       [1.        , 0.75134965, 0.08197854, 1.33333333, 0.        ],\n",
       "       [2.        , 0.51575524, 0.03381615, 1.33333333, 2.        ],\n",
       "       [0.        , 0.82202797, 0.12101594, 0.        , 0.        ],\n",
       "       [0.        , 1.0105035 , 0.5990777 , 1.33333333, 2.        ],\n",
       "       [2.        , 0.80483203, 0.03082315, 1.33333333, 2.        ],\n",
       "       [1.        , 0.77490909, 0.04098927, 1.33333333, 2.        ],\n",
       "       [2.        , 0.51575524, 0.02753776, 1.33333333, 2.        ],\n",
       "       [0.        , 1.03406294, 0.        , 1.33333333, 2.        ],\n",
       "       [1.        , 0.79846853, 0.08197854, 1.33333333, 2.        ],\n",
       "       [2.        , 0.54386895, 0.09427532, 0.66666667, 0.        ],\n",
       "       [2.        , 0.56287413, 0.03082315, 1.33333333, 2.        ],\n",
       "       [1.        , 0.86914685, 0.05074862, 1.33333333, 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "mmsc = MinMaxScaler(feature_range=(0, 2))\n",
    "sc = StandardScaler()\n",
    "X_train = mmsc.fit_transform(X_train)\n",
    "X_test = mmsc.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1776f27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "98d09fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.6870 - accuracy: 0.6618\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.6728 - accuracy: 0.6975\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.6626 - accuracy: 0.7041\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.6485 - accuracy: 0.7094\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.6308 - accuracy: 0.7437\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.6109 - accuracy: 0.78070s - loss: 0.6142 - accuracy: 0.77\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.5913 - accuracy: 0.7701\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.5709 - accuracy: 0.78340s - loss: 0.5676 - accuracy: 0.78\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.5533 - accuracy: 0.7701\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - 0s 957us/step - loss: 0.5307 - accuracy: 0.7847\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7794\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - 0s 904us/step - loss: 0.5051 - accuracy: 0.7715\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - 0s 691us/step - loss: 0.5008 - accuracy: 0.7886\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - 0s 665us/step - loss: 0.4847 - accuracy: 0.7926\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - 0s 665us/step - loss: 0.4829 - accuracy: 0.7966\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - 0s 691us/step - loss: 0.4892 - accuracy: 0.7847\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - 0s 665us/step - loss: 0.4700 - accuracy: 0.7860\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - 0s 721us/step - loss: 0.4701 - accuracy: 0.7873\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4767 - accuracy: 0.7913\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - 0s 665us/step - loss: 0.4745 - accuracy: 0.7913\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - 0s 691us/step - loss: 0.4622 - accuracy: 0.7952\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4645 - accuracy: 0.8032\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.4697 - accuracy: 0.7966\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4639 - accuracy: 0.7939\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.4601 - accuracy: 0.7979\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4539 - accuracy: 0.8071\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4585 - accuracy: 0.7939\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4581 - accuracy: 0.8018\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4670 - accuracy: 0.8018\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4666 - accuracy: 0.7939\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4573 - accuracy: 0.8005\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4630 - accuracy: 0.7979\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4568 - accuracy: 0.7992\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4546 - accuracy: 0.7992\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.4502 - accuracy: 0.8032\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4597 - accuracy: 0.8111\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4577 - accuracy: 0.7979\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4567 - accuracy: 0.8085\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4507 - accuracy: 0.80850s - loss: 0.4509 - accuracy: 0.80\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4437 - accuracy: 0.8098\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.4497 - accuracy: 0.8045\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4470 - accuracy: 0.8071\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4461 - accuracy: 0.8018\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4487 - accuracy: 0.8071\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - 0s 770us/step - loss: 0.4446 - accuracy: 0.7966\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4553 - accuracy: 0.8071\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.4488 - accuracy: 0.8058\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4549 - accuracy: 0.8085\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4464 - accuracy: 0.8111\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4574 - accuracy: 0.80180s - loss: 0.4537 - accuracy: 0.80\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4521 - accuracy: 0.8071\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4546 - accuracy: 0.8032\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4466 - accuracy: 0.8032\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4499 - accuracy: 0.8005\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4553 - accuracy: 0.7979\n",
      "Epoch 56/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4564 - accuracy: 0.8005\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4500 - accuracy: 0.8032\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - 0s 718us/step - loss: 0.4459 - accuracy: 0.8058\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4468 - accuracy: 0.8032\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4583 - accuracy: 0.8005\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4512 - accuracy: 0.8058\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4481 - accuracy: 0.8098\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - 0s 739us/step - loss: 0.4555 - accuracy: 0.8018\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4536 - accuracy: 0.8098\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4511 - accuracy: 0.8071\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4525 - accuracy: 0.8071\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4484 - accuracy: 0.8032\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4443 - accuracy: 0.8071\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4547 - accuracy: 0.8005\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - 0s 705us/step - loss: 0.4485 - accuracy: 0.7926\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - 0s 718us/step - loss: 0.4459 - accuracy: 0.8045\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4476 - accuracy: 0.8045\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4455 - accuracy: 0.8085\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4495 - accuracy: 0.7979\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4469 - accuracy: 0.8032\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - 0s 835us/step - loss: 0.4390 - accuracy: 0.8098\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4471 - accuracy: 0.8045\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4508 - accuracy: 0.8018\n",
      "Epoch 79/200\n",
      "76/76 [==============================] - 0s 691us/step - loss: 0.4380 - accuracy: 0.8071\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 785us/step - loss: 0.4590 - accuracy: 0.7966\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4476 - accuracy: 0.8058\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4413 - accuracy: 0.8058\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4465 - accuracy: 0.8018\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4471 - accuracy: 0.7979\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - 0s 788us/step - loss: 0.4430 - accuracy: 0.8071\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - 0s 797us/step - loss: 0.4527 - accuracy: 0.8071\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4530 - accuracy: 0.8098\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4450 - accuracy: 0.80710s - loss: 0.4360 - accuracy: 0.81\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.78 - 0s 865us/step - loss: 0.4443 - accuracy: 0.7992\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4519 - accuracy: 0.8045\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4478 - accuracy: 0.8045\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4477 - accuracy: 0.8045\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4465 - accuracy: 0.8018\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4490 - accuracy: 0.8032\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4493 - accuracy: 0.8045\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4462 - accuracy: 0.80580s - loss: 0.4347 - accuracy: 0.81\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4482 - accuracy: 0.8111\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4503 - accuracy: 0.8032\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4414 - accuracy: 0.8085\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4491 - accuracy: 0.8032\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - 0s 757us/step - loss: 0.4503 - accuracy: 0.8032\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - 0s 792us/step - loss: 0.4526 - accuracy: 0.8045\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4535 - accuracy: 0.8018\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4466 - accuracy: 0.8058\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4438 - accuracy: 0.8085\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4509 - accuracy: 0.8071\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4450 - accuracy: 0.8018\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - 0s 796us/step - loss: 0.4463 - accuracy: 0.8032\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - 0s 833us/step - loss: 0.4414 - accuracy: 0.8124\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4455 - accuracy: 0.8111\n",
      "Epoch 111/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4479 - accuracy: 0.8045\n",
      "Epoch 112/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4502 - accuracy: 0.8071\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4459 - accuracy: 0.8071\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4512 - accuracy: 0.8018\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4425 - accuracy: 0.8032\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4486 - accuracy: 0.8005\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4465 - accuracy: 0.8085\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4486 - accuracy: 0.8045\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4466 - accuracy: 0.8085\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4376 - accuracy: 0.8124\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4382 - accuracy: 0.8085\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4468 - accuracy: 0.8018\n",
      "Epoch 123/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4431 - accuracy: 0.8085\n",
      "Epoch 124/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4365 - accuracy: 0.8098\n",
      "Epoch 125/200\n",
      "76/76 [==============================] - 0s 877us/step - loss: 0.4585 - accuracy: 0.8005\n",
      "Epoch 126/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4432 - accuracy: 0.8032\n",
      "Epoch 127/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4372 - accuracy: 0.8005\n",
      "Epoch 128/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.81 - 0s 785us/step - loss: 0.4445 - accuracy: 0.8045\n",
      "Epoch 129/200\n",
      "76/76 [==============================] - 0s 793us/step - loss: 0.4405 - accuracy: 0.8058\n",
      "Epoch 130/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4428 - accuracy: 0.8071\n",
      "Epoch 131/200\n",
      "76/76 [==============================] - 0s 718us/step - loss: 0.4434 - accuracy: 0.80580s - loss: 0.4484 - accuracy: 0.80\n",
      "Epoch 132/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4411 - accuracy: 0.8045\n",
      "Epoch 133/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4485 - accuracy: 0.8045\n",
      "Epoch 134/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4498 - accuracy: 0.8045\n",
      "Epoch 135/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4437 - accuracy: 0.8058\n",
      "Epoch 136/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4386 - accuracy: 0.8098\n",
      "Epoch 137/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4428 - accuracy: 0.8018\n",
      "Epoch 138/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4416 - accuracy: 0.8071\n",
      "Epoch 139/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4369 - accuracy: 0.8085\n",
      "Epoch 140/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4430 - accuracy: 0.8071\n",
      "Epoch 141/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.4459 - accuracy: 0.8164\n",
      "Epoch 142/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4438 - accuracy: 0.8111\n",
      "Epoch 143/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4416 - accuracy: 0.8045\n",
      "Epoch 144/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4451 - accuracy: 0.8005\n",
      "Epoch 145/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4467 - accuracy: 0.8085\n",
      "Epoch 146/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4490 - accuracy: 0.8085\n",
      "Epoch 147/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4496 - accuracy: 0.8005\n",
      "Epoch 148/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4453 - accuracy: 0.8032\n",
      "Epoch 149/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4466 - accuracy: 0.8005\n",
      "Epoch 150/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4397 - accuracy: 0.8018\n",
      "Epoch 151/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.4419 - accuracy: 0.80710s - loss: 0.4590 - accuracy: 0.79\n",
      "Epoch 152/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4432 - accuracy: 0.8045\n",
      "Epoch 153/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4476 - accuracy: 0.8058\n",
      "Epoch 154/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4471 - accuracy: 0.8045\n",
      "Epoch 155/200\n",
      "76/76 [==============================] - 0s 872us/step - loss: 0.4422 - accuracy: 0.8124\n",
      "Epoch 156/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4474 - accuracy: 0.8058\n",
      "Epoch 157/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4440 - accuracy: 0.8098\n",
      "Epoch 158/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4362 - accuracy: 0.8085\n",
      "Epoch 159/200\n",
      "76/76 [==============================] - 0s 757us/step - loss: 0.4423 - accuracy: 0.8071\n",
      "Epoch 160/200\n",
      "76/76 [==============================] - 0s 743us/step - loss: 0.4442 - accuracy: 0.8085\n",
      "Epoch 161/200\n",
      "76/76 [==============================] - 0s 732us/step - loss: 0.4469 - accuracy: 0.8018\n",
      "Epoch 162/200\n",
      "76/76 [==============================] - 0s 705us/step - loss: 0.4345 - accuracy: 0.8177\n",
      "Epoch 163/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.80 - 0s 838us/step - loss: 0.4449 - accuracy: 0.8085\n",
      "Epoch 164/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4436 - accuracy: 0.8045\n",
      "Epoch 165/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4396 - accuracy: 0.8071\n",
      "Epoch 166/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4452 - accuracy: 0.8018\n",
      "Epoch 167/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4404 - accuracy: 0.8045\n",
      "Epoch 168/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4380 - accuracy: 0.8058\n",
      "Epoch 169/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4457 - accuracy: 0.8005\n",
      "Epoch 170/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4428 - accuracy: 0.8164\n",
      "Epoch 171/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.4412 - accuracy: 0.8005\n",
      "Epoch 172/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4352 - accuracy: 0.8124\n",
      "Epoch 173/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4504 - accuracy: 0.7992\n",
      "Epoch 174/200\n",
      "76/76 [==============================] - 0s 718us/step - loss: 0.4444 - accuracy: 0.8005\n",
      "Epoch 175/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4435 - accuracy: 0.8098\n",
      "Epoch 176/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4445 - accuracy: 0.8032\n",
      "Epoch 177/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4408 - accuracy: 0.8151\n",
      "Epoch 178/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4375 - accuracy: 0.8085\n",
      "Epoch 179/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4402 - accuracy: 0.8098\n",
      "Epoch 180/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4463 - accuracy: 0.8045\n",
      "Epoch 181/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4444 - accuracy: 0.8058\n",
      "Epoch 182/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4348 - accuracy: 0.8151\n",
      "Epoch 183/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4416 - accuracy: 0.8098\n",
      "Epoch 184/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4407 - accuracy: 0.8045\n",
      "Epoch 185/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4403 - accuracy: 0.8058\n",
      "Epoch 186/200\n",
      "76/76 [==============================] - 0s 797us/step - loss: 0.4518 - accuracy: 0.8045\n",
      "Epoch 187/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4385 - accuracy: 0.8085\n",
      "Epoch 188/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4453 - accuracy: 0.8137\n",
      "Epoch 189/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4380 - accuracy: 0.80320s - loss: 0.4313 - accuracy: 0.80\n",
      "Epoch 190/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4394 - accuracy: 0.8071\n",
      "Epoch 191/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4455 - accuracy: 0.8032\n",
      "Epoch 192/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4415 - accuracy: 0.8085\n",
      "Epoch 193/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4415 - accuracy: 0.8032\n",
      "Epoch 194/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4404 - accuracy: 0.8071\n",
      "Epoch 195/200\n",
      "76/76 [==============================] - 0s 760us/step - loss: 0.4427 - accuracy: 0.8111\n",
      "Epoch 196/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4455 - accuracy: 0.8111\n",
      "Epoch 197/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4351 - accuracy: 0.8098\n",
      "Epoch 198/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4471 - accuracy: 0.8005\n",
      "Epoch 199/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4440 - accuracy: 0.8058\n",
      "Epoch 200/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4427 - accuracy: 0.8032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22166edce20>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "### Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "### Adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(Dropout(0.2))\n",
    "ann.add(tf.keras.layers.Dense(units=7, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "### Compiling the ANN\n",
    "ann.compile(optimizer = 'SGD', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "### Training the ANN on the Training set\n",
    "ann.fit(X_train, y_train, batch_size = 10, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "04e5c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691440565198004\n",
      "0.6113063781178213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "y_pred_train = np.round(ann.predict(X_train),0)\n",
    "y_pred_test = np.round(ann.predict(X_test),0)\n",
    "precision_test = average_precision_score(y_test, y_pred_test)\n",
    "precision_train = average_precision_score(y_train, y_pred_train)\n",
    "print(precision_train)\n",
    "print(precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f6834fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7849940828402366\n",
      "0.7873015873015872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(f1_train)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f327dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "95/95 [==============================] - 1s 944us/step - loss: 0.7170 - accuracy: 0.6869\n",
      "Epoch 2/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6906 - accuracy: 0.7358\n",
      "Epoch 3/250\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.78 - 0s 753us/step - loss: 0.6771 - accuracy: 0.7807\n",
      "Epoch 4/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6698 - accuracy: 0.7754\n",
      "Epoch 5/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6648 - accuracy: 0.7847\n",
      "Epoch 6/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6630 - accuracy: 0.7701\n",
      "Epoch 7/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6607 - accuracy: 0.7728\n",
      "Epoch 8/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6602 - accuracy: 0.7847\n",
      "Epoch 9/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6593 - accuracy: 0.7860\n",
      "Epoch 10/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6590 - accuracy: 0.7768\n",
      "Epoch 11/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6580 - accuracy: 0.7979\n",
      "Epoch 12/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6561 - accuracy: 0.79000s - loss: 0.6451 - accuracy: 0.77\n",
      "Epoch 13/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6578 - accuracy: 0.7834\n",
      "Epoch 14/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6580 - accuracy: 0.7794\n",
      "Epoch 15/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6574 - accuracy: 0.7873\n",
      "Epoch 16/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6581 - accuracy: 0.7834\n",
      "Epoch 17/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6575 - accuracy: 0.7728\n",
      "Epoch 18/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6562 - accuracy: 0.7900\n",
      "Epoch 19/250\n",
      "95/95 [==============================] - 0s 785us/step - loss: 0.6579 - accuracy: 0.7807\n",
      "Epoch 20/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6560 - accuracy: 0.7820\n",
      "Epoch 21/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6568 - accuracy: 0.7768\n",
      "Epoch 22/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6576 - accuracy: 0.7847\n",
      "Epoch 23/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6560 - accuracy: 0.7675\n",
      "Epoch 24/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6569 - accuracy: 0.7860\n",
      "Epoch 25/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6567 - accuracy: 0.7768\n",
      "Epoch 26/250\n",
      "95/95 [==============================] - 0s 722us/step - loss: 0.6549 - accuracy: 0.7754\n",
      "Epoch 27/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6560 - accuracy: 0.7807\n",
      "Epoch 28/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6567 - accuracy: 0.7939\n",
      "Epoch 29/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6568 - accuracy: 0.7926\n",
      "Epoch 30/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6562 - accuracy: 0.7834\n",
      "Epoch 31/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6556 - accuracy: 0.7900\n",
      "Epoch 32/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6563 - accuracy: 0.7886\n",
      "Epoch 33/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6558 - accuracy: 0.7900\n",
      "Epoch 34/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6562 - accuracy: 0.7794\n",
      "Epoch 35/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6548 - accuracy: 0.7913\n",
      "Epoch 36/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6565 - accuracy: 0.7939\n",
      "Epoch 37/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6558 - accuracy: 0.7781\n",
      "Epoch 38/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6552 - accuracy: 0.8018\n",
      "Epoch 39/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6557 - accuracy: 0.7913\n",
      "Epoch 40/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6534 - accuracy: 0.7807\n",
      "Epoch 41/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6557 - accuracy: 0.7966\n",
      "Epoch 42/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6557 - accuracy: 0.7992\n",
      "Epoch 43/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6540 - accuracy: 0.7913\n",
      "Epoch 44/250\n",
      "95/95 [==============================] - 0s 842us/step - loss: 0.6555 - accuracy: 0.8071\n",
      "Epoch 45/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6550 - accuracy: 0.7939\n",
      "Epoch 46/250\n",
      "95/95 [==============================] - 0s 833us/step - loss: 0.6552 - accuracy: 0.7966\n",
      "Epoch 47/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6525 - accuracy: 0.7966\n",
      "Epoch 48/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6551 - accuracy: 0.7900\n",
      "Epoch 49/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6551 - accuracy: 0.7979\n",
      "Epoch 50/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6542 - accuracy: 0.7847\n",
      "Epoch 51/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6551 - accuracy: 0.7992\n",
      "Epoch 52/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6546 - accuracy: 0.7992\n",
      "Epoch 53/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6552 - accuracy: 0.7926\n",
      "Epoch 54/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6541 - accuracy: 0.8058\n",
      "Epoch 55/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6556 - accuracy: 0.7847\n",
      "Epoch 56/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6550 - accuracy: 0.8005\n",
      "Epoch 57/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6547 - accuracy: 0.7939\n",
      "Epoch 58/250\n",
      "95/95 [==============================] - 0s 615us/step - loss: 0.6544 - accuracy: 0.7992\n",
      "Epoch 59/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6526 - accuracy: 0.8045\n",
      "Epoch 60/250\n",
      "95/95 [==============================] - 0s 657us/step - loss: 0.6532 - accuracy: 0.8005\n",
      "Epoch 61/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.7873\n",
      "Epoch 62/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6533 - accuracy: 0.7926\n",
      "Epoch 63/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6540 - accuracy: 0.7992\n",
      "Epoch 64/250\n",
      "95/95 [==============================] - 0s 669us/step - loss: 0.6546 - accuracy: 0.8018\n",
      "Epoch 65/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6541 - accuracy: 0.79660s - loss: 0.6547 - accuracy: 0.79\n",
      "Epoch 66/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6538 - accuracy: 0.7992\n",
      "Epoch 67/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6538 - accuracy: 0.8058\n",
      "Epoch 68/250\n",
      "95/95 [==============================] - 0s 669us/step - loss: 0.6536 - accuracy: 0.7992\n",
      "Epoch 69/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.8085\n",
      "Epoch 70/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6547 - accuracy: 0.8032\n",
      "Epoch 71/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6539 - accuracy: 0.8005\n",
      "Epoch 72/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6533 - accuracy: 0.7979\n",
      "Epoch 73/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6537 - accuracy: 0.7900\n",
      "Epoch 74/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6533 - accuracy: 0.7966\n",
      "Epoch 75/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6529 - accuracy: 0.7966\n",
      "Epoch 76/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.8071\n",
      "Epoch 77/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6531 - accuracy: 0.8071\n",
      "Epoch 78/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6544 - accuracy: 0.8085\n",
      "Epoch 79/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6530 - accuracy: 0.7952\n",
      "Epoch 80/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6527 - accuracy: 0.7992\n",
      "Epoch 81/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6538 - accuracy: 0.8058\n",
      "Epoch 82/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6537 - accuracy: 0.8018\n",
      "Epoch 83/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6524 - accuracy: 0.8032\n",
      "Epoch 84/250\n",
      "95/95 [==============================] - 0s 615us/step - loss: 0.6541 - accuracy: 0.8005\n",
      "Epoch 85/250\n",
      "95/95 [==============================] - 0s 626us/step - loss: 0.6530 - accuracy: 0.8071\n",
      "Epoch 86/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6534 - accuracy: 0.8085\n",
      "Epoch 87/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6528 - accuracy: 0.8045\n",
      "Epoch 88/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6537 - accuracy: 0.8032\n",
      "Epoch 89/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6534 - accuracy: 0.8018\n",
      "Epoch 90/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6525 - accuracy: 0.8018\n",
      "Epoch 91/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6537 - accuracy: 0.8085\n",
      "Epoch 92/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6532 - accuracy: 0.8045\n",
      "Epoch 93/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6519 - accuracy: 0.8045\n",
      "Epoch 94/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6535 - accuracy: 0.8018\n",
      "Epoch 95/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6536 - accuracy: 0.8085\n",
      "Epoch 96/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6532 - accuracy: 0.7979\n",
      "Epoch 97/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6528 - accuracy: 0.7979\n",
      "Epoch 98/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6533 - accuracy: 0.8005\n",
      "Epoch 99/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6530 - accuracy: 0.8058\n",
      "Epoch 100/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6521 - accuracy: 0.7992\n",
      "Epoch 101/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6525 - accuracy: 0.8005\n",
      "Epoch 102/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6533 - accuracy: 0.8085\n",
      "Epoch 103/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6534 - accuracy: 0.8085\n",
      "Epoch 104/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6518 - accuracy: 0.8005\n",
      "Epoch 105/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6529 - accuracy: 0.7979\n",
      "Epoch 106/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6525 - accuracy: 0.8032\n",
      "Epoch 107/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6526 - accuracy: 0.8111\n",
      "Epoch 108/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6521 - accuracy: 0.8032\n",
      "Epoch 109/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6511 - accuracy: 0.8005\n",
      "Epoch 110/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6527 - accuracy: 0.8058\n",
      "Epoch 111/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6524 - accuracy: 0.8018\n",
      "Epoch 112/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6527 - accuracy: 0.8071\n",
      "Epoch 113/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6515 - accuracy: 0.8098\n",
      "Epoch 114/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6527 - accuracy: 0.7926\n",
      "Epoch 115/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6516 - accuracy: 0.8045\n",
      "Epoch 116/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6521 - accuracy: 0.8071\n",
      "Epoch 117/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6518 - accuracy: 0.8124\n",
      "Epoch 118/250\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.82 - 0s 870us/step - loss: 0.6529 - accuracy: 0.7992\n",
      "Epoch 119/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6525 - accuracy: 0.8058\n",
      "Epoch 120/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 121/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6517 - accuracy: 0.7939\n",
      "Epoch 122/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6526 - accuracy: 0.8032\n",
      "Epoch 123/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6511 - accuracy: 0.8045\n",
      "Epoch 124/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6535 - accuracy: 0.8058\n",
      "Epoch 125/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6527 - accuracy: 0.7992\n",
      "Epoch 126/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6521 - accuracy: 0.7939\n",
      "Epoch 127/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6520 - accuracy: 0.8098\n",
      "Epoch 128/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6519 - accuracy: 0.8005\n",
      "Epoch 129/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6528 - accuracy: 0.8018\n",
      "Epoch 130/250\n",
      "95/95 [==============================] - 0s 696us/step - loss: 0.6525 - accuracy: 0.8018\n",
      "Epoch 131/250\n",
      "95/95 [==============================] - 0s 689us/step - loss: 0.6510 - accuracy: 0.8098\n",
      "Epoch 132/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6523 - accuracy: 0.8032\n",
      "Epoch 133/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 134/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6523 - accuracy: 0.8124\n",
      "Epoch 135/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6507 - accuracy: 0.8098\n",
      "Epoch 136/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 137/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6519 - accuracy: 0.8005\n",
      "Epoch 138/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6518 - accuracy: 0.8018\n",
      "Epoch 139/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6514 - accuracy: 0.8071\n",
      "Epoch 140/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 141/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6527 - accuracy: 0.8071\n",
      "Epoch 142/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6517 - accuracy: 0.8085\n",
      "Epoch 143/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6521 - accuracy: 0.8005\n",
      "Epoch 144/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 145/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6512 - accuracy: 0.8018\n",
      "Epoch 146/250\n",
      "95/95 [==============================] - 0s 795us/step - loss: 0.6516 - accuracy: 0.8045\n",
      "Epoch 147/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6516 - accuracy: 0.8071\n",
      "Epoch 148/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6500 - accuracy: 0.8098\n",
      "Epoch 149/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6502 - accuracy: 0.79920s - loss: 0.6393 - accuracy: 0.80\n",
      "Epoch 150/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6520 - accuracy: 0.8071\n",
      "Epoch 151/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6517 - accuracy: 0.8045\n",
      "Epoch 152/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6511 - accuracy: 0.8018\n",
      "Epoch 153/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6505 - accuracy: 0.8137\n",
      "Epoch 154/250\n",
      "95/95 [==============================] - 0s 742us/step - loss: 0.6505 - accuracy: 0.8085\n",
      "Epoch 155/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6522 - accuracy: 0.8032\n",
      "Epoch 156/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6515 - accuracy: 0.8032\n",
      "Epoch 157/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6514 - accuracy: 0.8045\n",
      "Epoch 158/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6504 - accuracy: 0.8098\n",
      "Epoch 159/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 668us/step - loss: 0.6501 - accuracy: 0.7992\n",
      "Epoch 160/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6526 - accuracy: 0.8005\n",
      "Epoch 161/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6515 - accuracy: 0.8018\n",
      "Epoch 162/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6512 - accuracy: 0.8018\n",
      "Epoch 163/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6516 - accuracy: 0.8058\n",
      "Epoch 164/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6513 - accuracy: 0.8018\n",
      "Epoch 165/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6513 - accuracy: 0.8032\n",
      "Epoch 166/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6508 - accuracy: 0.8032\n",
      "Epoch 167/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6498 - accuracy: 0.8045\n",
      "Epoch 168/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6516 - accuracy: 0.7992\n",
      "Epoch 169/250\n",
      "95/95 [==============================] - 0s 698us/step - loss: 0.6505 - accuracy: 0.8032\n",
      "Epoch 170/250\n",
      "95/95 [==============================] - 0s 722us/step - loss: 0.6504 - accuracy: 0.8071\n",
      "Epoch 171/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6513 - accuracy: 0.8018\n",
      "Epoch 172/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6513 - accuracy: 0.8005\n",
      "Epoch 173/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6512 - accuracy: 0.80580s - loss: 0.6557 - accuracy: 0.79\n",
      "Epoch 174/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6509 - accuracy: 0.8032\n",
      "Epoch 175/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6513 - accuracy: 0.8058\n",
      "Epoch 176/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6506 - accuracy: 0.8098\n",
      "Epoch 177/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6517 - accuracy: 0.8005\n",
      "Epoch 178/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6506 - accuracy: 0.8058\n",
      "Epoch 179/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6511 - accuracy: 0.8071\n",
      "Epoch 180/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6502 - accuracy: 0.8085\n",
      "Epoch 181/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6488 - accuracy: 0.8058\n",
      "Epoch 182/250\n",
      "95/95 [==============================] - 0s 779us/step - loss: 0.6513 - accuracy: 0.8032\n",
      "Epoch 183/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6509 - accuracy: 0.8071\n",
      "Epoch 184/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6497 - accuracy: 0.8098\n",
      "Epoch 185/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6514 - accuracy: 0.8124\n",
      "Epoch 186/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6504 - accuracy: 0.8018\n",
      "Epoch 187/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6504 - accuracy: 0.80050s - loss: 0.6485 - accuracy: 0.79\n",
      "Epoch 188/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6495 - accuracy: 0.8005\n",
      "Epoch 189/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6509 - accuracy: 0.8032\n",
      "Epoch 190/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6499 - accuracy: 0.8085\n",
      "Epoch 191/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 192/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6496 - accuracy: 0.8032\n",
      "Epoch 193/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6505 - accuracy: 0.8071\n",
      "Epoch 194/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 195/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6508 - accuracy: 0.8098\n",
      "Epoch 196/250\n",
      "95/95 [==============================] - 0s 923us/step - loss: 0.6502 - accuracy: 0.8045\n",
      "Epoch 197/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6490 - accuracy: 0.8098\n",
      "Epoch 198/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6508 - accuracy: 0.8045\n",
      "Epoch 199/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6505 - accuracy: 0.8045\n",
      "Epoch 200/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6508 - accuracy: 0.8032\n",
      "Epoch 201/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6490 - accuracy: 0.8058\n",
      "Epoch 202/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6504 - accuracy: 0.8005\n",
      "Epoch 203/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 204/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6504 - accuracy: 0.8071\n",
      "Epoch 205/250\n",
      "95/95 [==============================] - 0s 657us/step - loss: 0.6494 - accuracy: 0.8018\n",
      "Epoch 206/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6498 - accuracy: 0.7966\n",
      "Epoch 207/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6489 - accuracy: 0.8045\n",
      "Epoch 208/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6505 - accuracy: 0.8032\n",
      "Epoch 209/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6495 - accuracy: 0.8018\n",
      "Epoch 210/250\n",
      "95/95 [==============================] - 0s 674us/step - loss: 0.6497 - accuracy: 0.8058\n",
      "Epoch 211/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6505 - accuracy: 0.8045\n",
      "Epoch 212/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6501 - accuracy: 0.8058\n",
      "Epoch 213/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6504 - accuracy: 0.8058\n",
      "Epoch 214/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6492 - accuracy: 0.8005\n",
      "Epoch 215/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6495 - accuracy: 0.8071\n",
      "Epoch 216/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6498 - accuracy: 0.8071\n",
      "Epoch 217/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6500 - accuracy: 0.80710s - loss: 0.6585 - accuracy: 0.80\n",
      "Epoch 218/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6493 - accuracy: 0.8032\n",
      "Epoch 219/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6505 - accuracy: 0.8058\n",
      "Epoch 220/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6503 - accuracy: 0.8005\n",
      "Epoch 221/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6496 - accuracy: 0.8018\n",
      "Epoch 222/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6497 - accuracy: 0.8045\n",
      "Epoch 223/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6479 - accuracy: 0.8018\n",
      "Epoch 224/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 225/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6499 - accuracy: 0.8045\n",
      "Epoch 226/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 227/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6497 - accuracy: 0.8045\n",
      "Epoch 228/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6497 - accuracy: 0.8058\n",
      "Epoch 229/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6492 - accuracy: 0.8111\n",
      "Epoch 230/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6492 - accuracy: 0.8032\n",
      "Epoch 231/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6490 - accuracy: 0.8058\n",
      "Epoch 232/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6496 - accuracy: 0.8032\n",
      "Epoch 233/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6492 - accuracy: 0.8071\n",
      "Epoch 234/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6497 - accuracy: 0.8018\n",
      "Epoch 235/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6501 - accuracy: 0.8071\n",
      "Epoch 236/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6488 - accuracy: 0.8032\n",
      "Epoch 237/250\n",
      "95/95 [==============================] - 0s 785us/step - loss: 0.6496 - accuracy: 0.8045\n",
      "Epoch 238/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6492 - accuracy: 0.8111\n",
      "Epoch 239/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6496 - accuracy: 0.8085\n",
      "Epoch 240/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6499 - accuracy: 0.8058\n",
      "Epoch 241/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6494 - accuracy: 0.8058\n",
      "Epoch 242/250\n",
      "95/95 [==============================] - 0s 807us/step - loss: 0.6495 - accuracy: 0.8045\n",
      "Epoch 243/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6496 - accuracy: 0.8045\n",
      "Epoch 244/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6489 - accuracy: 0.8032\n",
      "Epoch 245/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6490 - accuracy: 0.8071\n",
      "Epoch 246/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6499 - accuracy: 0.8058\n",
      "Epoch 247/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6492 - accuracy: 0.8071\n",
      "Epoch 248/250\n",
      "95/95 [==============================] - 0s 828us/step - loss: 0.6489 - accuracy: 0.8045\n",
      "Epoch 249/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6480 - accuracy: 0.8045\n",
      "Epoch 250/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6491 - accuracy: 0.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22164058fd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initializing the ANN\n",
    "ann_p = tf.keras.models.Sequential()\n",
    "### Adding the input layer and the first hidden layer\n",
    "ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "### Compiling the ANN\n",
    "ann_p.compile(optimizer = 'RMSprop', loss = 'poisson', metrics = ['accuracy'])\n",
    "### Training the ANN on the Training set\n",
    "ann_p.fit(X_train, y_train, batch_size = 8, epochs = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e16c3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210\n",
      "379/379 - 1s - loss: 0.5406 - accuracy: 0.7345 - val_loss: 0.4708 - val_accuracy: 0.7985\n",
      "Epoch 2/210\n",
      "379/379 - 0s - loss: 0.4676 - accuracy: 0.7952 - val_loss: 0.4603 - val_accuracy: 0.7910\n",
      "Epoch 3/210\n",
      "379/379 - 0s - loss: 0.4760 - accuracy: 0.7926 - val_loss: 0.4593 - val_accuracy: 0.7910\n",
      "Epoch 4/210\n",
      "379/379 - 0s - loss: 0.4606 - accuracy: 0.8058 - val_loss: 0.4569 - val_accuracy: 0.7985\n",
      "Epoch 5/210\n",
      "379/379 - 0s - loss: 0.4639 - accuracy: 0.8058 - val_loss: 0.4719 - val_accuracy: 0.8134\n",
      "Epoch 6/210\n",
      "379/379 - 0s - loss: 0.4443 - accuracy: 0.8071 - val_loss: 0.4703 - val_accuracy: 0.8060\n",
      "Epoch 7/210\n",
      "379/379 - 0s - loss: 0.4705 - accuracy: 0.8005 - val_loss: 0.4641 - val_accuracy: 0.8209\n",
      "Epoch 8/210\n",
      "379/379 - 0s - loss: 0.4538 - accuracy: 0.8124 - val_loss: 0.4563 - val_accuracy: 0.8060\n",
      "Epoch 9/210\n",
      "379/379 - 0s - loss: 0.4638 - accuracy: 0.8111 - val_loss: 0.4494 - val_accuracy: 0.8060\n",
      "Epoch 10/210\n",
      "379/379 - 0s - loss: 0.4529 - accuracy: 0.8164 - val_loss: 0.4600 - val_accuracy: 0.8134\n",
      "Epoch 11/210\n",
      "379/379 - 0s - loss: 0.4601 - accuracy: 0.8151 - val_loss: 0.4469 - val_accuracy: 0.8060\n",
      "Epoch 12/210\n",
      "379/379 - 0s - loss: 0.4650 - accuracy: 0.7979 - val_loss: 0.4441 - val_accuracy: 0.8209\n",
      "Epoch 13/210\n",
      "379/379 - 0s - loss: 0.4542 - accuracy: 0.8045 - val_loss: 0.4503 - val_accuracy: 0.8209\n",
      "Epoch 14/210\n",
      "379/379 - 0s - loss: 0.4515 - accuracy: 0.8085 - val_loss: 0.4514 - val_accuracy: 0.8134\n",
      "Epoch 15/210\n",
      "379/379 - 0s - loss: 0.4514 - accuracy: 0.8111 - val_loss: 0.4518 - val_accuracy: 0.8209\n",
      "Epoch 16/210\n",
      "379/379 - 0s - loss: 0.4481 - accuracy: 0.8071 - val_loss: 0.4526 - val_accuracy: 0.8134\n",
      "Epoch 17/210\n",
      "379/379 - 0s - loss: 0.4555 - accuracy: 0.8085 - val_loss: 0.4618 - val_accuracy: 0.8134\n",
      "Epoch 18/210\n",
      "379/379 - 0s - loss: 0.4488 - accuracy: 0.8111 - val_loss: 0.4694 - val_accuracy: 0.8209\n",
      "Epoch 19/210\n",
      "379/379 - 0s - loss: 0.4504 - accuracy: 0.8124 - val_loss: 0.4561 - val_accuracy: 0.8209\n",
      "Epoch 20/210\n",
      "379/379 - 0s - loss: 0.4449 - accuracy: 0.8151 - val_loss: 0.4549 - val_accuracy: 0.8209\n",
      "Epoch 21/210\n",
      "379/379 - 0s - loss: 0.4577 - accuracy: 0.8177 - val_loss: 0.4472 - val_accuracy: 0.8284\n",
      "Epoch 22/210\n",
      "379/379 - 0s - loss: 0.4594 - accuracy: 0.8164 - val_loss: 0.4583 - val_accuracy: 0.8209\n",
      "Epoch 23/210\n",
      "379/379 - 0s - loss: 0.4616 - accuracy: 0.8085 - val_loss: 0.4670 - val_accuracy: 0.8284\n",
      "Epoch 24/210\n",
      "379/379 - 0s - loss: 0.4315 - accuracy: 0.8203 - val_loss: 0.4572 - val_accuracy: 0.8284\n",
      "Epoch 25/210\n",
      "379/379 - 0s - loss: 0.4335 - accuracy: 0.8190 - val_loss: 0.4563 - val_accuracy: 0.8209\n",
      "Epoch 26/210\n",
      "379/379 - 0s - loss: 0.4625 - accuracy: 0.8151 - val_loss: 0.4513 - val_accuracy: 0.8209\n",
      "Epoch 27/210\n",
      "379/379 - 0s - loss: 0.4294 - accuracy: 0.8269 - val_loss: 0.4544 - val_accuracy: 0.8134\n",
      "Epoch 28/210\n",
      "379/379 - 0s - loss: 0.4348 - accuracy: 0.8085 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 29/210\n",
      "379/379 - 0s - loss: 0.4369 - accuracy: 0.8217 - val_loss: 0.4656 - val_accuracy: 0.8284\n",
      "Epoch 30/210\n",
      "379/379 - 0s - loss: 0.4423 - accuracy: 0.8203 - val_loss: 0.4544 - val_accuracy: 0.8284\n",
      "Epoch 31/210\n",
      "379/379 - 0s - loss: 0.4422 - accuracy: 0.8283 - val_loss: 0.4816 - val_accuracy: 0.8284\n",
      "Epoch 32/210\n",
      "379/379 - 0s - loss: 0.4493 - accuracy: 0.8151 - val_loss: 0.4647 - val_accuracy: 0.8284\n",
      "Epoch 33/210\n",
      "379/379 - 0s - loss: 0.4286 - accuracy: 0.8269 - val_loss: 0.4609 - val_accuracy: 0.8284\n",
      "Epoch 34/210\n",
      "379/379 - 0s - loss: 0.4511 - accuracy: 0.8190 - val_loss: 0.4600 - val_accuracy: 0.8284\n",
      "Epoch 35/210\n",
      "379/379 - 0s - loss: 0.4412 - accuracy: 0.8085 - val_loss: 0.4732 - val_accuracy: 0.8284\n",
      "Epoch 36/210\n",
      "379/379 - 0s - loss: 0.4596 - accuracy: 0.8098 - val_loss: 0.4757 - val_accuracy: 0.8284\n",
      "Epoch 37/210\n",
      "379/379 - 0s - loss: 0.4277 - accuracy: 0.8230 - val_loss: 0.4730 - val_accuracy: 0.8284\n",
      "Epoch 38/210\n",
      "379/379 - 0s - loss: 0.4467 - accuracy: 0.8151 - val_loss: 0.4745 - val_accuracy: 0.8209\n",
      "Epoch 39/210\n",
      "379/379 - 0s - loss: 0.4431 - accuracy: 0.8322 - val_loss: 0.4723 - val_accuracy: 0.8284\n",
      "Epoch 40/210\n",
      "379/379 - 0s - loss: 0.4463 - accuracy: 0.8269 - val_loss: 0.4682 - val_accuracy: 0.8284\n",
      "Epoch 41/210\n",
      "379/379 - 0s - loss: 0.4294 - accuracy: 0.8124 - val_loss: 0.4649 - val_accuracy: 0.8358\n",
      "Epoch 42/210\n",
      "379/379 - 0s - loss: 0.4458 - accuracy: 0.8124 - val_loss: 0.4629 - val_accuracy: 0.8358\n",
      "Epoch 43/210\n",
      "379/379 - 0s - loss: 0.4487 - accuracy: 0.8230 - val_loss: 0.4635 - val_accuracy: 0.8358\n",
      "Epoch 44/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8217 - val_loss: 0.4596 - val_accuracy: 0.8358\n",
      "Epoch 45/210\n",
      "379/379 - 0s - loss: 0.4417 - accuracy: 0.8230 - val_loss: 0.4689 - val_accuracy: 0.8358\n",
      "Epoch 46/210\n",
      "379/379 - 0s - loss: 0.4524 - accuracy: 0.8124 - val_loss: 0.4633 - val_accuracy: 0.8358\n",
      "Epoch 47/210\n",
      "379/379 - 0s - loss: 0.4364 - accuracy: 0.8336 - val_loss: 0.4635 - val_accuracy: 0.8358\n",
      "Epoch 48/210\n",
      "379/379 - 0s - loss: 0.4315 - accuracy: 0.8243 - val_loss: 0.4806 - val_accuracy: 0.8358\n",
      "Epoch 49/210\n",
      "379/379 - 0s - loss: 0.4395 - accuracy: 0.8230 - val_loss: 0.4752 - val_accuracy: 0.8358\n",
      "Epoch 50/210\n",
      "379/379 - 0s - loss: 0.4251 - accuracy: 0.8124 - val_loss: 0.4708 - val_accuracy: 0.8358\n",
      "Epoch 51/210\n",
      "379/379 - 0s - loss: 0.4277 - accuracy: 0.8283 - val_loss: 0.4644 - val_accuracy: 0.8358\n",
      "Epoch 52/210\n",
      "379/379 - 0s - loss: 0.4473 - accuracy: 0.8058 - val_loss: 0.4927 - val_accuracy: 0.8209\n",
      "Epoch 53/210\n",
      "379/379 - 0s - loss: 0.4479 - accuracy: 0.8124 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 54/210\n",
      "379/379 - 0s - loss: 0.4336 - accuracy: 0.8151 - val_loss: 0.4717 - val_accuracy: 0.8358\n",
      "Epoch 55/210\n",
      "379/379 - 0s - loss: 0.4286 - accuracy: 0.8243 - val_loss: 0.4644 - val_accuracy: 0.8358\n",
      "Epoch 56/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8269 - val_loss: 0.4685 - val_accuracy: 0.8358\n",
      "Epoch 57/210\n",
      "379/379 - 0s - loss: 0.4415 - accuracy: 0.8203 - val_loss: 0.4676 - val_accuracy: 0.8358\n",
      "Epoch 58/210\n",
      "379/379 - 0s - loss: 0.4441 - accuracy: 0.8230 - val_loss: 0.4719 - val_accuracy: 0.8358\n",
      "Epoch 59/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8111 - val_loss: 0.4709 - val_accuracy: 0.8358\n",
      "Epoch 60/210\n",
      "379/379 - 0s - loss: 0.4522 - accuracy: 0.8230 - val_loss: 0.4665 - val_accuracy: 0.8358\n",
      "Epoch 61/210\n",
      "379/379 - 0s - loss: 0.4479 - accuracy: 0.8203 - val_loss: 0.4625 - val_accuracy: 0.8358\n",
      "Epoch 62/210\n",
      "379/379 - 0s - loss: 0.4486 - accuracy: 0.8137 - val_loss: 0.4646 - val_accuracy: 0.8358\n",
      "Epoch 63/210\n",
      "379/379 - 0s - loss: 0.4407 - accuracy: 0.8137 - val_loss: 0.4695 - val_accuracy: 0.8358\n",
      "Epoch 64/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8243 - val_loss: 0.4596 - val_accuracy: 0.8284\n",
      "Epoch 65/210\n",
      "379/379 - 0s - loss: 0.4208 - accuracy: 0.8283 - val_loss: 0.4694 - val_accuracy: 0.8358\n",
      "Epoch 66/210\n",
      "379/379 - 0s - loss: 0.4398 - accuracy: 0.8309 - val_loss: 0.4625 - val_accuracy: 0.8358\n",
      "Epoch 67/210\n",
      "379/379 - 0s - loss: 0.4103 - accuracy: 0.8269 - val_loss: 0.4831 - val_accuracy: 0.8358\n",
      "Epoch 68/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8256 - val_loss: 0.4670 - val_accuracy: 0.8433\n",
      "Epoch 69/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8269 - val_loss: 0.4706 - val_accuracy: 0.8358\n",
      "Epoch 70/210\n",
      "379/379 - 0s - loss: 0.4554 - accuracy: 0.8177 - val_loss: 0.4679 - val_accuracy: 0.8358\n",
      "Epoch 71/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8137 - val_loss: 0.4832 - val_accuracy: 0.8134\n",
      "Epoch 72/210\n",
      "379/379 - 0s - loss: 0.4367 - accuracy: 0.8217 - val_loss: 0.4690 - val_accuracy: 0.8358\n",
      "Epoch 73/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8177 - val_loss: 0.4929 - val_accuracy: 0.8134\n",
      "Epoch 74/210\n",
      "379/379 - 0s - loss: 0.4311 - accuracy: 0.8137 - val_loss: 0.4830 - val_accuracy: 0.8358\n",
      "Epoch 75/210\n",
      "379/379 - 0s - loss: 0.4337 - accuracy: 0.8164 - val_loss: 0.4715 - val_accuracy: 0.8358\n",
      "Epoch 76/210\n",
      "379/379 - 0s - loss: 0.4226 - accuracy: 0.8230 - val_loss: 0.4485 - val_accuracy: 0.8433\n",
      "Epoch 77/210\n",
      "379/379 - 0s - loss: 0.4323 - accuracy: 0.8269 - val_loss: 0.4825 - val_accuracy: 0.8358\n",
      "Epoch 78/210\n",
      "379/379 - 0s - loss: 0.4428 - accuracy: 0.8190 - val_loss: 0.4870 - val_accuracy: 0.8284\n",
      "Epoch 79/210\n",
      "379/379 - 0s - loss: 0.4384 - accuracy: 0.8203 - val_loss: 0.4763 - val_accuracy: 0.8358\n",
      "Epoch 80/210\n",
      "379/379 - 0s - loss: 0.4499 - accuracy: 0.8203 - val_loss: 0.4763 - val_accuracy: 0.8358\n",
      "Epoch 81/210\n",
      "379/379 - 0s - loss: 0.4380 - accuracy: 0.8217 - val_loss: 0.4630 - val_accuracy: 0.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/210\n",
      "379/379 - 0s - loss: 0.4475 - accuracy: 0.8256 - val_loss: 0.4642 - val_accuracy: 0.8358\n",
      "Epoch 83/210\n",
      "379/379 - 0s - loss: 0.4464 - accuracy: 0.8177 - val_loss: 0.4762 - val_accuracy: 0.8358\n",
      "Epoch 84/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8164 - val_loss: 0.4804 - val_accuracy: 0.8284\n",
      "Epoch 85/210\n",
      "379/379 - 0s - loss: 0.4290 - accuracy: 0.8164 - val_loss: 0.4687 - val_accuracy: 0.8433\n",
      "Epoch 86/210\n",
      "379/379 - 0s - loss: 0.4442 - accuracy: 0.8217 - val_loss: 0.4710 - val_accuracy: 0.8433\n",
      "Epoch 87/210\n",
      "379/379 - 0s - loss: 0.4452 - accuracy: 0.8230 - val_loss: 0.4745 - val_accuracy: 0.8358\n",
      "Epoch 88/210\n",
      "379/379 - 0s - loss: 0.4316 - accuracy: 0.8269 - val_loss: 0.4652 - val_accuracy: 0.8433\n",
      "Epoch 89/210\n",
      "379/379 - 0s - loss: 0.4319 - accuracy: 0.8230 - val_loss: 0.4792 - val_accuracy: 0.8433\n",
      "Epoch 90/210\n",
      "379/379 - 0s - loss: 0.4383 - accuracy: 0.8230 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 91/210\n",
      "379/379 - 0s - loss: 0.4470 - accuracy: 0.8164 - val_loss: 0.4865 - val_accuracy: 0.8284\n",
      "Epoch 92/210\n",
      "379/379 - 0s - loss: 0.4565 - accuracy: 0.8137 - val_loss: 0.4800 - val_accuracy: 0.8358\n",
      "Epoch 93/210\n",
      "379/379 - 0s - loss: 0.4388 - accuracy: 0.8203 - val_loss: 0.4863 - val_accuracy: 0.8284\n",
      "Epoch 94/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8296 - val_loss: 0.4912 - val_accuracy: 0.8284\n",
      "Epoch 95/210\n",
      "379/379 - 0s - loss: 0.4382 - accuracy: 0.8217 - val_loss: 0.4845 - val_accuracy: 0.8358\n",
      "Epoch 96/210\n",
      "379/379 - 0s - loss: 0.4618 - accuracy: 0.8137 - val_loss: 0.4730 - val_accuracy: 0.8358\n",
      "Epoch 97/210\n",
      "379/379 - 0s - loss: 0.4442 - accuracy: 0.8177 - val_loss: 0.4878 - val_accuracy: 0.8358\n",
      "Epoch 98/210\n",
      "379/379 - 0s - loss: 0.4311 - accuracy: 0.8137 - val_loss: 0.4957 - val_accuracy: 0.8284\n",
      "Epoch 99/210\n",
      "379/379 - 0s - loss: 0.4273 - accuracy: 0.8269 - val_loss: 0.4814 - val_accuracy: 0.8433\n",
      "Epoch 100/210\n",
      "379/379 - 0s - loss: 0.4512 - accuracy: 0.8137 - val_loss: 0.4831 - val_accuracy: 0.8284\n",
      "Epoch 101/210\n",
      "379/379 - 0s - loss: 0.4361 - accuracy: 0.8190 - val_loss: 0.4591 - val_accuracy: 0.8433\n",
      "Epoch 102/210\n",
      "379/379 - 0s - loss: 0.4332 - accuracy: 0.8256 - val_loss: 0.4992 - val_accuracy: 0.8358\n",
      "Epoch 103/210\n",
      "379/379 - 0s - loss: 0.4291 - accuracy: 0.8098 - val_loss: 0.5025 - val_accuracy: 0.8358\n",
      "Epoch 104/210\n",
      "379/379 - 0s - loss: 0.4359 - accuracy: 0.8164 - val_loss: 0.4899 - val_accuracy: 0.8358\n",
      "Epoch 105/210\n",
      "379/379 - 0s - loss: 0.4398 - accuracy: 0.8203 - val_loss: 0.4752 - val_accuracy: 0.8433\n",
      "Epoch 106/210\n",
      "379/379 - 0s - loss: 0.4215 - accuracy: 0.8230 - val_loss: 0.4859 - val_accuracy: 0.8433\n",
      "Epoch 107/210\n",
      "379/379 - 0s - loss: 0.4345 - accuracy: 0.8177 - val_loss: 0.4845 - val_accuracy: 0.8358\n",
      "Epoch 108/210\n",
      "379/379 - 0s - loss: 0.4444 - accuracy: 0.8230 - val_loss: 0.4803 - val_accuracy: 0.8433\n",
      "Epoch 109/210\n",
      "379/379 - 0s - loss: 0.4320 - accuracy: 0.8164 - val_loss: 0.4798 - val_accuracy: 0.8358\n",
      "Epoch 110/210\n",
      "379/379 - 0s - loss: 0.4386 - accuracy: 0.8336 - val_loss: 0.4631 - val_accuracy: 0.8433\n",
      "Epoch 111/210\n",
      "379/379 - 0s - loss: 0.4394 - accuracy: 0.8230 - val_loss: 0.4921 - val_accuracy: 0.8358\n",
      "Epoch 112/210\n",
      "379/379 - 0s - loss: 0.4334 - accuracy: 0.8137 - val_loss: 0.4626 - val_accuracy: 0.8433\n",
      "Epoch 113/210\n",
      "379/379 - 0s - loss: 0.4314 - accuracy: 0.8309 - val_loss: 0.4982 - val_accuracy: 0.8433\n",
      "Epoch 114/210\n",
      "379/379 - 0s - loss: 0.4501 - accuracy: 0.8177 - val_loss: 0.4829 - val_accuracy: 0.8358\n",
      "Epoch 115/210\n",
      "379/379 - 0s - loss: 0.4465 - accuracy: 0.8190 - val_loss: 0.4792 - val_accuracy: 0.8284\n",
      "Epoch 116/210\n",
      "379/379 - 0s - loss: 0.4414 - accuracy: 0.8217 - val_loss: 0.4663 - val_accuracy: 0.8358\n",
      "Epoch 117/210\n",
      "379/379 - 0s - loss: 0.4374 - accuracy: 0.8336 - val_loss: 0.4858 - val_accuracy: 0.8433\n",
      "Epoch 118/210\n",
      "379/379 - 0s - loss: 0.4304 - accuracy: 0.8309 - val_loss: 0.4931 - val_accuracy: 0.8358\n",
      "Epoch 119/210\n",
      "379/379 - 0s - loss: 0.4482 - accuracy: 0.8217 - val_loss: 0.4826 - val_accuracy: 0.8358\n",
      "Epoch 120/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8256 - val_loss: 0.4912 - val_accuracy: 0.8358\n",
      "Epoch 121/210\n",
      "379/379 - 0s - loss: 0.4440 - accuracy: 0.8164 - val_loss: 0.4959 - val_accuracy: 0.8209\n",
      "Epoch 122/210\n",
      "379/379 - 0s - loss: 0.4356 - accuracy: 0.8203 - val_loss: 0.4863 - val_accuracy: 0.8433\n",
      "Epoch 123/210\n",
      "379/379 - 0s - loss: 0.4586 - accuracy: 0.8203 - val_loss: 0.4942 - val_accuracy: 0.8284\n",
      "Epoch 124/210\n",
      "379/379 - 0s - loss: 0.4370 - accuracy: 0.8203 - val_loss: 0.4893 - val_accuracy: 0.8209\n",
      "Epoch 125/210\n",
      "379/379 - 0s - loss: 0.4553 - accuracy: 0.8098 - val_loss: 0.4785 - val_accuracy: 0.8284\n",
      "Epoch 126/210\n",
      "379/379 - 0s - loss: 0.4534 - accuracy: 0.8283 - val_loss: 0.4747 - val_accuracy: 0.8358\n",
      "Epoch 127/210\n",
      "379/379 - 0s - loss: 0.4527 - accuracy: 0.8151 - val_loss: 0.4823 - val_accuracy: 0.8358\n",
      "Epoch 128/210\n",
      "379/379 - 0s - loss: 0.4416 - accuracy: 0.8256 - val_loss: 0.5097 - val_accuracy: 0.8060\n",
      "Epoch 129/210\n",
      "379/379 - 0s - loss: 0.4373 - accuracy: 0.8190 - val_loss: 0.4827 - val_accuracy: 0.8358\n",
      "Epoch 130/210\n",
      "379/379 - 0s - loss: 0.4394 - accuracy: 0.8164 - val_loss: 0.4893 - val_accuracy: 0.8358\n",
      "Epoch 131/210\n",
      "379/379 - 0s - loss: 0.4307 - accuracy: 0.8217 - val_loss: 0.4889 - val_accuracy: 0.8433\n",
      "Epoch 132/210\n",
      "379/379 - 0s - loss: 0.4606 - accuracy: 0.8256 - val_loss: 0.4762 - val_accuracy: 0.8433\n",
      "Epoch 133/210\n",
      "379/379 - 0s - loss: 0.4387 - accuracy: 0.8269 - val_loss: 0.4982 - val_accuracy: 0.8284\n",
      "Epoch 134/210\n",
      "379/379 - 0s - loss: 0.4448 - accuracy: 0.8217 - val_loss: 0.4801 - val_accuracy: 0.8209\n",
      "Epoch 135/210\n",
      "379/379 - 0s - loss: 0.4369 - accuracy: 0.8243 - val_loss: 0.4737 - val_accuracy: 0.8433\n",
      "Epoch 136/210\n",
      "379/379 - 0s - loss: 0.4523 - accuracy: 0.8164 - val_loss: 0.4832 - val_accuracy: 0.8433\n",
      "Epoch 137/210\n",
      "379/379 - 0s - loss: 0.4542 - accuracy: 0.8177 - val_loss: 0.4708 - val_accuracy: 0.8433\n",
      "Epoch 138/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8217 - val_loss: 0.4892 - val_accuracy: 0.8284\n",
      "Epoch 139/210\n",
      "379/379 - 0s - loss: 0.4548 - accuracy: 0.8098 - val_loss: 0.4773 - val_accuracy: 0.8433\n",
      "Epoch 140/210\n",
      "379/379 - 0s - loss: 0.4604 - accuracy: 0.8309 - val_loss: 0.4825 - val_accuracy: 0.8358\n",
      "Epoch 141/210\n",
      "379/379 - 0s - loss: 0.4666 - accuracy: 0.8177 - val_loss: 0.4678 - val_accuracy: 0.8433\n",
      "Epoch 142/210\n",
      "379/379 - 0s - loss: 0.4447 - accuracy: 0.8111 - val_loss: 0.4868 - val_accuracy: 0.8284\n",
      "Epoch 143/210\n",
      "379/379 - 0s - loss: 0.4383 - accuracy: 0.8243 - val_loss: 0.4747 - val_accuracy: 0.8433\n",
      "Epoch 144/210\n",
      "379/379 - 0s - loss: 0.4548 - accuracy: 0.8151 - val_loss: 0.4738 - val_accuracy: 0.8284\n",
      "Epoch 145/210\n",
      "379/379 - 0s - loss: 0.4380 - accuracy: 0.8269 - val_loss: 0.5115 - val_accuracy: 0.8134\n",
      "Epoch 146/210\n",
      "379/379 - 0s - loss: 0.4423 - accuracy: 0.8243 - val_loss: 0.4760 - val_accuracy: 0.8358\n",
      "Epoch 147/210\n",
      "379/379 - 0s - loss: 0.4526 - accuracy: 0.8243 - val_loss: 0.4855 - val_accuracy: 0.8284\n",
      "Epoch 148/210\n",
      "379/379 - 0s - loss: 0.4386 - accuracy: 0.8164 - val_loss: 0.4906 - val_accuracy: 0.8358\n",
      "Epoch 149/210\n",
      "379/379 - 0s - loss: 0.4446 - accuracy: 0.8203 - val_loss: 0.4836 - val_accuracy: 0.8358\n",
      "Epoch 150/210\n",
      "379/379 - 0s - loss: 0.4659 - accuracy: 0.8230 - val_loss: 0.4781 - val_accuracy: 0.8209\n",
      "Epoch 151/210\n",
      "379/379 - 0s - loss: 0.4193 - accuracy: 0.8164 - val_loss: 0.4831 - val_accuracy: 0.8209\n",
      "Epoch 152/210\n",
      "379/379 - 0s - loss: 0.4500 - accuracy: 0.8151 - val_loss: 0.4873 - val_accuracy: 0.8284\n",
      "Epoch 153/210\n",
      "379/379 - 0s - loss: 0.4314 - accuracy: 0.8322 - val_loss: 0.4856 - val_accuracy: 0.8284\n",
      "Epoch 154/210\n",
      "379/379 - 0s - loss: 0.4552 - accuracy: 0.8124 - val_loss: 0.4797 - val_accuracy: 0.8284\n",
      "Epoch 155/210\n",
      "379/379 - 0s - loss: 0.4487 - accuracy: 0.8190 - val_loss: 0.4590 - val_accuracy: 0.8433\n",
      "Epoch 156/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8283 - val_loss: 0.4965 - val_accuracy: 0.8209\n",
      "Epoch 157/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8269 - val_loss: 0.4935 - val_accuracy: 0.8134\n",
      "Epoch 158/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8190 - val_loss: 0.4714 - val_accuracy: 0.8284\n",
      "Epoch 159/210\n",
      "379/379 - 0s - loss: 0.4450 - accuracy: 0.8309 - val_loss: 0.4934 - val_accuracy: 0.8209\n",
      "Epoch 160/210\n",
      "379/379 - 0s - loss: 0.4459 - accuracy: 0.8336 - val_loss: 0.5017 - val_accuracy: 0.8209\n",
      "Epoch 161/210\n",
      "379/379 - 0s - loss: 0.4375 - accuracy: 0.8269 - val_loss: 0.4842 - val_accuracy: 0.8284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/210\n",
      "379/379 - 0s - loss: 0.4605 - accuracy: 0.8085 - val_loss: 0.5053 - val_accuracy: 0.8284\n",
      "Epoch 163/210\n",
      "379/379 - 0s - loss: 0.4581 - accuracy: 0.8124 - val_loss: 0.4624 - val_accuracy: 0.8358\n",
      "Epoch 164/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8177 - val_loss: 0.4757 - val_accuracy: 0.8284\n",
      "Epoch 165/210\n",
      "379/379 - 0s - loss: 0.4147 - accuracy: 0.8428 - val_loss: 0.4720 - val_accuracy: 0.8433\n",
      "Epoch 166/210\n",
      "379/379 - 0s - loss: 0.4619 - accuracy: 0.8283 - val_loss: 0.4774 - val_accuracy: 0.8284\n",
      "Epoch 167/210\n",
      "379/379 - 0s - loss: 0.4389 - accuracy: 0.8203 - val_loss: 0.4803 - val_accuracy: 0.8358\n",
      "Epoch 168/210\n",
      "379/379 - 0s - loss: 0.4376 - accuracy: 0.8164 - val_loss: 0.4821 - val_accuracy: 0.8433\n",
      "Epoch 169/210\n",
      "379/379 - 0s - loss: 0.4537 - accuracy: 0.8203 - val_loss: 0.4926 - val_accuracy: 0.8358\n",
      "Epoch 170/210\n",
      "379/379 - 0s - loss: 0.4347 - accuracy: 0.8283 - val_loss: 0.4903 - val_accuracy: 0.8358\n",
      "Epoch 171/210\n",
      "379/379 - 0s - loss: 0.4302 - accuracy: 0.8309 - val_loss: 0.4904 - val_accuracy: 0.8284\n",
      "Epoch 172/210\n",
      "379/379 - 0s - loss: 0.4257 - accuracy: 0.8415 - val_loss: 0.4992 - val_accuracy: 0.8358\n",
      "Epoch 173/210\n",
      "379/379 - 0s - loss: 0.4463 - accuracy: 0.8243 - val_loss: 0.4694 - val_accuracy: 0.8433\n",
      "Epoch 174/210\n",
      "379/379 - 0s - loss: 0.4306 - accuracy: 0.8137 - val_loss: 0.4833 - val_accuracy: 0.8433\n",
      "Epoch 175/210\n",
      "379/379 - 0s - loss: 0.4475 - accuracy: 0.8322 - val_loss: 0.4830 - val_accuracy: 0.8358\n",
      "Epoch 176/210\n",
      "379/379 - 0s - loss: 0.4409 - accuracy: 0.8203 - val_loss: 0.4704 - val_accuracy: 0.8358\n",
      "Epoch 177/210\n",
      "379/379 - 0s - loss: 0.4488 - accuracy: 0.8217 - val_loss: 0.4765 - val_accuracy: 0.8358\n",
      "Epoch 178/210\n",
      "379/379 - 0s - loss: 0.4325 - accuracy: 0.8217 - val_loss: 0.4888 - val_accuracy: 0.8433\n",
      "Epoch 179/210\n",
      "379/379 - 0s - loss: 0.4558 - accuracy: 0.8230 - val_loss: 0.4699 - val_accuracy: 0.8209\n",
      "Epoch 180/210\n",
      "379/379 - 0s - loss: 0.4464 - accuracy: 0.8137 - val_loss: 0.5080 - val_accuracy: 0.8284\n",
      "Epoch 181/210\n",
      "379/379 - 0s - loss: 0.4604 - accuracy: 0.8243 - val_loss: 0.4944 - val_accuracy: 0.8284\n",
      "Epoch 182/210\n",
      "379/379 - 0s - loss: 0.4453 - accuracy: 0.8256 - val_loss: 0.4776 - val_accuracy: 0.8358\n",
      "Epoch 183/210\n",
      "379/379 - 0s - loss: 0.4322 - accuracy: 0.8296 - val_loss: 0.4998 - val_accuracy: 0.8134\n",
      "Epoch 184/210\n",
      "379/379 - 0s - loss: 0.4295 - accuracy: 0.8283 - val_loss: 0.4807 - val_accuracy: 0.8433\n",
      "Epoch 185/210\n",
      "379/379 - 0s - loss: 0.4485 - accuracy: 0.8217 - val_loss: 0.4909 - val_accuracy: 0.8209\n",
      "Epoch 186/210\n",
      "379/379 - 0s - loss: 0.4494 - accuracy: 0.8349 - val_loss: 0.5073 - val_accuracy: 0.8358\n",
      "Epoch 187/210\n",
      "379/379 - 0s - loss: 0.4581 - accuracy: 0.8230 - val_loss: 0.4883 - val_accuracy: 0.8284\n",
      "Epoch 188/210\n",
      "379/379 - 0s - loss: 0.4306 - accuracy: 0.8336 - val_loss: 0.4955 - val_accuracy: 0.8284\n",
      "Epoch 189/210\n",
      "379/379 - 0s - loss: 0.4339 - accuracy: 0.8243 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 190/210\n",
      "379/379 - 0s - loss: 0.4207 - accuracy: 0.8349 - val_loss: 0.4908 - val_accuracy: 0.8358\n",
      "Epoch 191/210\n",
      "379/379 - 0s - loss: 0.4683 - accuracy: 0.8151 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 192/210\n",
      "379/379 - 0s - loss: 0.4335 - accuracy: 0.8336 - val_loss: 0.5021 - val_accuracy: 0.8134\n",
      "Epoch 193/210\n",
      "379/379 - 0s - loss: 0.4484 - accuracy: 0.8177 - val_loss: 0.4856 - val_accuracy: 0.8284\n",
      "Epoch 194/210\n",
      "379/379 - 0s - loss: 0.4570 - accuracy: 0.8322 - val_loss: 0.4904 - val_accuracy: 0.8209\n",
      "Epoch 195/210\n",
      "379/379 - 0s - loss: 0.4435 - accuracy: 0.8190 - val_loss: 0.5076 - val_accuracy: 0.8433\n",
      "Epoch 196/210\n",
      "379/379 - 0s - loss: 0.4504 - accuracy: 0.8269 - val_loss: 0.4855 - val_accuracy: 0.8209\n",
      "Epoch 197/210\n",
      "379/379 - 0s - loss: 0.4537 - accuracy: 0.8309 - val_loss: 0.4791 - val_accuracy: 0.8358\n",
      "Epoch 198/210\n",
      "379/379 - 0s - loss: 0.4363 - accuracy: 0.8217 - val_loss: 0.4741 - val_accuracy: 0.8284\n",
      "Epoch 199/210\n",
      "379/379 - 0s - loss: 0.4598 - accuracy: 0.8296 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 200/210\n",
      "379/379 - 0s - loss: 0.4527 - accuracy: 0.8111 - val_loss: 0.4811 - val_accuracy: 0.8284\n",
      "Epoch 201/210\n",
      "379/379 - 0s - loss: 0.4466 - accuracy: 0.8388 - val_loss: 0.5007 - val_accuracy: 0.8433\n",
      "Epoch 202/210\n",
      "379/379 - 0s - loss: 0.4526 - accuracy: 0.8230 - val_loss: 0.4897 - val_accuracy: 0.8358\n",
      "Epoch 203/210\n",
      "379/379 - 0s - loss: 0.4368 - accuracy: 0.8243 - val_loss: 0.5286 - val_accuracy: 0.8209\n",
      "Epoch 204/210\n",
      "379/379 - 0s - loss: 0.4252 - accuracy: 0.8322 - val_loss: 0.4865 - val_accuracy: 0.8358\n",
      "Epoch 205/210\n",
      "379/379 - 0s - loss: 0.4622 - accuracy: 0.8177 - val_loss: 0.4749 - val_accuracy: 0.8209\n",
      "Epoch 206/210\n",
      "379/379 - 0s - loss: 0.4326 - accuracy: 0.8190 - val_loss: 0.4805 - val_accuracy: 0.8433\n",
      "Epoch 207/210\n",
      "379/379 - 0s - loss: 0.4434 - accuracy: 0.8230 - val_loss: 0.4876 - val_accuracy: 0.8433\n",
      "Epoch 208/210\n",
      "379/379 - 0s - loss: 0.4613 - accuracy: 0.8124 - val_loss: 0.4886 - val_accuracy: 0.8433\n",
      "Epoch 209/210\n",
      "379/379 - 0s - loss: 0.4303 - accuracy: 0.8309 - val_loss: 0.4939 - val_accuracy: 0.8284\n",
      "Epoch 210/210\n",
      "379/379 - 0s - loss: 0.4327 - accuracy: 0.8269 - val_loss: 0.5093 - val_accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from keras.layers import Dropout\n",
    "def build_classifier(loss):\n",
    "    ### Initializing the ANN\n",
    "    ann_p = tf.keras.models.Sequential()\n",
    "    ### Adding the input layer and the hidden layers\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ### Compiling the ANN\n",
    "    ann_p.compile(optimizer = 'RMSprop', loss = loss, metrics = ['accuracy'])\n",
    "    return ann_p\n",
    "ann_p = KerasClassifier(build_fn = build_classifier, batch_size = 8, epochs = 200, \n",
    "                        validation_data=(X_test,y_test), verbose=2)\n",
    "parameters = {\"batch_size\": [2,5,10,14,20], \n",
    "             \"epochs\": [80,140,180,210],\n",
    "             \"loss\": [\"mean_squared_error\",\"poisson\", \"binary_crossentropy\"]}\n",
    "grid_search = GridSearchCV(estimator=ann_p, param_grid=parameters, scoring=\"accuracy\", cv=6, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09806261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362392200974877 {'batch_size': 2, 'epochs': 210, 'loss': 'binary_crossentropy'}\n"
     ]
    }
   ],
   "source": [
    "print(best_accuracy, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d77ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from keras.layers import Dropout\n",
    "def build_classifier():\n",
    "    ### Initializing the ANN\n",
    "    ann_p = tf.keras.models.Sequential()\n",
    "    ### Adding the input layer and the hidden layers\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ### Compiling the ANN\n",
    "    ann_p.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann_p\n",
    "ann_p = KerasClassifier(build_fn = build_classifier, batch_size = 2, epochs = 210, \n",
    "                        validation_data=(X_test,y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35602ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77952754 0.86507934 0.8174603  0.85714287 0.8174603  0.78571427]\n",
      "[0.8203974366188049] [0.0322418544823061]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=ann_p, X=X_train, y=y_train, cv=6, n_jobs=-1, verbose=1)\n",
    "print(accuracies)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()\n",
    "print([mean], [std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdeeeb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210\n",
      "67/67 - 1s - loss: 0.5548 - accuracy: 0.7164 - val_loss: 0.4907 - val_accuracy: 0.8060\n",
      "Epoch 2/210\n",
      "67/67 - 0s - loss: 0.4954 - accuracy: 0.8060 - val_loss: 0.4437 - val_accuracy: 0.7985\n",
      "Epoch 3/210\n",
      "67/67 - 0s - loss: 0.4951 - accuracy: 0.7910 - val_loss: 0.4159 - val_accuracy: 0.7910\n",
      "Epoch 4/210\n",
      "67/67 - 0s - loss: 0.4853 - accuracy: 0.7836 - val_loss: 0.4002 - val_accuracy: 0.7761\n",
      "Epoch 5/210\n",
      "67/67 - 0s - loss: 0.4222 - accuracy: 0.8134 - val_loss: 0.3931 - val_accuracy: 0.8060\n",
      "Epoch 6/210\n",
      "67/67 - 0s - loss: 0.4223 - accuracy: 0.7910 - val_loss: 0.3896 - val_accuracy: 0.8134\n",
      "Epoch 7/210\n",
      "67/67 - 0s - loss: 0.3975 - accuracy: 0.8209 - val_loss: 0.3869 - val_accuracy: 0.8284\n",
      "Epoch 8/210\n",
      "67/67 - 0s - loss: 0.4781 - accuracy: 0.7910 - val_loss: 0.3863 - val_accuracy: 0.8209\n",
      "Epoch 9/210\n",
      "67/67 - 0s - loss: 0.4229 - accuracy: 0.8284 - val_loss: 0.3860 - val_accuracy: 0.8209\n",
      "Epoch 10/210\n",
      "67/67 - 0s - loss: 0.4450 - accuracy: 0.8209 - val_loss: 0.3882 - val_accuracy: 0.8358\n",
      "Epoch 11/210\n",
      "67/67 - 0s - loss: 0.4484 - accuracy: 0.8134 - val_loss: 0.3888 - val_accuracy: 0.8358\n",
      "Epoch 12/210\n",
      "67/67 - 0s - loss: 0.4329 - accuracy: 0.7910 - val_loss: 0.3874 - val_accuracy: 0.8358\n",
      "Epoch 13/210\n",
      "67/67 - 0s - loss: 0.4101 - accuracy: 0.8284 - val_loss: 0.3868 - val_accuracy: 0.8209\n",
      "Epoch 14/210\n",
      "67/67 - 0s - loss: 0.5333 - accuracy: 0.8134 - val_loss: 0.3840 - val_accuracy: 0.8209\n",
      "Epoch 15/210\n",
      "67/67 - 0s - loss: 0.4833 - accuracy: 0.7761 - val_loss: 0.3820 - val_accuracy: 0.8284\n",
      "Epoch 16/210\n",
      "67/67 - 0s - loss: 0.3624 - accuracy: 0.8433 - val_loss: 0.3829 - val_accuracy: 0.8358\n",
      "Epoch 17/210\n",
      "67/67 - 0s - loss: 0.4492 - accuracy: 0.8060 - val_loss: 0.3815 - val_accuracy: 0.8284\n",
      "Epoch 18/210\n",
      "67/67 - 0s - loss: 0.4615 - accuracy: 0.7836 - val_loss: 0.3819 - val_accuracy: 0.8433\n",
      "Epoch 19/210\n",
      "67/67 - 0s - loss: 0.4279 - accuracy: 0.8209 - val_loss: 0.3816 - val_accuracy: 0.8358\n",
      "Epoch 20/210\n",
      "67/67 - 0s - loss: 0.4215 - accuracy: 0.8134 - val_loss: 0.3817 - val_accuracy: 0.8284\n",
      "Epoch 21/210\n",
      "67/67 - 0s - loss: 0.4228 - accuracy: 0.8134 - val_loss: 0.3828 - val_accuracy: 0.8358\n",
      "Epoch 22/210\n",
      "67/67 - 0s - loss: 0.3428 - accuracy: 0.8433 - val_loss: 0.3850 - val_accuracy: 0.8358\n",
      "Epoch 23/210\n",
      "67/67 - 0s - loss: 0.4381 - accuracy: 0.7761 - val_loss: 0.3820 - val_accuracy: 0.8358\n",
      "Epoch 24/210\n",
      "67/67 - 0s - loss: 0.3695 - accuracy: 0.8134 - val_loss: 0.3838 - val_accuracy: 0.8358\n",
      "Epoch 25/210\n",
      "67/67 - 0s - loss: 0.3972 - accuracy: 0.8358 - val_loss: 0.3844 - val_accuracy: 0.8284\n",
      "Epoch 26/210\n",
      "67/67 - 0s - loss: 0.4392 - accuracy: 0.7985 - val_loss: 0.3817 - val_accuracy: 0.8284\n",
      "Epoch 27/210\n",
      "67/67 - 0s - loss: 0.4545 - accuracy: 0.8209 - val_loss: 0.3813 - val_accuracy: 0.8284\n",
      "Epoch 28/210\n",
      "67/67 - 0s - loss: 0.4613 - accuracy: 0.8284 - val_loss: 0.3794 - val_accuracy: 0.8284\n",
      "Epoch 29/210\n",
      "67/67 - 0s - loss: 0.4120 - accuracy: 0.8284 - val_loss: 0.3823 - val_accuracy: 0.8284\n",
      "Epoch 30/210\n",
      "67/67 - 0s - loss: 0.4572 - accuracy: 0.8134 - val_loss: 0.3787 - val_accuracy: 0.8284\n",
      "Epoch 31/210\n",
      "67/67 - 0s - loss: 0.4850 - accuracy: 0.8209 - val_loss: 0.3761 - val_accuracy: 0.8209\n",
      "Epoch 32/210\n",
      "67/67 - 0s - loss: 0.4657 - accuracy: 0.8060 - val_loss: 0.3765 - val_accuracy: 0.8209\n",
      "Epoch 33/210\n",
      "67/67 - 0s - loss: 0.4123 - accuracy: 0.8209 - val_loss: 0.3771 - val_accuracy: 0.8284\n",
      "Epoch 34/210\n",
      "67/67 - 0s - loss: 0.4381 - accuracy: 0.8060 - val_loss: 0.3752 - val_accuracy: 0.8209\n",
      "Epoch 35/210\n",
      "67/67 - 0s - loss: 0.3910 - accuracy: 0.8433 - val_loss: 0.3751 - val_accuracy: 0.8209\n",
      "Epoch 36/210\n",
      "67/67 - 0s - loss: 0.4309 - accuracy: 0.8284 - val_loss: 0.3713 - val_accuracy: 0.8209\n",
      "Epoch 37/210\n",
      "67/67 - 0s - loss: 0.4218 - accuracy: 0.8060 - val_loss: 0.3728 - val_accuracy: 0.8209\n",
      "Epoch 38/210\n",
      "67/67 - 0s - loss: 0.4476 - accuracy: 0.8060 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 39/210\n",
      "67/67 - 0s - loss: 0.4099 - accuracy: 0.8284 - val_loss: 0.3736 - val_accuracy: 0.8209\n",
      "Epoch 40/210\n",
      "67/67 - 0s - loss: 0.4072 - accuracy: 0.8358 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 41/210\n",
      "67/67 - 0s - loss: 0.4277 - accuracy: 0.8433 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 42/210\n",
      "67/67 - 0s - loss: 0.4184 - accuracy: 0.8358 - val_loss: 0.3729 - val_accuracy: 0.8209\n",
      "Epoch 43/210\n",
      "67/67 - 0s - loss: 0.4684 - accuracy: 0.8060 - val_loss: 0.3696 - val_accuracy: 0.8284\n",
      "Epoch 44/210\n",
      "67/67 - 0s - loss: 0.4412 - accuracy: 0.7985 - val_loss: 0.3690 - val_accuracy: 0.8358\n",
      "Epoch 45/210\n",
      "67/67 - 0s - loss: 0.4210 - accuracy: 0.8209 - val_loss: 0.3689 - val_accuracy: 0.8358\n",
      "Epoch 46/210\n",
      "67/67 - 0s - loss: 0.4239 - accuracy: 0.8358 - val_loss: 0.3688 - val_accuracy: 0.8358\n",
      "Epoch 47/210\n",
      "67/67 - 0s - loss: 0.4177 - accuracy: 0.8507 - val_loss: 0.3705 - val_accuracy: 0.8433\n",
      "Epoch 48/210\n",
      "67/67 - 0s - loss: 0.4255 - accuracy: 0.8284 - val_loss: 0.3699 - val_accuracy: 0.8358\n",
      "Epoch 49/210\n",
      "67/67 - 0s - loss: 0.4460 - accuracy: 0.8433 - val_loss: 0.3681 - val_accuracy: 0.8358\n",
      "Epoch 50/210\n",
      "67/67 - 0s - loss: 0.3738 - accuracy: 0.8433 - val_loss: 0.3685 - val_accuracy: 0.8358\n",
      "Epoch 51/210\n",
      "67/67 - 0s - loss: 0.4304 - accuracy: 0.8284 - val_loss: 0.3683 - val_accuracy: 0.8358\n",
      "Epoch 52/210\n",
      "67/67 - 0s - loss: 0.4219 - accuracy: 0.8358 - val_loss: 0.3706 - val_accuracy: 0.8433\n",
      "Epoch 53/210\n",
      "67/67 - 0s - loss: 0.4347 - accuracy: 0.8134 - val_loss: 0.3718 - val_accuracy: 0.8358\n",
      "Epoch 54/210\n",
      "67/67 - 0s - loss: 0.4226 - accuracy: 0.8209 - val_loss: 0.3703 - val_accuracy: 0.8358\n",
      "Epoch 55/210\n",
      "67/67 - 0s - loss: 0.4042 - accuracy: 0.8507 - val_loss: 0.3698 - val_accuracy: 0.8358\n",
      "Epoch 56/210\n",
      "67/67 - 0s - loss: 0.4405 - accuracy: 0.8060 - val_loss: 0.3687 - val_accuracy: 0.8358\n",
      "Epoch 57/210\n",
      "67/67 - 0s - loss: 0.4004 - accuracy: 0.8209 - val_loss: 0.3698 - val_accuracy: 0.8358\n",
      "Epoch 58/210\n",
      "67/67 - 0s - loss: 0.3783 - accuracy: 0.8060 - val_loss: 0.3707 - val_accuracy: 0.8358\n",
      "Epoch 59/210\n",
      "67/67 - 0s - loss: 0.3948 - accuracy: 0.8358 - val_loss: 0.3706 - val_accuracy: 0.8358\n",
      "Epoch 60/210\n",
      "67/67 - 0s - loss: 0.4275 - accuracy: 0.8284 - val_loss: 0.3705 - val_accuracy: 0.8284\n",
      "Epoch 61/210\n",
      "67/67 - 0s - loss: 0.3470 - accuracy: 0.8507 - val_loss: 0.3699 - val_accuracy: 0.8284\n",
      "Epoch 62/210\n",
      "67/67 - 0s - loss: 0.4129 - accuracy: 0.8433 - val_loss: 0.3680 - val_accuracy: 0.8284\n",
      "Epoch 63/210\n",
      "67/67 - 0s - loss: 0.4483 - accuracy: 0.8209 - val_loss: 0.3675 - val_accuracy: 0.8284\n",
      "Epoch 64/210\n",
      "67/67 - 0s - loss: 0.4037 - accuracy: 0.8134 - val_loss: 0.3669 - val_accuracy: 0.8284\n",
      "Epoch 65/210\n",
      "67/67 - 0s - loss: 0.3743 - accuracy: 0.8731 - val_loss: 0.3667 - val_accuracy: 0.8358\n",
      "Epoch 66/210\n",
      "67/67 - 0s - loss: 0.3909 - accuracy: 0.8209 - val_loss: 0.3691 - val_accuracy: 0.8358\n",
      "Epoch 67/210\n",
      "67/67 - 0s - loss: 0.4464 - accuracy: 0.8134 - val_loss: 0.3676 - val_accuracy: 0.8358\n",
      "Epoch 68/210\n",
      "67/67 - 0s - loss: 0.4553 - accuracy: 0.8134 - val_loss: 0.3663 - val_accuracy: 0.8433\n",
      "Epoch 69/210\n",
      "67/67 - 0s - loss: 0.4390 - accuracy: 0.8507 - val_loss: 0.3633 - val_accuracy: 0.8358\n",
      "Epoch 70/210\n",
      "67/67 - 0s - loss: 0.4180 - accuracy: 0.8209 - val_loss: 0.3620 - val_accuracy: 0.8358\n",
      "Epoch 71/210\n",
      "67/67 - 0s - loss: 0.3669 - accuracy: 0.8433 - val_loss: 0.3650 - val_accuracy: 0.8358\n",
      "Epoch 72/210\n",
      "67/67 - 0s - loss: 0.4522 - accuracy: 0.8134 - val_loss: 0.3624 - val_accuracy: 0.8433\n",
      "Epoch 73/210\n",
      "67/67 - 0s - loss: 0.3445 - accuracy: 0.8507 - val_loss: 0.3670 - val_accuracy: 0.8358\n",
      "Epoch 74/210\n",
      "67/67 - 0s - loss: 0.4028 - accuracy: 0.8134 - val_loss: 0.3675 - val_accuracy: 0.8358\n",
      "Epoch 75/210\n",
      "67/67 - 0s - loss: 0.4135 - accuracy: 0.8358 - val_loss: 0.3677 - val_accuracy: 0.8433\n",
      "Epoch 76/210\n",
      "67/67 - 0s - loss: 0.4110 - accuracy: 0.8358 - val_loss: 0.3692 - val_accuracy: 0.8433\n",
      "Epoch 77/210\n",
      "67/67 - 0s - loss: 0.3966 - accuracy: 0.8507 - val_loss: 0.3672 - val_accuracy: 0.8433\n",
      "Epoch 78/210\n",
      "67/67 - 0s - loss: 0.3855 - accuracy: 0.8582 - val_loss: 0.3658 - val_accuracy: 0.8433\n",
      "Epoch 79/210\n",
      "67/67 - 0s - loss: 0.4296 - accuracy: 0.8284 - val_loss: 0.3628 - val_accuracy: 0.8358\n",
      "Epoch 80/210\n",
      "67/67 - 0s - loss: 0.3934 - accuracy: 0.8433 - val_loss: 0.3616 - val_accuracy: 0.8433\n",
      "Epoch 81/210\n",
      "67/67 - 0s - loss: 0.3528 - accuracy: 0.8284 - val_loss: 0.3634 - val_accuracy: 0.8358\n",
      "Epoch 82/210\n",
      "67/67 - 0s - loss: 0.3491 - accuracy: 0.8731 - val_loss: 0.3639 - val_accuracy: 0.8358\n",
      "Epoch 83/210\n",
      "67/67 - 0s - loss: 0.4120 - accuracy: 0.8358 - val_loss: 0.3660 - val_accuracy: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/210\n",
      "67/67 - 0s - loss: 0.3809 - accuracy: 0.8582 - val_loss: 0.3650 - val_accuracy: 0.8433\n",
      "Epoch 85/210\n",
      "67/67 - 0s - loss: 0.4149 - accuracy: 0.8134 - val_loss: 0.3625 - val_accuracy: 0.8433\n",
      "Epoch 86/210\n",
      "67/67 - 0s - loss: 0.4050 - accuracy: 0.8284 - val_loss: 0.3636 - val_accuracy: 0.8433\n",
      "Epoch 87/210\n",
      "67/67 - 0s - loss: 0.4055 - accuracy: 0.8284 - val_loss: 0.3646 - val_accuracy: 0.8433\n",
      "Epoch 88/210\n",
      "67/67 - 0s - loss: 0.4921 - accuracy: 0.8134 - val_loss: 0.3629 - val_accuracy: 0.8433\n",
      "Epoch 89/210\n",
      "67/67 - 0s - loss: 0.4631 - accuracy: 0.7612 - val_loss: 0.3605 - val_accuracy: 0.8433\n",
      "Epoch 90/210\n",
      "67/67 - 0s - loss: 0.3785 - accuracy: 0.8433 - val_loss: 0.3619 - val_accuracy: 0.8433\n",
      "Epoch 91/210\n",
      "67/67 - 0s - loss: 0.4043 - accuracy: 0.8433 - val_loss: 0.3625 - val_accuracy: 0.8433\n",
      "Epoch 92/210\n",
      "67/67 - 0s - loss: 0.3751 - accuracy: 0.8209 - val_loss: 0.3617 - val_accuracy: 0.8433\n",
      "Epoch 93/210\n",
      "67/67 - 0s - loss: 0.4035 - accuracy: 0.8358 - val_loss: 0.3619 - val_accuracy: 0.8433\n",
      "Epoch 94/210\n",
      "67/67 - 0s - loss: 0.4516 - accuracy: 0.8060 - val_loss: 0.3609 - val_accuracy: 0.8433\n",
      "Epoch 95/210\n",
      "67/67 - 0s - loss: 0.4005 - accuracy: 0.8582 - val_loss: 0.3602 - val_accuracy: 0.8433\n",
      "Epoch 96/210\n",
      "67/67 - 0s - loss: 0.3884 - accuracy: 0.8284 - val_loss: 0.3590 - val_accuracy: 0.8433\n",
      "Epoch 97/210\n",
      "67/67 - 0s - loss: 0.4357 - accuracy: 0.8209 - val_loss: 0.3572 - val_accuracy: 0.8433\n",
      "Epoch 98/210\n",
      "67/67 - 0s - loss: 0.3941 - accuracy: 0.8806 - val_loss: 0.3561 - val_accuracy: 0.8433\n",
      "Epoch 99/210\n",
      "67/67 - 0s - loss: 0.4223 - accuracy: 0.8582 - val_loss: 0.3561 - val_accuracy: 0.8358\n",
      "Epoch 100/210\n",
      "67/67 - 0s - loss: 0.4370 - accuracy: 0.7985 - val_loss: 0.3566 - val_accuracy: 0.8433\n",
      "Epoch 101/210\n",
      "67/67 - 0s - loss: 0.4019 - accuracy: 0.8358 - val_loss: 0.3540 - val_accuracy: 0.8433\n",
      "Epoch 102/210\n",
      "67/67 - 0s - loss: 0.3926 - accuracy: 0.8358 - val_loss: 0.3554 - val_accuracy: 0.8433\n",
      "Epoch 103/210\n",
      "67/67 - 0s - loss: 0.4142 - accuracy: 0.8209 - val_loss: 0.3548 - val_accuracy: 0.8433\n",
      "Epoch 104/210\n",
      "67/67 - 0s - loss: 0.3861 - accuracy: 0.8358 - val_loss: 0.3552 - val_accuracy: 0.8582\n",
      "Epoch 105/210\n",
      "67/67 - 0s - loss: 0.4014 - accuracy: 0.8358 - val_loss: 0.3571 - val_accuracy: 0.8433\n",
      "Epoch 106/210\n",
      "67/67 - 0s - loss: 0.3164 - accuracy: 0.8582 - val_loss: 0.3583 - val_accuracy: 0.8433\n",
      "Epoch 107/210\n",
      "67/67 - 0s - loss: 0.3824 - accuracy: 0.8284 - val_loss: 0.3559 - val_accuracy: 0.8433\n",
      "Epoch 108/210\n",
      "67/67 - 0s - loss: 0.4127 - accuracy: 0.8209 - val_loss: 0.3561 - val_accuracy: 0.8433\n",
      "Epoch 109/210\n",
      "67/67 - 0s - loss: 0.3894 - accuracy: 0.8433 - val_loss: 0.3576 - val_accuracy: 0.8433\n",
      "Epoch 110/210\n",
      "67/67 - 0s - loss: 0.3898 - accuracy: 0.8358 - val_loss: 0.3595 - val_accuracy: 0.8433\n",
      "Epoch 111/210\n",
      "67/67 - 0s - loss: 0.4096 - accuracy: 0.8433 - val_loss: 0.3590 - val_accuracy: 0.8433\n",
      "Epoch 112/210\n",
      "67/67 - 0s - loss: 0.3689 - accuracy: 0.8881 - val_loss: 0.3559 - val_accuracy: 0.8433\n",
      "Epoch 113/210\n",
      "67/67 - 0s - loss: 0.3998 - accuracy: 0.8284 - val_loss: 0.3540 - val_accuracy: 0.8433\n",
      "Epoch 114/210\n",
      "67/67 - 0s - loss: 0.4277 - accuracy: 0.8433 - val_loss: 0.3533 - val_accuracy: 0.8433\n",
      "Epoch 115/210\n",
      "67/67 - 0s - loss: 0.4590 - accuracy: 0.7836 - val_loss: 0.3547 - val_accuracy: 0.8433\n",
      "Epoch 116/210\n",
      "67/67 - 0s - loss: 0.3761 - accuracy: 0.8582 - val_loss: 0.3590 - val_accuracy: 0.8507\n",
      "Epoch 117/210\n",
      "67/67 - 0s - loss: 0.4002 - accuracy: 0.8134 - val_loss: 0.3583 - val_accuracy: 0.8507\n",
      "Epoch 118/210\n",
      "67/67 - 0s - loss: 0.4069 - accuracy: 0.8209 - val_loss: 0.3570 - val_accuracy: 0.8507\n",
      "Epoch 119/210\n",
      "67/67 - 0s - loss: 0.3812 - accuracy: 0.8731 - val_loss: 0.3562 - val_accuracy: 0.8582\n",
      "Epoch 120/210\n",
      "67/67 - 0s - loss: 0.3527 - accuracy: 0.8657 - val_loss: 0.3588 - val_accuracy: 0.8507\n",
      "Epoch 121/210\n",
      "67/67 - 0s - loss: 0.4073 - accuracy: 0.8209 - val_loss: 0.3557 - val_accuracy: 0.8507\n",
      "Epoch 122/210\n",
      "67/67 - 0s - loss: 0.4313 - accuracy: 0.8134 - val_loss: 0.3546 - val_accuracy: 0.8507\n",
      "Epoch 123/210\n",
      "67/67 - 0s - loss: 0.3790 - accuracy: 0.8582 - val_loss: 0.3563 - val_accuracy: 0.8507\n",
      "Epoch 124/210\n",
      "67/67 - 0s - loss: 0.4048 - accuracy: 0.8284 - val_loss: 0.3550 - val_accuracy: 0.8507\n",
      "Epoch 125/210\n",
      "67/67 - 0s - loss: 0.4164 - accuracy: 0.8358 - val_loss: 0.3572 - val_accuracy: 0.8582\n",
      "Epoch 126/210\n",
      "67/67 - 0s - loss: 0.4060 - accuracy: 0.8060 - val_loss: 0.3569 - val_accuracy: 0.8507\n",
      "Epoch 127/210\n",
      "67/67 - 0s - loss: 0.4075 - accuracy: 0.8507 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 128/210\n",
      "67/67 - 0s - loss: 0.3523 - accuracy: 0.8582 - val_loss: 0.3584 - val_accuracy: 0.8582\n",
      "Epoch 129/210\n",
      "67/67 - 0s - loss: 0.4205 - accuracy: 0.8284 - val_loss: 0.3551 - val_accuracy: 0.8582\n",
      "Epoch 130/210\n",
      "67/67 - 0s - loss: 0.3785 - accuracy: 0.8582 - val_loss: 0.3548 - val_accuracy: 0.8582\n",
      "Epoch 131/210\n",
      "67/67 - 0s - loss: 0.3650 - accuracy: 0.8507 - val_loss: 0.3559 - val_accuracy: 0.8582\n",
      "Epoch 132/210\n",
      "67/67 - 0s - loss: 0.3952 - accuracy: 0.8657 - val_loss: 0.3532 - val_accuracy: 0.8507\n",
      "Epoch 133/210\n",
      "67/67 - 0s - loss: 0.4084 - accuracy: 0.8209 - val_loss: 0.3520 - val_accuracy: 0.8507\n",
      "Epoch 134/210\n",
      "67/67 - 0s - loss: 0.4157 - accuracy: 0.8582 - val_loss: 0.3496 - val_accuracy: 0.8507\n",
      "Epoch 135/210\n",
      "67/67 - 0s - loss: 0.3836 - accuracy: 0.8284 - val_loss: 0.3489 - val_accuracy: 0.8507\n",
      "Epoch 136/210\n",
      "67/67 - 0s - loss: 0.4059 - accuracy: 0.8209 - val_loss: 0.3495 - val_accuracy: 0.8507\n",
      "Epoch 137/210\n",
      "67/67 - 0s - loss: 0.3863 - accuracy: 0.8284 - val_loss: 0.3532 - val_accuracy: 0.8582\n",
      "Epoch 138/210\n",
      "67/67 - 0s - loss: 0.3461 - accuracy: 0.8806 - val_loss: 0.3558 - val_accuracy: 0.8507\n",
      "Epoch 139/210\n",
      "67/67 - 0s - loss: 0.3565 - accuracy: 0.8582 - val_loss: 0.3564 - val_accuracy: 0.8507\n",
      "Epoch 140/210\n",
      "67/67 - 0s - loss: 0.4333 - accuracy: 0.8433 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 141/210\n",
      "67/67 - 0s - loss: 0.3952 - accuracy: 0.8507 - val_loss: 0.3544 - val_accuracy: 0.8507\n",
      "Epoch 142/210\n",
      "67/67 - 0s - loss: 0.3896 - accuracy: 0.8582 - val_loss: 0.3558 - val_accuracy: 0.8507\n",
      "Epoch 143/210\n",
      "67/67 - 0s - loss: 0.3537 - accuracy: 0.8582 - val_loss: 0.3547 - val_accuracy: 0.8433\n",
      "Epoch 144/210\n",
      "67/67 - 0s - loss: 0.4372 - accuracy: 0.8284 - val_loss: 0.3543 - val_accuracy: 0.8433\n",
      "Epoch 145/210\n",
      "67/67 - 0s - loss: 0.3861 - accuracy: 0.8433 - val_loss: 0.3527 - val_accuracy: 0.8433\n",
      "Epoch 146/210\n",
      "67/67 - 0s - loss: 0.4051 - accuracy: 0.8433 - val_loss: 0.3510 - val_accuracy: 0.8507\n",
      "Epoch 147/210\n",
      "67/67 - 0s - loss: 0.3876 - accuracy: 0.8731 - val_loss: 0.3511 - val_accuracy: 0.8358\n",
      "Epoch 148/210\n",
      "67/67 - 0s - loss: 0.4506 - accuracy: 0.8433 - val_loss: 0.3500 - val_accuracy: 0.8433\n",
      "Epoch 149/210\n",
      "67/67 - 0s - loss: 0.4489 - accuracy: 0.8358 - val_loss: 0.3499 - val_accuracy: 0.8507\n",
      "Epoch 150/210\n",
      "67/67 - 0s - loss: 0.4183 - accuracy: 0.8358 - val_loss: 0.3507 - val_accuracy: 0.8433\n",
      "Epoch 151/210\n",
      "67/67 - 0s - loss: 0.3518 - accuracy: 0.8507 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 152/210\n",
      "67/67 - 0s - loss: 0.4264 - accuracy: 0.8433 - val_loss: 0.3559 - val_accuracy: 0.8507\n",
      "Epoch 153/210\n",
      "67/67 - 0s - loss: 0.3865 - accuracy: 0.8358 - val_loss: 0.3546 - val_accuracy: 0.8507\n",
      "Epoch 154/210\n",
      "67/67 - 0s - loss: 0.3904 - accuracy: 0.8433 - val_loss: 0.3556 - val_accuracy: 0.8507\n",
      "Epoch 155/210\n",
      "67/67 - 0s - loss: 0.3938 - accuracy: 0.8507 - val_loss: 0.3534 - val_accuracy: 0.8433\n",
      "Epoch 156/210\n",
      "67/67 - 0s - loss: 0.3875 - accuracy: 0.8657 - val_loss: 0.3506 - val_accuracy: 0.8358\n",
      "Epoch 157/210\n",
      "67/67 - 0s - loss: 0.3701 - accuracy: 0.8582 - val_loss: 0.3504 - val_accuracy: 0.8358\n",
      "Epoch 158/210\n",
      "67/67 - 0s - loss: 0.4598 - accuracy: 0.8060 - val_loss: 0.3483 - val_accuracy: 0.8358\n",
      "Epoch 159/210\n",
      "67/67 - 0s - loss: 0.3694 - accuracy: 0.8582 - val_loss: 0.3477 - val_accuracy: 0.8507\n",
      "Epoch 160/210\n",
      "67/67 - 0s - loss: 0.3594 - accuracy: 0.8507 - val_loss: 0.3484 - val_accuracy: 0.8507\n",
      "Epoch 161/210\n",
      "67/67 - 0s - loss: 0.3925 - accuracy: 0.8433 - val_loss: 0.3486 - val_accuracy: 0.8507\n",
      "Epoch 162/210\n",
      "67/67 - 0s - loss: 0.4264 - accuracy: 0.8582 - val_loss: 0.3491 - val_accuracy: 0.8507\n",
      "Epoch 163/210\n",
      "67/67 - 0s - loss: 0.4537 - accuracy: 0.8358 - val_loss: 0.3493 - val_accuracy: 0.8433\n",
      "Epoch 164/210\n",
      "67/67 - 0s - loss: 0.4784 - accuracy: 0.8358 - val_loss: 0.3498 - val_accuracy: 0.8433\n",
      "Epoch 165/210\n",
      "67/67 - 0s - loss: 0.4022 - accuracy: 0.8433 - val_loss: 0.3493 - val_accuracy: 0.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/210\n",
      "67/67 - 0s - loss: 0.3971 - accuracy: 0.8806 - val_loss: 0.3504 - val_accuracy: 0.8507\n",
      "Epoch 167/210\n",
      "67/67 - 0s - loss: 0.4817 - accuracy: 0.8060 - val_loss: 0.3485 - val_accuracy: 0.8582\n",
      "Epoch 168/210\n",
      "67/67 - 0s - loss: 0.3984 - accuracy: 0.8358 - val_loss: 0.3485 - val_accuracy: 0.8582\n",
      "Epoch 169/210\n",
      "67/67 - 0s - loss: 0.3367 - accuracy: 0.8582 - val_loss: 0.3507 - val_accuracy: 0.8507\n",
      "Epoch 170/210\n",
      "67/67 - 0s - loss: 0.3617 - accuracy: 0.8582 - val_loss: 0.3509 - val_accuracy: 0.8507\n",
      "Epoch 171/210\n",
      "67/67 - 0s - loss: 0.4461 - accuracy: 0.8209 - val_loss: 0.3496 - val_accuracy: 0.8507\n",
      "Epoch 172/210\n",
      "67/67 - 0s - loss: 0.4307 - accuracy: 0.7985 - val_loss: 0.3460 - val_accuracy: 0.8507\n",
      "Epoch 173/210\n",
      "67/67 - 0s - loss: 0.4168 - accuracy: 0.8358 - val_loss: 0.3459 - val_accuracy: 0.8507\n",
      "Epoch 174/210\n",
      "67/67 - 0s - loss: 0.3299 - accuracy: 0.8731 - val_loss: 0.3475 - val_accuracy: 0.8433\n",
      "Epoch 175/210\n",
      "67/67 - 0s - loss: 0.4272 - accuracy: 0.8433 - val_loss: 0.3464 - val_accuracy: 0.8433\n",
      "Epoch 176/210\n",
      "67/67 - 0s - loss: 0.3724 - accuracy: 0.8507 - val_loss: 0.3480 - val_accuracy: 0.8582\n",
      "Epoch 177/210\n",
      "67/67 - 0s - loss: 0.4027 - accuracy: 0.8209 - val_loss: 0.3497 - val_accuracy: 0.8582\n",
      "Epoch 178/210\n",
      "67/67 - 0s - loss: 0.3772 - accuracy: 0.8134 - val_loss: 0.3499 - val_accuracy: 0.8582\n",
      "Epoch 179/210\n",
      "67/67 - 0s - loss: 0.3755 - accuracy: 0.8433 - val_loss: 0.3505 - val_accuracy: 0.8507\n",
      "Epoch 180/210\n",
      "67/67 - 0s - loss: 0.4154 - accuracy: 0.8433 - val_loss: 0.3501 - val_accuracy: 0.8507\n",
      "Epoch 181/210\n",
      "67/67 - 0s - loss: 0.4549 - accuracy: 0.8358 - val_loss: 0.3510 - val_accuracy: 0.8582\n",
      "Epoch 182/210\n",
      "67/67 - 0s - loss: 0.3259 - accuracy: 0.8284 - val_loss: 0.3549 - val_accuracy: 0.8507\n",
      "Epoch 183/210\n",
      "67/67 - 0s - loss: 0.4290 - accuracy: 0.8433 - val_loss: 0.3522 - val_accuracy: 0.8433\n",
      "Epoch 184/210\n",
      "67/67 - 0s - loss: 0.4008 - accuracy: 0.8209 - val_loss: 0.3523 - val_accuracy: 0.8433\n",
      "Epoch 185/210\n",
      "67/67 - 0s - loss: 0.4332 - accuracy: 0.8433 - val_loss: 0.3502 - val_accuracy: 0.8433\n",
      "Epoch 186/210\n",
      "67/67 - 0s - loss: 0.3779 - accuracy: 0.8358 - val_loss: 0.3516 - val_accuracy: 0.8358\n",
      "Epoch 187/210\n",
      "67/67 - 0s - loss: 0.3928 - accuracy: 0.8582 - val_loss: 0.3493 - val_accuracy: 0.8582\n",
      "Epoch 188/210\n",
      "67/67 - 0s - loss: 0.3880 - accuracy: 0.8433 - val_loss: 0.3505 - val_accuracy: 0.8582\n",
      "Epoch 189/210\n",
      "67/67 - 0s - loss: 0.3701 - accuracy: 0.8507 - val_loss: 0.3528 - val_accuracy: 0.8507\n",
      "Epoch 190/210\n",
      "67/67 - 0s - loss: 0.3801 - accuracy: 0.8657 - val_loss: 0.3502 - val_accuracy: 0.8507\n",
      "Epoch 191/210\n",
      "67/67 - 0s - loss: 0.3917 - accuracy: 0.8657 - val_loss: 0.3507 - val_accuracy: 0.8507\n",
      "Epoch 192/210\n",
      "67/67 - 0s - loss: 0.4094 - accuracy: 0.8657 - val_loss: 0.3487 - val_accuracy: 0.8507\n",
      "Epoch 193/210\n",
      "67/67 - 0s - loss: 0.3987 - accuracy: 0.8582 - val_loss: 0.3467 - val_accuracy: 0.8433\n",
      "Epoch 194/210\n",
      "67/67 - 0s - loss: 0.4401 - accuracy: 0.8507 - val_loss: 0.3468 - val_accuracy: 0.8433\n",
      "Epoch 195/210\n",
      "67/67 - 0s - loss: 0.3571 - accuracy: 0.8582 - val_loss: 0.3491 - val_accuracy: 0.8507\n",
      "Epoch 196/210\n",
      "67/67 - 0s - loss: 0.3460 - accuracy: 0.8582 - val_loss: 0.3495 - val_accuracy: 0.8507\n",
      "Epoch 197/210\n",
      "67/67 - 0s - loss: 0.3802 - accuracy: 0.8507 - val_loss: 0.3505 - val_accuracy: 0.8582\n",
      "Epoch 198/210\n",
      "67/67 - 0s - loss: 0.3844 - accuracy: 0.8358 - val_loss: 0.3499 - val_accuracy: 0.8582\n",
      "Epoch 199/210\n",
      "67/67 - 0s - loss: 0.3635 - accuracy: 0.8657 - val_loss: 0.3525 - val_accuracy: 0.8582\n",
      "Epoch 200/210\n",
      "67/67 - 0s - loss: 0.3938 - accuracy: 0.8358 - val_loss: 0.3529 - val_accuracy: 0.8507\n",
      "Epoch 201/210\n",
      "67/67 - 0s - loss: 0.4024 - accuracy: 0.8433 - val_loss: 0.3528 - val_accuracy: 0.8507\n",
      "Epoch 202/210\n",
      "67/67 - 0s - loss: 0.4419 - accuracy: 0.8358 - val_loss: 0.3519 - val_accuracy: 0.8582\n",
      "Epoch 203/210\n",
      "67/67 - 0s - loss: 0.4091 - accuracy: 0.8060 - val_loss: 0.3477 - val_accuracy: 0.8582\n",
      "Epoch 204/210\n",
      "67/67 - 0s - loss: 0.4210 - accuracy: 0.8657 - val_loss: 0.3459 - val_accuracy: 0.8582\n",
      "Epoch 205/210\n",
      "67/67 - 0s - loss: 0.3491 - accuracy: 0.8657 - val_loss: 0.3468 - val_accuracy: 0.8582\n",
      "Epoch 206/210\n",
      "67/67 - 0s - loss: 0.4380 - accuracy: 0.8284 - val_loss: 0.3451 - val_accuracy: 0.8582\n",
      "Epoch 207/210\n",
      "67/67 - 0s - loss: 0.4074 - accuracy: 0.8433 - val_loss: 0.3447 - val_accuracy: 0.8582\n",
      "Epoch 208/210\n",
      "67/67 - 0s - loss: 0.3699 - accuracy: 0.8657 - val_loss: 0.3446 - val_accuracy: 0.8582\n",
      "Epoch 209/210\n",
      "67/67 - 0s - loss: 0.3524 - accuracy: 0.8582 - val_loss: 0.3460 - val_accuracy: 0.8582\n",
      "Epoch 210/210\n",
      "67/67 - 0s - loss: 0.3199 - accuracy: 0.8657 - val_loss: 0.3491 - val_accuracy: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2aef51f10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_p.fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d0e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6220139966875858\n",
      "0.6479923129672116\n",
      "0.7607795429345929\n",
      "0.8266045548654244\n",
      "train accuracy score= 0.771\n",
      "test accuracy score= 0.851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, f1_score, accuracy_score\n",
    "y_pred_train2 = np.round(ann_p.predict(X_train),0)\n",
    "y_pred_test2 = np.round(ann_p.predict(X_test),0)\n",
    "precision_test = average_precision_score(y_test, y_pred_test2)\n",
    "precision_train = average_precision_score(y_train, y_pred_train2)\n",
    "print(precision_train)\n",
    "print(precision_test)\n",
    "f1_train = f1_score(y_train, y_pred_train2, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test2, average='macro')\n",
    "print(f1_train)\n",
    "print(f1_test)\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_train2)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_test2)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "981dcffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744360902255639\n",
      "0.7435897435897436\n",
      "train f1 score= 0.781\n",
      "test f1 score= 0.804\n",
      "train accuracy score= 0.795\n",
      "test accuracy score= 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-9293d8fe7273>:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clfrf.fit(X_train,y_train)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a svm Classifier#\n",
    "clfrf=RandomForestClassifier(n_estimators=12, criterion=\"entropy\", max_depth=None,\n",
    "    min_samples_split=11, min_samples_leaf=1, min_weight_fraction_leaf=0.01, max_features='auto', max_leaf_nodes=10,\n",
    "    min_impurity_decrease=0.035, min_impurity_split=0.1, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "    verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.04, max_samples=70, random_state = 262)\n",
    "\n",
    "# Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clfrf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred_rf_train = clfrf.predict(X_train)\n",
    "y_pred_rf_test = clfrf.predict(X_test)\n",
    "print(metrics.precision_score(y_train, y_pred_rf_train))\n",
    "print(metrics.precision_score(y_test, y_pred_rf_test))\n",
    "f1_train = f1_score(y_train, y_pred_rf_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_rf_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_rf_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_rf_test)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e35b2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883\n",
      "0.833\n",
      "train f1 score= 0.819\n",
      "test f1 score= 0.799\n",
      "train accuracy score= 0.835\n",
      "test accuracy score= 0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf', degree =3, gamma = 0.34, C = 100, max_iter=-1, random_state = 262) \n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm_train = clf.predict(X_train)\n",
    "y_pred_svm_test = clf.predict(X_test)\n",
    "print(np.round(metrics.precision_score(y_train, y_pred_svm_train),3))\n",
    "print(np.round(metrics.precision_score(y_test, y_pred_svm_test),3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_svm_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_svm_test)\n",
    "f1_train = f1_score(y_train, y_pred_svm_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_svm_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1baabd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7481481481481481\n",
      "0.7045454545454546\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf2 = svm.SVC(kernel='linear', degree = 3, gamma = 0.01, C = 1000, random_state = 262) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm2_train = clf2.predict(X_train)\n",
    "y_pred_svm2_test = clf2.predict(X_test)\n",
    "print(metrics.precision_score(y_train, y_pred_svm2_train))\n",
    "print(metrics.precision_score(y_test, y_pred_svm2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61603581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:32:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "train precision= 0.909\n",
      "test precision= 0.889\n",
      "train f1 score= 0.792\n",
      "test f1 score= 0.803\n",
      "train accuracy score= 0.816\n",
      "test accuracy score= 0.851\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "xgb = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='dart', eta=0.25, max_depth=5, min_child_weight=10,\n",
    "                    max_delta_step=0.7, subsample=0.7, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"exact\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=1, n_estimators=9, rate_drop=\"0.2\", skip_drop=\"0.2\",\n",
    "                    random_state = 262)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "print(\"train precision=\", np.round(metrics.precision_score(y_train, y_pred_xgb_train),3))\n",
    "print(\"test precision=\", np.round(metrics.precision_score(y_test, y_pred_xgb_test),3))\n",
    "f1_train = f1_score(y_train, y_pred_xgb_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_xgb_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_xgb_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_xgb_test)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8a7138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19072308 0.06691053 0.04166646 0.06552701 0.6351729 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(xgb.feature_importances_)\n",
    "import matplotlib.pyplot as mp\n",
    "mp.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\n",
    "mp.show()\n",
    "\n",
    "# plot feature importance\n",
    "from xgboost import plot_importance\n",
    "plot_importance(xgb)\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40f971e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                          Name     Sex  \\\n",
       "PassengerId                                                                 \n",
       "892               3                              Kelly, Mr. James    male   \n",
       "893               3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "894               2                     Myles, Mr. Thomas Francis    male   \n",
       "895               3                              Wirz, Mr. Albert    male   \n",
       "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "...             ...                                           ...     ...   \n",
       "1305              3                            Spector, Mr. Woolf    male   \n",
       "1306              1                  Oliva y Ocana, Dona. Fermina  female   \n",
       "1307              3                  Saether, Mr. Simon Sivertsen    male   \n",
       "1308              3                           Ware, Mr. Frederick    male   \n",
       "1309              3                      Peter, Master. Michael J    male   \n",
       "\n",
       "              Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \n",
       "PassengerId                                                                   \n",
       "892          34.5      0      0              330911    7.8292   NaN        Q  \n",
       "893          47.0      1      0              363272    7.0000   NaN        S  \n",
       "894          62.0      0      0              240276    9.6875   NaN        Q  \n",
       "895          27.0      0      0              315154    8.6625   NaN        S  \n",
       "896          22.0      1      1             3101298   12.2875   NaN        S  \n",
       "...           ...    ...    ...                 ...       ...   ...      ...  \n",
       "1305          NaN      0      0           A.5. 3236    8.0500   NaN        S  \n",
       "1306         39.0      0      0            PC 17758  108.9000  C105        C  \n",
       "1307         38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "1308          NaN      0      0              359309    8.0500   NaN        S  \n",
       "1309          NaN      1      1                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv(\"test.csv\", index_col=0)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "babe5243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                          Name  Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "892               3                              Kelly, Mr. James    1  34.5   \n",
       "893               3              Wilkes, Mrs. James (Ellen Needs)    0  47.0   \n",
       "894               2                     Myles, Mr. Thomas Francis    1  62.0   \n",
       "895               3                              Wirz, Mr. Albert    1  27.0   \n",
       "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)    0  22.0   \n",
       "...             ...                                           ...  ...   ...   \n",
       "1305              3                            Spector, Mr. Woolf    1   NaN   \n",
       "1306              1                  Oliva y Ocana, Dona. Fermina    0  39.0   \n",
       "1307              3                  Saether, Mr. Simon Sivertsen    1  38.5   \n",
       "1308              3                           Ware, Mr. Frederick    1   NaN   \n",
       "1309              3                      Peter, Master. Michael J    1   NaN   \n",
       "\n",
       "             SibSp  Parch              Ticket      Fare Cabin  Embarked  \n",
       "PassengerId                                                              \n",
       "892              0      0              330911    7.8292   NaN         1  \n",
       "893              1      0              363272    7.0000   NaN         2  \n",
       "894              0      0              240276    9.6875   NaN         1  \n",
       "895              0      0              315154    8.6625   NaN         2  \n",
       "896              1      1             3101298   12.2875   NaN         2  \n",
       "...            ...    ...                 ...       ...   ...       ...  \n",
       "1305             0      0           A.5. 3236    8.0500   NaN         2  \n",
       "1306             0      0            PC 17758  108.9000  C105         0  \n",
       "1307             0      0  SOTON/O.Q. 3101262    7.2500   NaN         2  \n",
       "1308             0      0              359309    8.0500   NaN         2  \n",
       "1309             1      1                2668   22.3583   NaN         0  \n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "dft['Embarked'] = le.fit_transform(dft['Embarked'])\n",
    "dft['Sex'] = le.fit_transform(dft['Sex'])\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fe056c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.2250</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82.2667</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61.1750</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Name  Sex   Age  SibSp  Parch  Ticket     Fare  Cabin  \\\n",
       "PassengerId                                                                  \n",
       "892               3     1    1  34.5      0      0       1   7.8292      1   \n",
       "893               3     2    0  47.0      1      0       2   7.0000      2   \n",
       "894               2     1    1  62.0      0      0       1   9.6875      1   \n",
       "895               3     2    1  27.0      0      0       2   8.6625      2   \n",
       "896               3     2    0  22.0      1      1       2  12.2875      2   \n",
       "897               3     2    1  14.0      0      0       2   9.2250      2   \n",
       "898               3     1    0  30.0      0      0       1   7.6292      1   \n",
       "899               2     2    1  26.0      1      1       2  29.0000      2   \n",
       "900               3     0    0  18.0      0      0       0   7.2292      0   \n",
       "901               3     2    1  21.0      2      0       2  24.1500      2   \n",
       "902               3     2    1   NaN      0      0       2   7.8958      2   \n",
       "903               1     2    1  46.0      0      0       2  26.0000      2   \n",
       "904               1     2    0  23.0      1      0       2  82.2667      2   \n",
       "905               2     2    1  63.0      1      0       2  26.0000      2   \n",
       "906               1     2    0  47.0      1      0       2  61.1750      2   \n",
       "\n",
       "             Embarked  \n",
       "PassengerId            \n",
       "892                 1  \n",
       "893                 2  \n",
       "894                 1  \n",
       "895                 2  \n",
       "896                 2  \n",
       "897                 2  \n",
       "898                 1  \n",
       "899                 2  \n",
       "900                 0  \n",
       "901                 2  \n",
       "902                 2  \n",
       "903                 2  \n",
       "904                 2  \n",
       "905                 2  \n",
       "906                 2  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = dft.apply(lambda col: LabelEncoder().fit_transform(dft[\"Embarked\"]) if col.dtype == \"object\" else col)\n",
    "dft.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b84651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 87\n",
      "Missing: 0\n",
      "Mean Accuracy: 1.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation transform for the horse colic dataset\n",
    "from numpy import isnan, mean, std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dft1 = dft.drop(columns={\"Name\",\"Cabin\",\"Ticket\"})\n",
    "# split into input and output elements\n",
    "data = dft1.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 6]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "#define modeling pipeline\n",
    "model = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='gbtree', eta=0.3, max_depth=4, min_child_weight=20,\n",
    "                    max_delta_step=0.5, subsample=0.6, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"approx\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=0, n_estimators=10, rate_drop=\"0.01\", skip_drop=\"0.8\",\n",
    "                    random_state = 262)# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "dft1 = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(dft1).flatten()))\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "793ecd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 87\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KNNImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e744b0b35cb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# define imputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# fit on the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KNNImputer' is not defined"
     ]
    }
   ],
   "source": [
    "dft1 = dft.drop(columns={\"Name\",\"Cabin\",\"Ticket\"})\n",
    "# split into input and output elements\n",
    "data = dft1.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 6]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "dft1 = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(dft1).flatten()))\n",
    "dft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f91ce20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.396993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1          2    3    4         5    6\n",
       "0    3.0  1.0  34.500000  0.0  0.0    7.8292  1.0\n",
       "1    3.0  0.0  47.000000  1.0  0.0    7.0000  2.0\n",
       "2    2.0  1.0  62.000000  0.0  0.0    9.6875  1.0\n",
       "3    3.0  1.0  27.000000  0.0  0.0    8.6625  2.0\n",
       "4    3.0  0.0  22.000000  1.0  1.0   12.2875  2.0\n",
       "..   ...  ...        ...  ...  ...       ...  ...\n",
       "413  3.0  1.0  25.224941  0.0  0.0    8.0500  2.0\n",
       "414  1.0  0.0  39.000000  0.0  0.0  108.9000  0.0\n",
       "415  3.0  1.0  38.500000  0.0  0.0    7.2500  2.0\n",
       "416  3.0  1.0  25.224941  0.0  0.0    8.0500  2.0\n",
       "417  3.0  1.0  23.396993  1.0  1.0   22.3583  0.0\n",
       "\n",
       "[418 rows x 7 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft1 = pd.DataFrame(dft1)\n",
    "dft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      0\n",
      "Age         0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "Sex         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.396993</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age      Fare  Embarked  Sex\n",
       "0       3.0  34.500000    7.8292       1.0  1.0\n",
       "1       3.0  47.000000    7.0000       2.0  0.0\n",
       "2       2.0  62.000000    9.6875       1.0  1.0\n",
       "3       3.0  27.000000    8.6625       2.0  1.0\n",
       "4       3.0  22.000000   12.2875       2.0  0.0\n",
       "..      ...        ...       ...       ...  ...\n",
       "413     3.0  25.224941    8.0500       2.0  1.0\n",
       "414     1.0  39.000000  108.9000       0.0  0.0\n",
       "415     3.0  38.500000    7.2500       2.0  1.0\n",
       "416     3.0  25.224941    8.0500       2.0  1.0\n",
       "417     3.0  23.396993   22.3583       0.0  1.0\n",
       "\n",
       "[418 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft1.columns = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]\n",
    "dftnew = dft1[[\"Pclass\",\"Age\",\"Fare\",\"Embarked\",\"Sex\"]]\n",
    "print(dftnew.isnull().sum())\n",
    "dftnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab82d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 0.90544639, 0.03056316, 1.        , 2.        ],\n",
       "       [2.        , 1.23513121, 0.02732618, 2.        , 0.        ],\n",
       "       [1.        , 1.630753  , 0.03781748, 1.        , 2.        ],\n",
       "       ...,\n",
       "       [2.        , 1.01094554, 0.02830212, 2.        , 2.        ],\n",
       "       [2.        , 0.66081869, 0.03142511, 2.        , 2.        ],\n",
       "       [2.        , 0.61260696, 0.08728099, 0.        , 2.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftnew1 = mmsc.fit_transform(dftnew)\n",
    "dftnew1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b51e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = pd.DataFrame(np.round(xgb.predict(dftnew1),0))\n",
    "y_pred_xgb.to_csv(\"xgb_xgbimpute_fs2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6b8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
