{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4348df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80b9c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
      "       'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "PassengerId                                                                \n",
       "1                   0       3    male  22.0      1      0   7.2500        S\n",
       "2                   1       1  female  38.0      1      0  71.2833        C\n",
       "3                   1       3  female  26.0      0      0   7.9250        S\n",
       "4                   1       1  female  35.0      1      0  53.1000        S\n",
       "5                   0       3    male  35.0      0      0   8.0500        S\n",
       "...               ...     ...     ...   ...    ...    ...      ...      ...\n",
       "887                 0       2    male  27.0      0      0  13.0000        S\n",
       "888                 1       1  female  19.0      0      0  30.0000        S\n",
       "889                 0       3  female   NaN      1      2  23.4500        S\n",
       "890                 1       1    male  26.0      0      0  30.0000        C\n",
       "891                 0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "print(df.columns)\n",
    "df = df.drop([\"Cabin\",\"Name\",\"Ticket\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a2060a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare Embarked  Sex_M\n",
       "PassengerId                                                               \n",
       "1                   0       3  22.0      1      0   7.2500        S      1\n",
       "2                   1       1  38.0      1      0  71.2833        C      0\n",
       "3                   1       3  26.0      0      0   7.9250        S      0\n",
       "4                   1       1  35.0      1      0  53.1000        S      0\n",
       "5                   0       3  35.0      0      0   8.0500        S      1\n",
       "...               ...     ...   ...    ...    ...      ...      ...    ...\n",
       "887                 0       2  27.0      0      0  13.0000        S      1\n",
       "888                 1       1  19.0      0      0  30.0000        S      0\n",
       "889                 0       3   NaN      1      2  23.4500        S      0\n",
       "890                 1       1  26.0      0      0  30.0000        C      1\n",
       "891                 0       3  32.0      0      0   7.7500        Q      1\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[\"Sex\"]\n",
    "df_dummy = pd.get_dummies(df1,drop_first=True)\n",
    "df[\"Sex_M\"] = df_dummy #Male is one\n",
    "df = df.drop(columns=[\"Sex\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "610af723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      2\n",
       "Sex_M         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d424dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare  Embarked  Sex_M\n",
       "PassengerId                                                                \n",
       "1                   0       3  22.0      1      0   7.2500         2      1\n",
       "2                   1       1  38.0      1      0  71.2833         0      0\n",
       "3                   1       3  26.0      0      0   7.9250         2      0\n",
       "4                   1       1  35.0      1      0  53.1000         2      0\n",
       "5                   0       3  35.0      0      0   8.0500         2      1\n",
       "6                   0       3   NaN      0      0   8.4583         1      1\n",
       "7                   0       1  54.0      0      0  51.8625         2      1\n",
       "8                   0       3   2.0      3      1  21.0750         2      1\n",
       "9                   1       3  27.0      0      2  11.1333         2      0\n",
       "10                  1       2  14.0      1      0  30.0708         0      0\n",
       "11                  1       3   4.0      1      1  16.7000         2      0\n",
       "12                  1       1  58.0      0      0  26.5500         2      0\n",
       "13                  0       3  20.0      0      0   8.0500         2      1\n",
       "14                  0       3  39.0      1      5  31.2750         2      1\n",
       "15                  0       3  14.0      0      0   7.8542         2      0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = df.apply(lambda col: LabelEncoder().fit_transform(df[\"Embarked\"]) if col.dtype == \"object\" else col)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c965c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      0\n",
       "Sex_M         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337f683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 177\n",
      "Missing: 0\n",
      "Mean Accuracy: 1.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation transform for the horse colic dataset\n",
    "from numpy import isnan, mean, std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "# load dataset\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 7]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "#define modeling pipeline\n",
    "## model = svm.SVC(kernel='rbf', degree =3, gamma = 0.01, C = 100, max_iter=-1, random_state = 262)\n",
    "model = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='gbtree', eta=0.3, max_depth=4, min_child_weight=20,\n",
    "                    max_delta_step=0.5, subsample=0.6, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"approx\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=0, n_estimators=10, rate_drop=\"0.01\", skip_drop=\"0.8\",\n",
    "                    random_state = 262)\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "df = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(df).flatten()))\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN imputer\n",
    "from numpy import isnan\n",
    "from sklearn.impute import KNNImputer\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 7]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "df = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(df).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5f04ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.388746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass        Age  SibSp  Parch     Fare  Embarked  Sex_M\n",
       "0         0.0     3.0  22.000000    1.0    0.0   7.2500       2.0    1.0\n",
       "1         1.0     1.0  38.000000    1.0    0.0  71.2833       0.0    0.0\n",
       "2         1.0     3.0  26.000000    0.0    0.0   7.9250       2.0    0.0\n",
       "3         1.0     1.0  35.000000    1.0    0.0  53.1000       2.0    0.0\n",
       "4         0.0     3.0  35.000000    0.0    0.0   8.0500       2.0    1.0\n",
       "..        ...     ...        ...    ...    ...      ...       ...    ...\n",
       "886       0.0     2.0  27.000000    0.0    0.0  13.0000       2.0    1.0\n",
       "887       1.0     1.0  19.000000    0.0    0.0  30.0000       2.0    0.0\n",
       "888       0.0     3.0  23.388746    1.0    2.0  23.4500       2.0    0.0\n",
       "889       1.0     1.0  26.000000    0.0    0.0  30.0000       0.0    1.0\n",
       "890       0.0     3.0  32.000000    0.0    0.0   7.7500       1.0    1.0\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = [\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Sex_M\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74522df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    0\n",
       "Pclass      0\n",
       "Age         0\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Embarked    0\n",
       "Sex_M       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72d5199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>3</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>3</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass     Fare  Embarked  Sex_M\n",
       "PassengerId                                  \n",
       "1                 3   7.2500         2      1\n",
       "2                 1  71.2833         0      0\n",
       "3                 3   7.9250         2      0\n",
       "4                 1  53.1000         2      0\n",
       "5                 3   8.0500         2      1\n",
       "...             ...      ...       ...    ...\n",
       "887               2  13.0000         2      1\n",
       "888               1  30.0000         2      0\n",
       "889               3  23.4500         2      0\n",
       "890               1  30.0000         0      1\n",
       "891               3   7.7500         1      1\n",
       "\n",
       "[891 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[\"Pclass\",\"Fare\",\"Embarked\",\"Sex_M\"]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed7d2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "1                   0\n",
       "2                   1\n",
       "3                   1\n",
       "4                   1\n",
       "5                   0\n",
       "...               ...\n",
       "887                 0\n",
       "888                 1\n",
       "889                 0\n",
       "890                 1\n",
       "891                 0\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[[\"Survived\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6073d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1</td>\n",
       "      <td>27.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>3</td>\n",
       "      <td>27.9000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "      <td>66.6000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>79.6500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass     Fare  Embarked  Sex_M\n",
       "PassengerId                                  \n",
       "260               2  26.0000         2      0\n",
       "124               2  13.0000         2      0\n",
       "453               1  27.7500         0      1\n",
       "643               3  27.9000         2      0\n",
       "337               1  66.6000         2      1\n",
       "...             ...      ...       ...    ...\n",
       "340               1  35.5000         2      1\n",
       "592               1  78.2667         0      0\n",
       "559               1  79.6500         2      0\n",
       "855               2  26.0000         2      0\n",
       "718               2  10.5000         2      0\n",
       "\n",
       "[757 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split dataset into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=262)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "668dc0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01546857, 0.66666667, 0.        ],\n",
       "       [1.        , 0.0179898 , 0.66666667, 1.        ],\n",
       "       [1.        , 0.02821272, 0.        , 1.        ],\n",
       "       [1.        , 0.03025399, 0.33333333, 1.        ],\n",
       "       [1.        , 0.13575256, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 1.        ],\n",
       "       [1.        , 0.03142511, 0.66666667, 1.        ],\n",
       "       [0.        , 0.05604307, 0.        , 0.        ],\n",
       "       [1.        , 0.018006  , 0.66666667, 1.        ],\n",
       "       [1.        , 0.01473662, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.05123659, 0.66666667, 1.        ],\n",
       "       [1.        , 0.06104473, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01537098, 0.66666667, 1.        ],\n",
       "       [0.        , 0.10149724, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.04006213, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01510259, 0.33333333, 1.        ],\n",
       "       [1.        , 0.13575256, 0.66666667, 1.        ],\n",
       "       [1.        , 0.02042144, 0.66666667, 0.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [0.        , 0.05060223, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.05074862, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 0.        ],\n",
       "       [0.        , 0.10149724, 0.66666667, 0.        ],\n",
       "       [0.        , 0.16293235, 0.66666667, 0.        ],\n",
       "       [0.5       , 0.03806147, 0.66666667, 0.        ],\n",
       "       [0.        , 0.1545881 , 0.        , 1.        ],\n",
       "       [0.        , 0.1036443 , 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01517579, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01854277, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01517579, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01411046, 0.        , 1.        ],\n",
       "       [1.        , 0.01415106, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02635025, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.11027246, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01410226, 0.        , 1.        ],\n",
       "       [1.        , 0.07746484, 0.66666667, 1.        ],\n",
       "       [1.        , 0.03142511, 0.66666667, 1.        ],\n",
       "       [1.        , 0.03025399, 0.33333333, 1.        ],\n",
       "       [1.        , 0.01824998, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01512699, 0.33333333, 1.        ],\n",
       "       [0.        , 0.29953885, 0.66666667, 0.        ],\n",
       "       [1.        , 0.03142511, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01463083, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01517579, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.04098927, 0.66666667, 1.        ],\n",
       "       [1.        , 0.02434958, 0.66666667, 1.        ],\n",
       "       [1.        , 0.05445717, 0.66666667, 0.        ],\n",
       "       [0.        , 0.075147  , 0.66666667, 1.        ],\n",
       "       [0.        , 0.05074862, 0.66666667, 1.        ],\n",
       "       [0.        , 0.07831878, 0.        , 1.        ],\n",
       "       [0.5       , 0.08115719, 0.        , 0.        ],\n",
       "       [0.5       , 0.07173122, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01415106, 0.66666667, 1.        ],\n",
       "       [1.        , 0.02821272, 0.        , 0.        ],\n",
       "       [1.        , 0.04538098, 0.33333333, 0.        ],\n",
       "       [1.        , 0.01533038, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01512699, 0.33333333, 1.        ],\n",
       "       [1.        , 0.04015973, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01541158, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.05074862, 0.66666667, 1.        ],\n",
       "       [0.        , 0.06764049, 0.        , 1.        ],\n",
       "       [0.        , 0.1111184 , 0.        , 0.        ],\n",
       "       [1.        , 0.11027246, 0.66666667, 1.        ],\n",
       "       [0.        , 0.0975935 , 0.66666667, 1.        ],\n",
       "       [1.        , 0.01411046, 0.        , 1.        ],\n",
       "       [0.        , 0.063086  , 0.66666667, 1.        ],\n",
       "       [1.        , 0.06709553, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01541158, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.0270496 , 0.        , 0.        ],\n",
       "       [0.5       , 0.08115719, 0.        , 0.        ],\n",
       "       [1.        , 0.03513366, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01646071, 0.66666667, 1.        ],\n",
       "       [0.        , 0.1561496 , 1.        , 0.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [0.        , 0.44409922, 0.        , 0.        ],\n",
       "       [1.        , 0.01517579, 0.66666667, 1.        ],\n",
       "       [0.        , 0.05797054, 0.        , 1.        ],\n",
       "       [0.        , 0.41250333, 0.66666667, 0.        ],\n",
       "       [1.        , 0.03945217, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01546857, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01541158, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.04713766, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01632251, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02830212, 0.66666667, 0.        ],\n",
       "       [0.        , 0.10257897, 0.66666667, 0.        ],\n",
       "       [0.5       , 0.05074862, 0.66666667, 0.        ],\n",
       "       [1.        , 0.06126432, 0.66666667, 0.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [1.        , 0.02821272, 0.        , 0.        ],\n",
       "       [1.        , 0.01512699, 0.33333333, 0.        ],\n",
       "       [1.        , 0.01411046, 0.        , 1.        ],\n",
       "       [1.        , 0.03093714, 0.66666667, 0.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01528158, 0.33333333, 1.        ],\n",
       "       [1.        , 0.03259623, 0.66666667, 0.        ],\n",
       "       [0.        , 0.05130978, 0.66666667, 1.        ],\n",
       "       [1.        , 0.02822072, 0.        , 1.        ],\n",
       "       [1.        , 0.02822072, 0.        , 0.        ],\n",
       "       [1.        , 0.01546857, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02244651, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01512699, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01546857, 0.66666667, 1.        ],\n",
       "       [1.        , 0.03952537, 0.66666667, 0.        ],\n",
       "       [0.        , 0.16038672, 0.        , 1.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01512699, 0.33333333, 0.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 1.        ],\n",
       "       [1.        , 0.04970769, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01410226, 0.        , 0.        ],\n",
       "       [0.5       , 0.02342244, 0.        , 0.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01571255, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.04098927, 0.66666667, 0.        ],\n",
       "       [1.        , 0.01690807, 0.66666667, 1.        ],\n",
       "       [0.        , 0.06050797, 0.        , 0.        ],\n",
       "       [0.        , 0.29953885, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01541158, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02049464, 0.66666667, 1.        ],\n",
       "       [1.        , 0.01376888, 0.66666667, 1.        ],\n",
       "       [0.        , 0.        , 0.66666667, 1.        ],\n",
       "       [0.5       , 0.04098927, 0.66666667, 1.        ],\n",
       "       [1.        , 0.04713766, 0.33333333, 0.        ],\n",
       "       [1.        , 0.01541158, 0.66666667, 1.        ],\n",
       "       [0.5       , 0.02537431, 0.66666667, 0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "mmsc = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "X_train = mmsc.fit_transform(X_train)\n",
    "X_test = mmsc.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1776f27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98d09fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "76/76 [==============================] - 2s 1ms/step - loss: 0.6624 - accuracy: 0.5892\n",
      "Epoch 2/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.6436 - accuracy: 0.5984\n",
      "Epoch 3/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.6316 - accuracy: 0.5997\n",
      "Epoch 4/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.6188 - accuracy: 0.6024\n",
      "Epoch 5/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.6092 - accuracy: 0.6103\n",
      "Epoch 6/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.5965 - accuracy: 0.6486\n",
      "Epoch 7/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.5857 - accuracy: 0.6711\n",
      "Epoch 8/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.5761 - accuracy: 0.7120\n",
      "Epoch 9/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.5694 - accuracy: 0.7094\n",
      "Epoch 10/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.5521 - accuracy: 0.7332\n",
      "Epoch 11/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.5444 - accuracy: 0.7728\n",
      "Epoch 12/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.5462 - accuracy: 0.7741\n",
      "Epoch 13/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.5329 - accuracy: 0.7728\n",
      "Epoch 14/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.5302 - accuracy: 0.7794\n",
      "Epoch 15/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.5211 - accuracy: 0.7847\n",
      "Epoch 16/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.5157 - accuracy: 0.7768\n",
      "Epoch 17/200\n",
      "76/76 [==============================] - 0s 917us/step - loss: 0.5226 - accuracy: 0.7715\n",
      "Epoch 18/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.5136 - accuracy: 0.7715\n",
      "Epoch 19/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.5065 - accuracy: 0.7820\n",
      "Epoch 20/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4981 - accuracy: 0.7807\n",
      "Epoch 21/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.5071 - accuracy: 0.7781\n",
      "Epoch 22/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4974 - accuracy: 0.7926\n",
      "Epoch 23/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4972 - accuracy: 0.78200s - loss: 0.4878 - accuracy: 0.78\n",
      "Epoch 24/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4908 - accuracy: 0.78070s - loss: 0.4993 - accuracy: 0.77\n",
      "Epoch 25/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4966 - accuracy: 0.7860\n",
      "Epoch 26/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4889 - accuracy: 0.7873\n",
      "Epoch 27/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4911 - accuracy: 0.7873\n",
      "Epoch 28/200\n",
      "76/76 [==============================] - 0s 957us/step - loss: 0.4808 - accuracy: 0.7913\n",
      "Epoch 29/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4933 - accuracy: 0.77680s - loss: 0.4868 - accuracy: 0.78\n",
      "Epoch 30/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4929 - accuracy: 0.7728\n",
      "Epoch 31/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4980 - accuracy: 0.7860\n",
      "Epoch 32/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4933 - accuracy: 0.7609\n",
      "Epoch 33/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4809 - accuracy: 0.7847\n",
      "Epoch 34/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4931 - accuracy: 0.7754\n",
      "Epoch 35/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4881 - accuracy: 0.7675\n",
      "Epoch 36/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4845 - accuracy: 0.7847\n",
      "Epoch 37/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4857 - accuracy: 0.7847\n",
      "Epoch 38/200\n",
      "76/76 [==============================] - 0s 904us/step - loss: 0.4808 - accuracy: 0.7860\n",
      "Epoch 39/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4893 - accuracy: 0.7794\n",
      "Epoch 40/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4835 - accuracy: 0.7900\n",
      "Epoch 41/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4701 - accuracy: 0.7794\n",
      "Epoch 42/200\n",
      "76/76 [==============================] - 0s 731us/step - loss: 0.4710 - accuracy: 0.7979\n",
      "Epoch 43/200\n",
      "76/76 [==============================] - 0s 691us/step - loss: 0.4800 - accuracy: 0.7807\n",
      "Epoch 44/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4802 - accuracy: 0.7886\n",
      "Epoch 45/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4763 - accuracy: 0.7807\n",
      "Epoch 46/200\n",
      "76/76 [==============================] - 0s 718us/step - loss: 0.4711 - accuracy: 0.7926\n",
      "Epoch 47/200\n",
      "76/76 [==============================] - 0s 705us/step - loss: 0.4790 - accuracy: 0.7834\n",
      "Epoch 48/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4682 - accuracy: 0.7741\n",
      "Epoch 49/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4791 - accuracy: 0.7900\n",
      "Epoch 50/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4742 - accuracy: 0.7847\n",
      "Epoch 51/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4799 - accuracy: 0.7701\n",
      "Epoch 52/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4739 - accuracy: 0.7873\n",
      "Epoch 53/200\n",
      "76/76 [==============================] - 0s 930us/step - loss: 0.4743 - accuracy: 0.7768\n",
      "Epoch 54/200\n",
      "76/76 [==============================] - 0s 931us/step - loss: 0.4692 - accuracy: 0.7847\n",
      "Epoch 55/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4769 - accuracy: 0.7873\n",
      "Epoch 56/200\n",
      "76/76 [==============================] - 0s 773us/step - loss: 0.4698 - accuracy: 0.7966\n",
      "Epoch 57/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4700 - accuracy: 0.7900\n",
      "Epoch 58/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4639 - accuracy: 0.7886\n",
      "Epoch 59/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4720 - accuracy: 0.7900\n",
      "Epoch 60/200\n",
      "76/76 [==============================] - 0s 847us/step - loss: 0.4728 - accuracy: 0.7979\n",
      "Epoch 61/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4684 - accuracy: 0.7873\n",
      "Epoch 62/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4669 - accuracy: 0.79790s - loss: 0.4641 - accuracy: 0.80\n",
      "Epoch 63/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4650 - accuracy: 0.7900\n",
      "Epoch 64/200\n",
      "76/76 [==============================] - 0s 892us/step - loss: 0.4729 - accuracy: 0.7768\n",
      "Epoch 65/200\n",
      "76/76 [==============================] - 0s 931us/step - loss: 0.4635 - accuracy: 0.7952\n",
      "Epoch 66/200\n",
      "76/76 [==============================] - 0s 957us/step - loss: 0.4621 - accuracy: 0.8018\n",
      "Epoch 67/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4614 - accuracy: 0.7979\n",
      "Epoch 68/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4618 - accuracy: 0.7913\n",
      "Epoch 69/200\n",
      "76/76 [==============================] - 0s 812us/step - loss: 0.4664 - accuracy: 0.78200s - loss: 0.4704 - accuracy: 0.78\n",
      "Epoch 70/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4632 - accuracy: 0.7966\n",
      "Epoch 71/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4629 - accuracy: 0.7873\n",
      "Epoch 72/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4616 - accuracy: 0.7952\n",
      "Epoch 73/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4590 - accuracy: 0.7966\n",
      "Epoch 74/200\n",
      "76/76 [==============================] - 0s 904us/step - loss: 0.4619 - accuracy: 0.7834\n",
      "Epoch 75/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4652 - accuracy: 0.7847\n",
      "Epoch 76/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4639 - accuracy: 0.7834\n",
      "Epoch 77/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4626 - accuracy: 0.7886\n",
      "Epoch 78/200\n",
      "76/76 [==============================] - 0s 812us/step - loss: 0.4570 - accuracy: 0.7992\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 864us/step - loss: 0.4633 - accuracy: 0.8071\n",
      "Epoch 80/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4597 - accuracy: 0.7847\n",
      "Epoch 81/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4595 - accuracy: 0.7992\n",
      "Epoch 82/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4648 - accuracy: 0.7834\n",
      "Epoch 83/200\n",
      "76/76 [==============================] - 0s 784us/step - loss: 0.4671 - accuracy: 0.7873\n",
      "Epoch 84/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4540 - accuracy: 0.7926\n",
      "Epoch 85/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4578 - accuracy: 0.8045\n",
      "Epoch 86/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4596 - accuracy: 0.79660s - loss: 0.4516 - accuracy: 0.80\n",
      "Epoch 87/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4560 - accuracy: 0.7966\n",
      "Epoch 88/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4615 - accuracy: 0.7913\n",
      "Epoch 89/200\n",
      "76/76 [==============================] - 0s 781us/step - loss: 0.4544 - accuracy: 0.7900\n",
      "Epoch 90/200\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.78 - 0s 838us/step - loss: 0.4600 - accuracy: 0.7913\n",
      "Epoch 91/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4496 - accuracy: 0.7992\n",
      "Epoch 92/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4596 - accuracy: 0.7886\n",
      "Epoch 93/200\n",
      "76/76 [==============================] - 0s 984us/step - loss: 0.4572 - accuracy: 0.8032\n",
      "Epoch 94/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4658 - accuracy: 0.7794\n",
      "Epoch 95/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4584 - accuracy: 0.80050s - loss: 0.4603 - accuracy: 0.79\n",
      "Epoch 96/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4512 - accuracy: 0.7979\n",
      "Epoch 97/200\n",
      "76/76 [==============================] - 0s 811us/step - loss: 0.4507 - accuracy: 0.7992\n",
      "Epoch 98/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4535 - accuracy: 0.8032\n",
      "Epoch 99/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4494 - accuracy: 0.7939\n",
      "Epoch 100/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4553 - accuracy: 0.7992\n",
      "Epoch 101/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4538 - accuracy: 0.80320s - loss: 0.4577 - accuracy: 0.80\n",
      "Epoch 102/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4480 - accuracy: 0.8071\n",
      "Epoch 103/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4618 - accuracy: 0.8045\n",
      "Epoch 104/200\n",
      "76/76 [==============================] - 0s 904us/step - loss: 0.4595 - accuracy: 0.7952\n",
      "Epoch 105/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4558 - accuracy: 0.7992\n",
      "Epoch 106/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4427 - accuracy: 0.8058\n",
      "Epoch 107/200\n",
      "76/76 [==============================] - 0s 789us/step - loss: 0.4539 - accuracy: 0.8045\n",
      "Epoch 108/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4529 - accuracy: 0.8032\n",
      "Epoch 109/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4529 - accuracy: 0.8045\n",
      "Epoch 110/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4576 - accuracy: 0.7952\n",
      "Epoch 111/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4550 - accuracy: 0.8018\n",
      "Epoch 112/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4601 - accuracy: 0.7966\n",
      "Epoch 113/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4493 - accuracy: 0.7966\n",
      "Epoch 114/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4561 - accuracy: 0.7966\n",
      "Epoch 115/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4536 - accuracy: 0.80180s - loss: 0.4461 - accuracy: 0.80\n",
      "Epoch 116/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4522 - accuracy: 0.8045\n",
      "Epoch 117/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4505 - accuracy: 0.8124\n",
      "Epoch 118/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4540 - accuracy: 0.7966\n",
      "Epoch 119/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4616 - accuracy: 0.7847\n",
      "Epoch 120/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4636 - accuracy: 0.7926\n",
      "Epoch 121/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4492 - accuracy: 0.7992\n",
      "Epoch 122/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4529 - accuracy: 0.7992\n",
      "Epoch 123/200\n",
      "76/76 [==============================] - 0s 797us/step - loss: 0.4519 - accuracy: 0.7979\n",
      "Epoch 124/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4474 - accuracy: 0.7900\n",
      "Epoch 125/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4512 - accuracy: 0.8018\n",
      "Epoch 126/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4505 - accuracy: 0.8045\n",
      "Epoch 127/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4545 - accuracy: 0.8032\n",
      "Epoch 128/200\n",
      "76/76 [==============================] - 0s 931us/step - loss: 0.4532 - accuracy: 0.8005\n",
      "Epoch 129/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4534 - accuracy: 0.7926\n",
      "Epoch 130/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4485 - accuracy: 0.8151\n",
      "Epoch 131/200\n",
      "76/76 [==============================] - 0s 812us/step - loss: 0.4578 - accuracy: 0.7886\n",
      "Epoch 132/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4509 - accuracy: 0.7952\n",
      "Epoch 133/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4533 - accuracy: 0.7979\n",
      "Epoch 134/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4529 - accuracy: 0.7939\n",
      "Epoch 135/200\n",
      "76/76 [==============================] - 0s 852us/step - loss: 0.4584 - accuracy: 0.7952\n",
      "Epoch 136/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4484 - accuracy: 0.80710s - loss: 0.4482 - accuracy: 0.81\n",
      "Epoch 137/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4502 - accuracy: 0.7952\n",
      "Epoch 138/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4527 - accuracy: 0.7926\n",
      "Epoch 139/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4466 - accuracy: 0.7939\n",
      "Epoch 140/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4525 - accuracy: 0.8032\n",
      "Epoch 141/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4524 - accuracy: 0.8005\n",
      "Epoch 142/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4542 - accuracy: 0.7979\n",
      "Epoch 143/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4512 - accuracy: 0.7979\n",
      "Epoch 144/200\n",
      "76/76 [==============================] - 0s 825us/step - loss: 0.4536 - accuracy: 0.8032\n",
      "Epoch 145/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4500 - accuracy: 0.7979\n",
      "Epoch 146/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4484 - accuracy: 0.8032\n",
      "Epoch 147/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4520 - accuracy: 0.8032\n",
      "Epoch 148/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4513 - accuracy: 0.8058\n",
      "Epoch 149/200\n",
      "76/76 [==============================] - 0s 891us/step - loss: 0.4477 - accuracy: 0.8005\n",
      "Epoch 150/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4481 - accuracy: 0.7992\n",
      "Epoch 151/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4507 - accuracy: 0.8018\n",
      "Epoch 152/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4480 - accuracy: 0.8005\n",
      "Epoch 153/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4514 - accuracy: 0.7966\n",
      "Epoch 154/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4526 - accuracy: 0.7834\n",
      "Epoch 155/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4476 - accuracy: 0.7979\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 798us/step - loss: 0.4463 - accuracy: 0.8005\n",
      "Epoch 157/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4478 - accuracy: 0.7966\n",
      "Epoch 158/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4499 - accuracy: 0.8071\n",
      "Epoch 159/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4567 - accuracy: 0.7966\n",
      "Epoch 160/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4463 - accuracy: 0.8058\n",
      "Epoch 161/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4479 - accuracy: 0.7979\n",
      "Epoch 162/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4563 - accuracy: 0.7926\n",
      "Epoch 163/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4507 - accuracy: 0.8058\n",
      "Epoch 164/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4520 - accuracy: 0.8018\n",
      "Epoch 165/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4508 - accuracy: 0.7992\n",
      "Epoch 166/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4499 - accuracy: 0.7979\n",
      "Epoch 167/200\n",
      "76/76 [==============================] - 0s 930us/step - loss: 0.4508 - accuracy: 0.8058\n",
      "Epoch 168/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4526 - accuracy: 0.7926\n",
      "Epoch 169/200\n",
      "76/76 [==============================] - 0s 824us/step - loss: 0.4506 - accuracy: 0.8124\n",
      "Epoch 170/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4539 - accuracy: 0.8005\n",
      "Epoch 171/200\n",
      "76/76 [==============================] - 0s 771us/step - loss: 0.4470 - accuracy: 0.8005\n",
      "Epoch 172/200\n",
      "76/76 [==============================] - 0s 745us/step - loss: 0.4440 - accuracy: 0.8018\n",
      "Epoch 173/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4434 - accuracy: 0.7979\n",
      "Epoch 174/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4449 - accuracy: 0.7939\n",
      "Epoch 175/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4526 - accuracy: 0.7966\n",
      "Epoch 176/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4500 - accuracy: 0.8124\n",
      "Epoch 177/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4538 - accuracy: 0.7926\n",
      "Epoch 178/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4480 - accuracy: 0.7926\n",
      "Epoch 179/200\n",
      "76/76 [==============================] - 0s 758us/step - loss: 0.4497 - accuracy: 0.8071\n",
      "Epoch 180/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4502 - accuracy: 0.7952\n",
      "Epoch 181/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4448 - accuracy: 0.7939\n",
      "Epoch 182/200\n",
      "76/76 [==============================] - 0s 871us/step - loss: 0.4530 - accuracy: 0.7847\n",
      "Epoch 183/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4541 - accuracy: 0.8005\n",
      "Epoch 184/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4488 - accuracy: 0.8005\n",
      "Epoch 185/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4511 - accuracy: 0.8045\n",
      "Epoch 186/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4540 - accuracy: 0.8032\n",
      "Epoch 187/200\n",
      "76/76 [==============================] - 0s 864us/step - loss: 0.4474 - accuracy: 0.8032\n",
      "Epoch 188/200\n",
      "76/76 [==============================] - 0s 861us/step - loss: 0.4502 - accuracy: 0.8085\n",
      "Epoch 189/200\n",
      "76/76 [==============================] - 0s 878us/step - loss: 0.4514 - accuracy: 0.8058\n",
      "Epoch 190/200\n",
      "76/76 [==============================] - 0s 852us/step - loss: 0.4430 - accuracy: 0.8085\n",
      "Epoch 191/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4519 - accuracy: 0.7966\n",
      "Epoch 192/200\n",
      "76/76 [==============================] - 0s 772us/step - loss: 0.4481 - accuracy: 0.8032\n",
      "Epoch 193/200\n",
      "76/76 [==============================] - 0s 798us/step - loss: 0.4493 - accuracy: 0.7939\n",
      "Epoch 194/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4444 - accuracy: 0.8085\n",
      "Epoch 195/200\n",
      "76/76 [==============================] - 0s 918us/step - loss: 0.4441 - accuracy: 0.79920s - loss: 0.4638 - accuracy: 0.79\n",
      "Epoch 196/200\n",
      "76/76 [==============================] - 0s 851us/step - loss: 0.4479 - accuracy: 0.7979\n",
      "Epoch 197/200\n",
      "76/76 [==============================] - 0s 838us/step - loss: 0.4466 - accuracy: 0.8085\n",
      "Epoch 198/200\n",
      "76/76 [==============================] - 0s 865us/step - loss: 0.4528 - accuracy: 0.7992\n",
      "Epoch 199/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4460 - accuracy: 0.8045\n",
      "Epoch 200/200\n",
      "76/76 [==============================] - 0s 785us/step - loss: 0.4502 - accuracy: 0.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2216111bc10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "### Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "### Adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(Dropout(0.2))\n",
    "ann.add(tf.keras.layers.Dense(units=14, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "### Compiling the ANN\n",
    "ann.compile(optimizer = 'SGD', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "### Training the ANN on the Training set\n",
    "ann.fit(X_train, y_train, batch_size = 10, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04e5c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691440565198004\n",
      "0.6113063781178213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "y_pred_train = np.round(ann.predict(X_train),0)\n",
    "y_pred_test = np.round(ann.predict(X_test),0)\n",
    "precision_test = average_precision_score(y_test, y_pred_test)\n",
    "precision_train = average_precision_score(y_train, y_pred_train)\n",
    "print(precision_train)\n",
    "print(precision_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6834fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7849940828402366\n",
      "0.7873015873015872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "print(f1_train)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f327dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "95/95 [==============================] - 1s 944us/step - loss: 0.7170 - accuracy: 0.6869\n",
      "Epoch 2/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6906 - accuracy: 0.7358\n",
      "Epoch 3/250\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.78 - 0s 753us/step - loss: 0.6771 - accuracy: 0.7807\n",
      "Epoch 4/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6698 - accuracy: 0.7754\n",
      "Epoch 5/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6648 - accuracy: 0.7847\n",
      "Epoch 6/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6630 - accuracy: 0.7701\n",
      "Epoch 7/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6607 - accuracy: 0.7728\n",
      "Epoch 8/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6602 - accuracy: 0.7847\n",
      "Epoch 9/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6593 - accuracy: 0.7860\n",
      "Epoch 10/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6590 - accuracy: 0.7768\n",
      "Epoch 11/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6580 - accuracy: 0.7979\n",
      "Epoch 12/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6561 - accuracy: 0.79000s - loss: 0.6451 - accuracy: 0.77\n",
      "Epoch 13/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6578 - accuracy: 0.7834\n",
      "Epoch 14/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6580 - accuracy: 0.7794\n",
      "Epoch 15/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6574 - accuracy: 0.7873\n",
      "Epoch 16/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6581 - accuracy: 0.7834\n",
      "Epoch 17/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6575 - accuracy: 0.7728\n",
      "Epoch 18/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6562 - accuracy: 0.7900\n",
      "Epoch 19/250\n",
      "95/95 [==============================] - 0s 785us/step - loss: 0.6579 - accuracy: 0.7807\n",
      "Epoch 20/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6560 - accuracy: 0.7820\n",
      "Epoch 21/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6568 - accuracy: 0.7768\n",
      "Epoch 22/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6576 - accuracy: 0.7847\n",
      "Epoch 23/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6560 - accuracy: 0.7675\n",
      "Epoch 24/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6569 - accuracy: 0.7860\n",
      "Epoch 25/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6567 - accuracy: 0.7768\n",
      "Epoch 26/250\n",
      "95/95 [==============================] - 0s 722us/step - loss: 0.6549 - accuracy: 0.7754\n",
      "Epoch 27/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6560 - accuracy: 0.7807\n",
      "Epoch 28/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6567 - accuracy: 0.7939\n",
      "Epoch 29/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6568 - accuracy: 0.7926\n",
      "Epoch 30/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6562 - accuracy: 0.7834\n",
      "Epoch 31/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6556 - accuracy: 0.7900\n",
      "Epoch 32/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6563 - accuracy: 0.7886\n",
      "Epoch 33/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6558 - accuracy: 0.7900\n",
      "Epoch 34/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6562 - accuracy: 0.7794\n",
      "Epoch 35/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6548 - accuracy: 0.7913\n",
      "Epoch 36/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6565 - accuracy: 0.7939\n",
      "Epoch 37/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6558 - accuracy: 0.7781\n",
      "Epoch 38/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6552 - accuracy: 0.8018\n",
      "Epoch 39/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6557 - accuracy: 0.7913\n",
      "Epoch 40/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6534 - accuracy: 0.7807\n",
      "Epoch 41/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6557 - accuracy: 0.7966\n",
      "Epoch 42/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6557 - accuracy: 0.7992\n",
      "Epoch 43/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6540 - accuracy: 0.7913\n",
      "Epoch 44/250\n",
      "95/95 [==============================] - 0s 842us/step - loss: 0.6555 - accuracy: 0.8071\n",
      "Epoch 45/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6550 - accuracy: 0.7939\n",
      "Epoch 46/250\n",
      "95/95 [==============================] - 0s 833us/step - loss: 0.6552 - accuracy: 0.7966\n",
      "Epoch 47/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6525 - accuracy: 0.7966\n",
      "Epoch 48/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6551 - accuracy: 0.7900\n",
      "Epoch 49/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6551 - accuracy: 0.7979\n",
      "Epoch 50/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6542 - accuracy: 0.7847\n",
      "Epoch 51/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6551 - accuracy: 0.7992\n",
      "Epoch 52/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6546 - accuracy: 0.7992\n",
      "Epoch 53/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6552 - accuracy: 0.7926\n",
      "Epoch 54/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6541 - accuracy: 0.8058\n",
      "Epoch 55/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6556 - accuracy: 0.7847\n",
      "Epoch 56/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6550 - accuracy: 0.8005\n",
      "Epoch 57/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6547 - accuracy: 0.7939\n",
      "Epoch 58/250\n",
      "95/95 [==============================] - 0s 615us/step - loss: 0.6544 - accuracy: 0.7992\n",
      "Epoch 59/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6526 - accuracy: 0.8045\n",
      "Epoch 60/250\n",
      "95/95 [==============================] - 0s 657us/step - loss: 0.6532 - accuracy: 0.8005\n",
      "Epoch 61/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.7873\n",
      "Epoch 62/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6533 - accuracy: 0.7926\n",
      "Epoch 63/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6540 - accuracy: 0.7992\n",
      "Epoch 64/250\n",
      "95/95 [==============================] - 0s 669us/step - loss: 0.6546 - accuracy: 0.8018\n",
      "Epoch 65/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6541 - accuracy: 0.79660s - loss: 0.6547 - accuracy: 0.79\n",
      "Epoch 66/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6538 - accuracy: 0.7992\n",
      "Epoch 67/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6538 - accuracy: 0.8058\n",
      "Epoch 68/250\n",
      "95/95 [==============================] - 0s 669us/step - loss: 0.6536 - accuracy: 0.7992\n",
      "Epoch 69/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.8085\n",
      "Epoch 70/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6547 - accuracy: 0.8032\n",
      "Epoch 71/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6539 - accuracy: 0.8005\n",
      "Epoch 72/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6533 - accuracy: 0.7979\n",
      "Epoch 73/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6537 - accuracy: 0.7900\n",
      "Epoch 74/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6533 - accuracy: 0.7966\n",
      "Epoch 75/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6529 - accuracy: 0.7966\n",
      "Epoch 76/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6536 - accuracy: 0.8071\n",
      "Epoch 77/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6531 - accuracy: 0.8071\n",
      "Epoch 78/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6544 - accuracy: 0.8085\n",
      "Epoch 79/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6530 - accuracy: 0.7952\n",
      "Epoch 80/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 679us/step - loss: 0.6527 - accuracy: 0.7992\n",
      "Epoch 81/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6538 - accuracy: 0.8058\n",
      "Epoch 82/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6537 - accuracy: 0.8018\n",
      "Epoch 83/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6524 - accuracy: 0.8032\n",
      "Epoch 84/250\n",
      "95/95 [==============================] - 0s 615us/step - loss: 0.6541 - accuracy: 0.8005\n",
      "Epoch 85/250\n",
      "95/95 [==============================] - 0s 626us/step - loss: 0.6530 - accuracy: 0.8071\n",
      "Epoch 86/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6534 - accuracy: 0.8085\n",
      "Epoch 87/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6528 - accuracy: 0.8045\n",
      "Epoch 88/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6537 - accuracy: 0.8032\n",
      "Epoch 89/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6534 - accuracy: 0.8018\n",
      "Epoch 90/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6525 - accuracy: 0.8018\n",
      "Epoch 91/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6537 - accuracy: 0.8085\n",
      "Epoch 92/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6532 - accuracy: 0.8045\n",
      "Epoch 93/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6519 - accuracy: 0.8045\n",
      "Epoch 94/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6535 - accuracy: 0.8018\n",
      "Epoch 95/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6536 - accuracy: 0.8085\n",
      "Epoch 96/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6532 - accuracy: 0.7979\n",
      "Epoch 97/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6528 - accuracy: 0.7979\n",
      "Epoch 98/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6533 - accuracy: 0.8005\n",
      "Epoch 99/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6530 - accuracy: 0.8058\n",
      "Epoch 100/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6521 - accuracy: 0.7992\n",
      "Epoch 101/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6525 - accuracy: 0.8005\n",
      "Epoch 102/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6533 - accuracy: 0.8085\n",
      "Epoch 103/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6534 - accuracy: 0.8085\n",
      "Epoch 104/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6518 - accuracy: 0.8005\n",
      "Epoch 105/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6529 - accuracy: 0.7979\n",
      "Epoch 106/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6525 - accuracy: 0.8032\n",
      "Epoch 107/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6526 - accuracy: 0.8111\n",
      "Epoch 108/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6521 - accuracy: 0.8032\n",
      "Epoch 109/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6511 - accuracy: 0.8005\n",
      "Epoch 110/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6527 - accuracy: 0.8058\n",
      "Epoch 111/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6524 - accuracy: 0.8018\n",
      "Epoch 112/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6527 - accuracy: 0.8071\n",
      "Epoch 113/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6515 - accuracy: 0.8098\n",
      "Epoch 114/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6527 - accuracy: 0.7926\n",
      "Epoch 115/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6516 - accuracy: 0.8045\n",
      "Epoch 116/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6521 - accuracy: 0.8071\n",
      "Epoch 117/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6518 - accuracy: 0.8124\n",
      "Epoch 118/250\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.82 - 0s 870us/step - loss: 0.6529 - accuracy: 0.7992\n",
      "Epoch 119/250\n",
      "95/95 [==============================] - 0s 859us/step - loss: 0.6525 - accuracy: 0.8058\n",
      "Epoch 120/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 121/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6517 - accuracy: 0.7939\n",
      "Epoch 122/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6526 - accuracy: 0.8032\n",
      "Epoch 123/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6511 - accuracy: 0.8045\n",
      "Epoch 124/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6535 - accuracy: 0.8058\n",
      "Epoch 125/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6527 - accuracy: 0.7992\n",
      "Epoch 126/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6521 - accuracy: 0.7939\n",
      "Epoch 127/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6520 - accuracy: 0.8098\n",
      "Epoch 128/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6519 - accuracy: 0.8005\n",
      "Epoch 129/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6528 - accuracy: 0.8018\n",
      "Epoch 130/250\n",
      "95/95 [==============================] - 0s 696us/step - loss: 0.6525 - accuracy: 0.8018\n",
      "Epoch 131/250\n",
      "95/95 [==============================] - 0s 689us/step - loss: 0.6510 - accuracy: 0.8098\n",
      "Epoch 132/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6523 - accuracy: 0.8032\n",
      "Epoch 133/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 134/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6523 - accuracy: 0.8124\n",
      "Epoch 135/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6507 - accuracy: 0.8098\n",
      "Epoch 136/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6518 - accuracy: 0.8045\n",
      "Epoch 137/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6519 - accuracy: 0.8005\n",
      "Epoch 138/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6518 - accuracy: 0.8018\n",
      "Epoch 139/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6514 - accuracy: 0.8071\n",
      "Epoch 140/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 141/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6527 - accuracy: 0.8071\n",
      "Epoch 142/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6517 - accuracy: 0.8085\n",
      "Epoch 143/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6521 - accuracy: 0.8005\n",
      "Epoch 144/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 145/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6512 - accuracy: 0.8018\n",
      "Epoch 146/250\n",
      "95/95 [==============================] - 0s 795us/step - loss: 0.6516 - accuracy: 0.8045\n",
      "Epoch 147/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6516 - accuracy: 0.8071\n",
      "Epoch 148/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6500 - accuracy: 0.8098\n",
      "Epoch 149/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6502 - accuracy: 0.79920s - loss: 0.6393 - accuracy: 0.80\n",
      "Epoch 150/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6520 - accuracy: 0.8071\n",
      "Epoch 151/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6517 - accuracy: 0.8045\n",
      "Epoch 152/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6511 - accuracy: 0.8018\n",
      "Epoch 153/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6505 - accuracy: 0.8137\n",
      "Epoch 154/250\n",
      "95/95 [==============================] - 0s 742us/step - loss: 0.6505 - accuracy: 0.8085\n",
      "Epoch 155/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6522 - accuracy: 0.8032\n",
      "Epoch 156/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6515 - accuracy: 0.8032\n",
      "Epoch 157/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6514 - accuracy: 0.8045\n",
      "Epoch 158/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6504 - accuracy: 0.8098\n",
      "Epoch 159/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 668us/step - loss: 0.6501 - accuracy: 0.7992\n",
      "Epoch 160/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6526 - accuracy: 0.8005\n",
      "Epoch 161/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6515 - accuracy: 0.8018\n",
      "Epoch 162/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6512 - accuracy: 0.8018\n",
      "Epoch 163/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6516 - accuracy: 0.8058\n",
      "Epoch 164/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6513 - accuracy: 0.8018\n",
      "Epoch 165/250\n",
      "95/95 [==============================] - 0s 637us/step - loss: 0.6513 - accuracy: 0.8032\n",
      "Epoch 166/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6508 - accuracy: 0.8032\n",
      "Epoch 167/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6498 - accuracy: 0.8045\n",
      "Epoch 168/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6516 - accuracy: 0.7992\n",
      "Epoch 169/250\n",
      "95/95 [==============================] - 0s 698us/step - loss: 0.6505 - accuracy: 0.8032\n",
      "Epoch 170/250\n",
      "95/95 [==============================] - 0s 722us/step - loss: 0.6504 - accuracy: 0.8071\n",
      "Epoch 171/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6513 - accuracy: 0.8018\n",
      "Epoch 172/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6513 - accuracy: 0.8005\n",
      "Epoch 173/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6512 - accuracy: 0.80580s - loss: 0.6557 - accuracy: 0.79\n",
      "Epoch 174/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6509 - accuracy: 0.8032\n",
      "Epoch 175/250\n",
      "95/95 [==============================] - 0s 690us/step - loss: 0.6513 - accuracy: 0.8058\n",
      "Epoch 176/250\n",
      "95/95 [==============================] - 0s 648us/step - loss: 0.6506 - accuracy: 0.8098\n",
      "Epoch 177/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6517 - accuracy: 0.8005\n",
      "Epoch 178/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6506 - accuracy: 0.8058\n",
      "Epoch 179/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6511 - accuracy: 0.8071\n",
      "Epoch 180/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6502 - accuracy: 0.8085\n",
      "Epoch 181/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6488 - accuracy: 0.8058\n",
      "Epoch 182/250\n",
      "95/95 [==============================] - 0s 779us/step - loss: 0.6513 - accuracy: 0.8032\n",
      "Epoch 183/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6509 - accuracy: 0.8071\n",
      "Epoch 184/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6497 - accuracy: 0.8098\n",
      "Epoch 185/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6514 - accuracy: 0.8124\n",
      "Epoch 186/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6504 - accuracy: 0.8018\n",
      "Epoch 187/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6504 - accuracy: 0.80050s - loss: 0.6485 - accuracy: 0.79\n",
      "Epoch 188/250\n",
      "95/95 [==============================] - 0s 775us/step - loss: 0.6495 - accuracy: 0.8005\n",
      "Epoch 189/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6509 - accuracy: 0.8032\n",
      "Epoch 190/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6499 - accuracy: 0.8085\n",
      "Epoch 191/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 192/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6496 - accuracy: 0.8032\n",
      "Epoch 193/250\n",
      "95/95 [==============================] - 0s 774us/step - loss: 0.6505 - accuracy: 0.8071\n",
      "Epoch 194/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 195/250\n",
      "95/95 [==============================] - 0s 806us/step - loss: 0.6508 - accuracy: 0.8098\n",
      "Epoch 196/250\n",
      "95/95 [==============================] - 0s 923us/step - loss: 0.6502 - accuracy: 0.8045\n",
      "Epoch 197/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6490 - accuracy: 0.8098\n",
      "Epoch 198/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6508 - accuracy: 0.8045\n",
      "Epoch 199/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6505 - accuracy: 0.8045\n",
      "Epoch 200/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6508 - accuracy: 0.8032\n",
      "Epoch 201/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6490 - accuracy: 0.8058\n",
      "Epoch 202/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6504 - accuracy: 0.8005\n",
      "Epoch 203/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6508 - accuracy: 0.8058\n",
      "Epoch 204/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6504 - accuracy: 0.8071\n",
      "Epoch 205/250\n",
      "95/95 [==============================] - 0s 657us/step - loss: 0.6494 - accuracy: 0.8018\n",
      "Epoch 206/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6498 - accuracy: 0.7966\n",
      "Epoch 207/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6489 - accuracy: 0.8045\n",
      "Epoch 208/250\n",
      "95/95 [==============================] - 0s 658us/step - loss: 0.6505 - accuracy: 0.8032\n",
      "Epoch 209/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6495 - accuracy: 0.8018\n",
      "Epoch 210/250\n",
      "95/95 [==============================] - 0s 674us/step - loss: 0.6497 - accuracy: 0.8058\n",
      "Epoch 211/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6505 - accuracy: 0.8045\n",
      "Epoch 212/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6501 - accuracy: 0.8058\n",
      "Epoch 213/250\n",
      "95/95 [==============================] - 0s 647us/step - loss: 0.6504 - accuracy: 0.8058\n",
      "Epoch 214/250\n",
      "95/95 [==============================] - 0s 668us/step - loss: 0.6492 - accuracy: 0.8005\n",
      "Epoch 215/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6495 - accuracy: 0.8071\n",
      "Epoch 216/250\n",
      "95/95 [==============================] - 0s 721us/step - loss: 0.6498 - accuracy: 0.8071\n",
      "Epoch 217/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6500 - accuracy: 0.80710s - loss: 0.6585 - accuracy: 0.80\n",
      "Epoch 218/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6493 - accuracy: 0.8032\n",
      "Epoch 219/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6505 - accuracy: 0.8058\n",
      "Epoch 220/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6503 - accuracy: 0.8005\n",
      "Epoch 221/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6496 - accuracy: 0.8018\n",
      "Epoch 222/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6497 - accuracy: 0.8045\n",
      "Epoch 223/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6479 - accuracy: 0.8018\n",
      "Epoch 224/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 225/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6499 - accuracy: 0.8045\n",
      "Epoch 226/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6496 - accuracy: 0.8071\n",
      "Epoch 227/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6497 - accuracy: 0.8045\n",
      "Epoch 228/250\n",
      "95/95 [==============================] - 0s 679us/step - loss: 0.6497 - accuracy: 0.8058\n",
      "Epoch 229/250\n",
      "95/95 [==============================] - 0s 700us/step - loss: 0.6492 - accuracy: 0.8111\n",
      "Epoch 230/250\n",
      "95/95 [==============================] - 0s 732us/step - loss: 0.6492 - accuracy: 0.8032\n",
      "Epoch 231/250\n",
      "95/95 [==============================] - 0s 701us/step - loss: 0.6490 - accuracy: 0.8058\n",
      "Epoch 232/250\n",
      "95/95 [==============================] - 0s 711us/step - loss: 0.6496 - accuracy: 0.8032\n",
      "Epoch 233/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6492 - accuracy: 0.8071\n",
      "Epoch 234/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6497 - accuracy: 0.8018\n",
      "Epoch 235/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6501 - accuracy: 0.8071\n",
      "Epoch 236/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6488 - accuracy: 0.8032\n",
      "Epoch 237/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 785us/step - loss: 0.6496 - accuracy: 0.8045\n",
      "Epoch 238/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6492 - accuracy: 0.8111\n",
      "Epoch 239/250\n",
      "95/95 [==============================] - 0s 753us/step - loss: 0.6496 - accuracy: 0.8085\n",
      "Epoch 240/250\n",
      "95/95 [==============================] - 0s 743us/step - loss: 0.6499 - accuracy: 0.8058\n",
      "Epoch 241/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6494 - accuracy: 0.8058\n",
      "Epoch 242/250\n",
      "95/95 [==============================] - 0s 807us/step - loss: 0.6495 - accuracy: 0.8045\n",
      "Epoch 243/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6496 - accuracy: 0.8045\n",
      "Epoch 244/250\n",
      "95/95 [==============================] - 0s 796us/step - loss: 0.6489 - accuracy: 0.8032\n",
      "Epoch 245/250\n",
      "95/95 [==============================] - 0s 764us/step - loss: 0.6490 - accuracy: 0.8071\n",
      "Epoch 246/250\n",
      "95/95 [==============================] - 0s 849us/step - loss: 0.6499 - accuracy: 0.8058\n",
      "Epoch 247/250\n",
      "95/95 [==============================] - 0s 870us/step - loss: 0.6492 - accuracy: 0.8071\n",
      "Epoch 248/250\n",
      "95/95 [==============================] - 0s 828us/step - loss: 0.6489 - accuracy: 0.8045\n",
      "Epoch 249/250\n",
      "95/95 [==============================] - 0s 817us/step - loss: 0.6480 - accuracy: 0.8045\n",
      "Epoch 250/250\n",
      "95/95 [==============================] - 0s 838us/step - loss: 0.6491 - accuracy: 0.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22164058fd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initializing the ANN\n",
    "ann_p = tf.keras.models.Sequential()\n",
    "### Adding the input layer and the first hidden layer\n",
    "ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "### Compiling the ANN\n",
    "ann_p.compile(optimizer = 'RMSprop', loss = 'poisson', metrics = ['accuracy'])\n",
    "### Training the ANN on the Training set\n",
    "ann_p.fit(X_train, y_train, batch_size = 8, epochs = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e16c3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210\n",
      "379/379 - 1s - loss: 0.5406 - accuracy: 0.7345 - val_loss: 0.4708 - val_accuracy: 0.7985\n",
      "Epoch 2/210\n",
      "379/379 - 0s - loss: 0.4676 - accuracy: 0.7952 - val_loss: 0.4603 - val_accuracy: 0.7910\n",
      "Epoch 3/210\n",
      "379/379 - 0s - loss: 0.4760 - accuracy: 0.7926 - val_loss: 0.4593 - val_accuracy: 0.7910\n",
      "Epoch 4/210\n",
      "379/379 - 0s - loss: 0.4606 - accuracy: 0.8058 - val_loss: 0.4569 - val_accuracy: 0.7985\n",
      "Epoch 5/210\n",
      "379/379 - 0s - loss: 0.4639 - accuracy: 0.8058 - val_loss: 0.4719 - val_accuracy: 0.8134\n",
      "Epoch 6/210\n",
      "379/379 - 0s - loss: 0.4443 - accuracy: 0.8071 - val_loss: 0.4703 - val_accuracy: 0.8060\n",
      "Epoch 7/210\n",
      "379/379 - 0s - loss: 0.4705 - accuracy: 0.8005 - val_loss: 0.4641 - val_accuracy: 0.8209\n",
      "Epoch 8/210\n",
      "379/379 - 0s - loss: 0.4538 - accuracy: 0.8124 - val_loss: 0.4563 - val_accuracy: 0.8060\n",
      "Epoch 9/210\n",
      "379/379 - 0s - loss: 0.4638 - accuracy: 0.8111 - val_loss: 0.4494 - val_accuracy: 0.8060\n",
      "Epoch 10/210\n",
      "379/379 - 0s - loss: 0.4529 - accuracy: 0.8164 - val_loss: 0.4600 - val_accuracy: 0.8134\n",
      "Epoch 11/210\n",
      "379/379 - 0s - loss: 0.4601 - accuracy: 0.8151 - val_loss: 0.4469 - val_accuracy: 0.8060\n",
      "Epoch 12/210\n",
      "379/379 - 0s - loss: 0.4650 - accuracy: 0.7979 - val_loss: 0.4441 - val_accuracy: 0.8209\n",
      "Epoch 13/210\n",
      "379/379 - 0s - loss: 0.4542 - accuracy: 0.8045 - val_loss: 0.4503 - val_accuracy: 0.8209\n",
      "Epoch 14/210\n",
      "379/379 - 0s - loss: 0.4515 - accuracy: 0.8085 - val_loss: 0.4514 - val_accuracy: 0.8134\n",
      "Epoch 15/210\n",
      "379/379 - 0s - loss: 0.4514 - accuracy: 0.8111 - val_loss: 0.4518 - val_accuracy: 0.8209\n",
      "Epoch 16/210\n",
      "379/379 - 0s - loss: 0.4481 - accuracy: 0.8071 - val_loss: 0.4526 - val_accuracy: 0.8134\n",
      "Epoch 17/210\n",
      "379/379 - 0s - loss: 0.4555 - accuracy: 0.8085 - val_loss: 0.4618 - val_accuracy: 0.8134\n",
      "Epoch 18/210\n",
      "379/379 - 0s - loss: 0.4488 - accuracy: 0.8111 - val_loss: 0.4694 - val_accuracy: 0.8209\n",
      "Epoch 19/210\n",
      "379/379 - 0s - loss: 0.4504 - accuracy: 0.8124 - val_loss: 0.4561 - val_accuracy: 0.8209\n",
      "Epoch 20/210\n",
      "379/379 - 0s - loss: 0.4449 - accuracy: 0.8151 - val_loss: 0.4549 - val_accuracy: 0.8209\n",
      "Epoch 21/210\n",
      "379/379 - 0s - loss: 0.4577 - accuracy: 0.8177 - val_loss: 0.4472 - val_accuracy: 0.8284\n",
      "Epoch 22/210\n",
      "379/379 - 0s - loss: 0.4594 - accuracy: 0.8164 - val_loss: 0.4583 - val_accuracy: 0.8209\n",
      "Epoch 23/210\n",
      "379/379 - 0s - loss: 0.4616 - accuracy: 0.8085 - val_loss: 0.4670 - val_accuracy: 0.8284\n",
      "Epoch 24/210\n",
      "379/379 - 0s - loss: 0.4315 - accuracy: 0.8203 - val_loss: 0.4572 - val_accuracy: 0.8284\n",
      "Epoch 25/210\n",
      "379/379 - 0s - loss: 0.4335 - accuracy: 0.8190 - val_loss: 0.4563 - val_accuracy: 0.8209\n",
      "Epoch 26/210\n",
      "379/379 - 0s - loss: 0.4625 - accuracy: 0.8151 - val_loss: 0.4513 - val_accuracy: 0.8209\n",
      "Epoch 27/210\n",
      "379/379 - 0s - loss: 0.4294 - accuracy: 0.8269 - val_loss: 0.4544 - val_accuracy: 0.8134\n",
      "Epoch 28/210\n",
      "379/379 - 0s - loss: 0.4348 - accuracy: 0.8085 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 29/210\n",
      "379/379 - 0s - loss: 0.4369 - accuracy: 0.8217 - val_loss: 0.4656 - val_accuracy: 0.8284\n",
      "Epoch 30/210\n",
      "379/379 - 0s - loss: 0.4423 - accuracy: 0.8203 - val_loss: 0.4544 - val_accuracy: 0.8284\n",
      "Epoch 31/210\n",
      "379/379 - 0s - loss: 0.4422 - accuracy: 0.8283 - val_loss: 0.4816 - val_accuracy: 0.8284\n",
      "Epoch 32/210\n",
      "379/379 - 0s - loss: 0.4493 - accuracy: 0.8151 - val_loss: 0.4647 - val_accuracy: 0.8284\n",
      "Epoch 33/210\n",
      "379/379 - 0s - loss: 0.4286 - accuracy: 0.8269 - val_loss: 0.4609 - val_accuracy: 0.8284\n",
      "Epoch 34/210\n",
      "379/379 - 0s - loss: 0.4511 - accuracy: 0.8190 - val_loss: 0.4600 - val_accuracy: 0.8284\n",
      "Epoch 35/210\n",
      "379/379 - 0s - loss: 0.4412 - accuracy: 0.8085 - val_loss: 0.4732 - val_accuracy: 0.8284\n",
      "Epoch 36/210\n",
      "379/379 - 0s - loss: 0.4596 - accuracy: 0.8098 - val_loss: 0.4757 - val_accuracy: 0.8284\n",
      "Epoch 37/210\n",
      "379/379 - 0s - loss: 0.4277 - accuracy: 0.8230 - val_loss: 0.4730 - val_accuracy: 0.8284\n",
      "Epoch 38/210\n",
      "379/379 - 0s - loss: 0.4467 - accuracy: 0.8151 - val_loss: 0.4745 - val_accuracy: 0.8209\n",
      "Epoch 39/210\n",
      "379/379 - 0s - loss: 0.4431 - accuracy: 0.8322 - val_loss: 0.4723 - val_accuracy: 0.8284\n",
      "Epoch 40/210\n",
      "379/379 - 0s - loss: 0.4463 - accuracy: 0.8269 - val_loss: 0.4682 - val_accuracy: 0.8284\n",
      "Epoch 41/210\n",
      "379/379 - 0s - loss: 0.4294 - accuracy: 0.8124 - val_loss: 0.4649 - val_accuracy: 0.8358\n",
      "Epoch 42/210\n",
      "379/379 - 0s - loss: 0.4458 - accuracy: 0.8124 - val_loss: 0.4629 - val_accuracy: 0.8358\n",
      "Epoch 43/210\n",
      "379/379 - 0s - loss: 0.4487 - accuracy: 0.8230 - val_loss: 0.4635 - val_accuracy: 0.8358\n",
      "Epoch 44/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8217 - val_loss: 0.4596 - val_accuracy: 0.8358\n",
      "Epoch 45/210\n",
      "379/379 - 0s - loss: 0.4417 - accuracy: 0.8230 - val_loss: 0.4689 - val_accuracy: 0.8358\n",
      "Epoch 46/210\n",
      "379/379 - 0s - loss: 0.4524 - accuracy: 0.8124 - val_loss: 0.4633 - val_accuracy: 0.8358\n",
      "Epoch 47/210\n",
      "379/379 - 0s - loss: 0.4364 - accuracy: 0.8336 - val_loss: 0.4635 - val_accuracy: 0.8358\n",
      "Epoch 48/210\n",
      "379/379 - 0s - loss: 0.4315 - accuracy: 0.8243 - val_loss: 0.4806 - val_accuracy: 0.8358\n",
      "Epoch 49/210\n",
      "379/379 - 0s - loss: 0.4395 - accuracy: 0.8230 - val_loss: 0.4752 - val_accuracy: 0.8358\n",
      "Epoch 50/210\n",
      "379/379 - 0s - loss: 0.4251 - accuracy: 0.8124 - val_loss: 0.4708 - val_accuracy: 0.8358\n",
      "Epoch 51/210\n",
      "379/379 - 0s - loss: 0.4277 - accuracy: 0.8283 - val_loss: 0.4644 - val_accuracy: 0.8358\n",
      "Epoch 52/210\n",
      "379/379 - 0s - loss: 0.4473 - accuracy: 0.8058 - val_loss: 0.4927 - val_accuracy: 0.8209\n",
      "Epoch 53/210\n",
      "379/379 - 0s - loss: 0.4479 - accuracy: 0.8124 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 54/210\n",
      "379/379 - 0s - loss: 0.4336 - accuracy: 0.8151 - val_loss: 0.4717 - val_accuracy: 0.8358\n",
      "Epoch 55/210\n",
      "379/379 - 0s - loss: 0.4286 - accuracy: 0.8243 - val_loss: 0.4644 - val_accuracy: 0.8358\n",
      "Epoch 56/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8269 - val_loss: 0.4685 - val_accuracy: 0.8358\n",
      "Epoch 57/210\n",
      "379/379 - 0s - loss: 0.4415 - accuracy: 0.8203 - val_loss: 0.4676 - val_accuracy: 0.8358\n",
      "Epoch 58/210\n",
      "379/379 - 0s - loss: 0.4441 - accuracy: 0.8230 - val_loss: 0.4719 - val_accuracy: 0.8358\n",
      "Epoch 59/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8111 - val_loss: 0.4709 - val_accuracy: 0.8358\n",
      "Epoch 60/210\n",
      "379/379 - 0s - loss: 0.4522 - accuracy: 0.8230 - val_loss: 0.4665 - val_accuracy: 0.8358\n",
      "Epoch 61/210\n",
      "379/379 - 0s - loss: 0.4479 - accuracy: 0.8203 - val_loss: 0.4625 - val_accuracy: 0.8358\n",
      "Epoch 62/210\n",
      "379/379 - 0s - loss: 0.4486 - accuracy: 0.8137 - val_loss: 0.4646 - val_accuracy: 0.8358\n",
      "Epoch 63/210\n",
      "379/379 - 0s - loss: 0.4407 - accuracy: 0.8137 - val_loss: 0.4695 - val_accuracy: 0.8358\n",
      "Epoch 64/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8243 - val_loss: 0.4596 - val_accuracy: 0.8284\n",
      "Epoch 65/210\n",
      "379/379 - 0s - loss: 0.4208 - accuracy: 0.8283 - val_loss: 0.4694 - val_accuracy: 0.8358\n",
      "Epoch 66/210\n",
      "379/379 - 0s - loss: 0.4398 - accuracy: 0.8309 - val_loss: 0.4625 - val_accuracy: 0.8358\n",
      "Epoch 67/210\n",
      "379/379 - 0s - loss: 0.4103 - accuracy: 0.8269 - val_loss: 0.4831 - val_accuracy: 0.8358\n",
      "Epoch 68/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8256 - val_loss: 0.4670 - val_accuracy: 0.8433\n",
      "Epoch 69/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8269 - val_loss: 0.4706 - val_accuracy: 0.8358\n",
      "Epoch 70/210\n",
      "379/379 - 0s - loss: 0.4554 - accuracy: 0.8177 - val_loss: 0.4679 - val_accuracy: 0.8358\n",
      "Epoch 71/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8137 - val_loss: 0.4832 - val_accuracy: 0.8134\n",
      "Epoch 72/210\n",
      "379/379 - 0s - loss: 0.4367 - accuracy: 0.8217 - val_loss: 0.4690 - val_accuracy: 0.8358\n",
      "Epoch 73/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8177 - val_loss: 0.4929 - val_accuracy: 0.8134\n",
      "Epoch 74/210\n",
      "379/379 - 0s - loss: 0.4311 - accuracy: 0.8137 - val_loss: 0.4830 - val_accuracy: 0.8358\n",
      "Epoch 75/210\n",
      "379/379 - 0s - loss: 0.4337 - accuracy: 0.8164 - val_loss: 0.4715 - val_accuracy: 0.8358\n",
      "Epoch 76/210\n",
      "379/379 - 0s - loss: 0.4226 - accuracy: 0.8230 - val_loss: 0.4485 - val_accuracy: 0.8433\n",
      "Epoch 77/210\n",
      "379/379 - 0s - loss: 0.4323 - accuracy: 0.8269 - val_loss: 0.4825 - val_accuracy: 0.8358\n",
      "Epoch 78/210\n",
      "379/379 - 0s - loss: 0.4428 - accuracy: 0.8190 - val_loss: 0.4870 - val_accuracy: 0.8284\n",
      "Epoch 79/210\n",
      "379/379 - 0s - loss: 0.4384 - accuracy: 0.8203 - val_loss: 0.4763 - val_accuracy: 0.8358\n",
      "Epoch 80/210\n",
      "379/379 - 0s - loss: 0.4499 - accuracy: 0.8203 - val_loss: 0.4763 - val_accuracy: 0.8358\n",
      "Epoch 81/210\n",
      "379/379 - 0s - loss: 0.4380 - accuracy: 0.8217 - val_loss: 0.4630 - val_accuracy: 0.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/210\n",
      "379/379 - 0s - loss: 0.4475 - accuracy: 0.8256 - val_loss: 0.4642 - val_accuracy: 0.8358\n",
      "Epoch 83/210\n",
      "379/379 - 0s - loss: 0.4464 - accuracy: 0.8177 - val_loss: 0.4762 - val_accuracy: 0.8358\n",
      "Epoch 84/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8164 - val_loss: 0.4804 - val_accuracy: 0.8284\n",
      "Epoch 85/210\n",
      "379/379 - 0s - loss: 0.4290 - accuracy: 0.8164 - val_loss: 0.4687 - val_accuracy: 0.8433\n",
      "Epoch 86/210\n",
      "379/379 - 0s - loss: 0.4442 - accuracy: 0.8217 - val_loss: 0.4710 - val_accuracy: 0.8433\n",
      "Epoch 87/210\n",
      "379/379 - 0s - loss: 0.4452 - accuracy: 0.8230 - val_loss: 0.4745 - val_accuracy: 0.8358\n",
      "Epoch 88/210\n",
      "379/379 - 0s - loss: 0.4316 - accuracy: 0.8269 - val_loss: 0.4652 - val_accuracy: 0.8433\n",
      "Epoch 89/210\n",
      "379/379 - 0s - loss: 0.4319 - accuracy: 0.8230 - val_loss: 0.4792 - val_accuracy: 0.8433\n",
      "Epoch 90/210\n",
      "379/379 - 0s - loss: 0.4383 - accuracy: 0.8230 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 91/210\n",
      "379/379 - 0s - loss: 0.4470 - accuracy: 0.8164 - val_loss: 0.4865 - val_accuracy: 0.8284\n",
      "Epoch 92/210\n",
      "379/379 - 0s - loss: 0.4565 - accuracy: 0.8137 - val_loss: 0.4800 - val_accuracy: 0.8358\n",
      "Epoch 93/210\n",
      "379/379 - 0s - loss: 0.4388 - accuracy: 0.8203 - val_loss: 0.4863 - val_accuracy: 0.8284\n",
      "Epoch 94/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8296 - val_loss: 0.4912 - val_accuracy: 0.8284\n",
      "Epoch 95/210\n",
      "379/379 - 0s - loss: 0.4382 - accuracy: 0.8217 - val_loss: 0.4845 - val_accuracy: 0.8358\n",
      "Epoch 96/210\n",
      "379/379 - 0s - loss: 0.4618 - accuracy: 0.8137 - val_loss: 0.4730 - val_accuracy: 0.8358\n",
      "Epoch 97/210\n",
      "379/379 - 0s - loss: 0.4442 - accuracy: 0.8177 - val_loss: 0.4878 - val_accuracy: 0.8358\n",
      "Epoch 98/210\n",
      "379/379 - 0s - loss: 0.4311 - accuracy: 0.8137 - val_loss: 0.4957 - val_accuracy: 0.8284\n",
      "Epoch 99/210\n",
      "379/379 - 0s - loss: 0.4273 - accuracy: 0.8269 - val_loss: 0.4814 - val_accuracy: 0.8433\n",
      "Epoch 100/210\n",
      "379/379 - 0s - loss: 0.4512 - accuracy: 0.8137 - val_loss: 0.4831 - val_accuracy: 0.8284\n",
      "Epoch 101/210\n",
      "379/379 - 0s - loss: 0.4361 - accuracy: 0.8190 - val_loss: 0.4591 - val_accuracy: 0.8433\n",
      "Epoch 102/210\n",
      "379/379 - 0s - loss: 0.4332 - accuracy: 0.8256 - val_loss: 0.4992 - val_accuracy: 0.8358\n",
      "Epoch 103/210\n",
      "379/379 - 0s - loss: 0.4291 - accuracy: 0.8098 - val_loss: 0.5025 - val_accuracy: 0.8358\n",
      "Epoch 104/210\n",
      "379/379 - 0s - loss: 0.4359 - accuracy: 0.8164 - val_loss: 0.4899 - val_accuracy: 0.8358\n",
      "Epoch 105/210\n",
      "379/379 - 0s - loss: 0.4398 - accuracy: 0.8203 - val_loss: 0.4752 - val_accuracy: 0.8433\n",
      "Epoch 106/210\n",
      "379/379 - 0s - loss: 0.4215 - accuracy: 0.8230 - val_loss: 0.4859 - val_accuracy: 0.8433\n",
      "Epoch 107/210\n",
      "379/379 - 0s - loss: 0.4345 - accuracy: 0.8177 - val_loss: 0.4845 - val_accuracy: 0.8358\n",
      "Epoch 108/210\n",
      "379/379 - 0s - loss: 0.4444 - accuracy: 0.8230 - val_loss: 0.4803 - val_accuracy: 0.8433\n",
      "Epoch 109/210\n",
      "379/379 - 0s - loss: 0.4320 - accuracy: 0.8164 - val_loss: 0.4798 - val_accuracy: 0.8358\n",
      "Epoch 110/210\n",
      "379/379 - 0s - loss: 0.4386 - accuracy: 0.8336 - val_loss: 0.4631 - val_accuracy: 0.8433\n",
      "Epoch 111/210\n",
      "379/379 - 0s - loss: 0.4394 - accuracy: 0.8230 - val_loss: 0.4921 - val_accuracy: 0.8358\n",
      "Epoch 112/210\n",
      "379/379 - 0s - loss: 0.4334 - accuracy: 0.8137 - val_loss: 0.4626 - val_accuracy: 0.8433\n",
      "Epoch 113/210\n",
      "379/379 - 0s - loss: 0.4314 - accuracy: 0.8309 - val_loss: 0.4982 - val_accuracy: 0.8433\n",
      "Epoch 114/210\n",
      "379/379 - 0s - loss: 0.4501 - accuracy: 0.8177 - val_loss: 0.4829 - val_accuracy: 0.8358\n",
      "Epoch 115/210\n",
      "379/379 - 0s - loss: 0.4465 - accuracy: 0.8190 - val_loss: 0.4792 - val_accuracy: 0.8284\n",
      "Epoch 116/210\n",
      "379/379 - 0s - loss: 0.4414 - accuracy: 0.8217 - val_loss: 0.4663 - val_accuracy: 0.8358\n",
      "Epoch 117/210\n",
      "379/379 - 0s - loss: 0.4374 - accuracy: 0.8336 - val_loss: 0.4858 - val_accuracy: 0.8433\n",
      "Epoch 118/210\n",
      "379/379 - 0s - loss: 0.4304 - accuracy: 0.8309 - val_loss: 0.4931 - val_accuracy: 0.8358\n",
      "Epoch 119/210\n",
      "379/379 - 0s - loss: 0.4482 - accuracy: 0.8217 - val_loss: 0.4826 - val_accuracy: 0.8358\n",
      "Epoch 120/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8256 - val_loss: 0.4912 - val_accuracy: 0.8358\n",
      "Epoch 121/210\n",
      "379/379 - 0s - loss: 0.4440 - accuracy: 0.8164 - val_loss: 0.4959 - val_accuracy: 0.8209\n",
      "Epoch 122/210\n",
      "379/379 - 0s - loss: 0.4356 - accuracy: 0.8203 - val_loss: 0.4863 - val_accuracy: 0.8433\n",
      "Epoch 123/210\n",
      "379/379 - 0s - loss: 0.4586 - accuracy: 0.8203 - val_loss: 0.4942 - val_accuracy: 0.8284\n",
      "Epoch 124/210\n",
      "379/379 - 0s - loss: 0.4370 - accuracy: 0.8203 - val_loss: 0.4893 - val_accuracy: 0.8209\n",
      "Epoch 125/210\n",
      "379/379 - 0s - loss: 0.4553 - accuracy: 0.8098 - val_loss: 0.4785 - val_accuracy: 0.8284\n",
      "Epoch 126/210\n",
      "379/379 - 0s - loss: 0.4534 - accuracy: 0.8283 - val_loss: 0.4747 - val_accuracy: 0.8358\n",
      "Epoch 127/210\n",
      "379/379 - 0s - loss: 0.4527 - accuracy: 0.8151 - val_loss: 0.4823 - val_accuracy: 0.8358\n",
      "Epoch 128/210\n",
      "379/379 - 0s - loss: 0.4416 - accuracy: 0.8256 - val_loss: 0.5097 - val_accuracy: 0.8060\n",
      "Epoch 129/210\n",
      "379/379 - 0s - loss: 0.4373 - accuracy: 0.8190 - val_loss: 0.4827 - val_accuracy: 0.8358\n",
      "Epoch 130/210\n",
      "379/379 - 0s - loss: 0.4394 - accuracy: 0.8164 - val_loss: 0.4893 - val_accuracy: 0.8358\n",
      "Epoch 131/210\n",
      "379/379 - 0s - loss: 0.4307 - accuracy: 0.8217 - val_loss: 0.4889 - val_accuracy: 0.8433\n",
      "Epoch 132/210\n",
      "379/379 - 0s - loss: 0.4606 - accuracy: 0.8256 - val_loss: 0.4762 - val_accuracy: 0.8433\n",
      "Epoch 133/210\n",
      "379/379 - 0s - loss: 0.4387 - accuracy: 0.8269 - val_loss: 0.4982 - val_accuracy: 0.8284\n",
      "Epoch 134/210\n",
      "379/379 - 0s - loss: 0.4448 - accuracy: 0.8217 - val_loss: 0.4801 - val_accuracy: 0.8209\n",
      "Epoch 135/210\n",
      "379/379 - 0s - loss: 0.4369 - accuracy: 0.8243 - val_loss: 0.4737 - val_accuracy: 0.8433\n",
      "Epoch 136/210\n",
      "379/379 - 0s - loss: 0.4523 - accuracy: 0.8164 - val_loss: 0.4832 - val_accuracy: 0.8433\n",
      "Epoch 137/210\n",
      "379/379 - 0s - loss: 0.4542 - accuracy: 0.8177 - val_loss: 0.4708 - val_accuracy: 0.8433\n",
      "Epoch 138/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8217 - val_loss: 0.4892 - val_accuracy: 0.8284\n",
      "Epoch 139/210\n",
      "379/379 - 0s - loss: 0.4548 - accuracy: 0.8098 - val_loss: 0.4773 - val_accuracy: 0.8433\n",
      "Epoch 140/210\n",
      "379/379 - 0s - loss: 0.4604 - accuracy: 0.8309 - val_loss: 0.4825 - val_accuracy: 0.8358\n",
      "Epoch 141/210\n",
      "379/379 - 0s - loss: 0.4666 - accuracy: 0.8177 - val_loss: 0.4678 - val_accuracy: 0.8433\n",
      "Epoch 142/210\n",
      "379/379 - 0s - loss: 0.4447 - accuracy: 0.8111 - val_loss: 0.4868 - val_accuracy: 0.8284\n",
      "Epoch 143/210\n",
      "379/379 - 0s - loss: 0.4383 - accuracy: 0.8243 - val_loss: 0.4747 - val_accuracy: 0.8433\n",
      "Epoch 144/210\n",
      "379/379 - 0s - loss: 0.4548 - accuracy: 0.8151 - val_loss: 0.4738 - val_accuracy: 0.8284\n",
      "Epoch 145/210\n",
      "379/379 - 0s - loss: 0.4380 - accuracy: 0.8269 - val_loss: 0.5115 - val_accuracy: 0.8134\n",
      "Epoch 146/210\n",
      "379/379 - 0s - loss: 0.4423 - accuracy: 0.8243 - val_loss: 0.4760 - val_accuracy: 0.8358\n",
      "Epoch 147/210\n",
      "379/379 - 0s - loss: 0.4526 - accuracy: 0.8243 - val_loss: 0.4855 - val_accuracy: 0.8284\n",
      "Epoch 148/210\n",
      "379/379 - 0s - loss: 0.4386 - accuracy: 0.8164 - val_loss: 0.4906 - val_accuracy: 0.8358\n",
      "Epoch 149/210\n",
      "379/379 - 0s - loss: 0.4446 - accuracy: 0.8203 - val_loss: 0.4836 - val_accuracy: 0.8358\n",
      "Epoch 150/210\n",
      "379/379 - 0s - loss: 0.4659 - accuracy: 0.8230 - val_loss: 0.4781 - val_accuracy: 0.8209\n",
      "Epoch 151/210\n",
      "379/379 - 0s - loss: 0.4193 - accuracy: 0.8164 - val_loss: 0.4831 - val_accuracy: 0.8209\n",
      "Epoch 152/210\n",
      "379/379 - 0s - loss: 0.4500 - accuracy: 0.8151 - val_loss: 0.4873 - val_accuracy: 0.8284\n",
      "Epoch 153/210\n",
      "379/379 - 0s - loss: 0.4314 - accuracy: 0.8322 - val_loss: 0.4856 - val_accuracy: 0.8284\n",
      "Epoch 154/210\n",
      "379/379 - 0s - loss: 0.4552 - accuracy: 0.8124 - val_loss: 0.4797 - val_accuracy: 0.8284\n",
      "Epoch 155/210\n",
      "379/379 - 0s - loss: 0.4487 - accuracy: 0.8190 - val_loss: 0.4590 - val_accuracy: 0.8433\n",
      "Epoch 156/210\n",
      "379/379 - 0s - loss: 0.4406 - accuracy: 0.8283 - val_loss: 0.4965 - val_accuracy: 0.8209\n",
      "Epoch 157/210\n",
      "379/379 - 0s - loss: 0.4430 - accuracy: 0.8269 - val_loss: 0.4935 - val_accuracy: 0.8134\n",
      "Epoch 158/210\n",
      "379/379 - 0s - loss: 0.4362 - accuracy: 0.8190 - val_loss: 0.4714 - val_accuracy: 0.8284\n",
      "Epoch 159/210\n",
      "379/379 - 0s - loss: 0.4450 - accuracy: 0.8309 - val_loss: 0.4934 - val_accuracy: 0.8209\n",
      "Epoch 160/210\n",
      "379/379 - 0s - loss: 0.4459 - accuracy: 0.8336 - val_loss: 0.5017 - val_accuracy: 0.8209\n",
      "Epoch 161/210\n",
      "379/379 - 0s - loss: 0.4375 - accuracy: 0.8269 - val_loss: 0.4842 - val_accuracy: 0.8284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/210\n",
      "379/379 - 0s - loss: 0.4605 - accuracy: 0.8085 - val_loss: 0.5053 - val_accuracy: 0.8284\n",
      "Epoch 163/210\n",
      "379/379 - 0s - loss: 0.4581 - accuracy: 0.8124 - val_loss: 0.4624 - val_accuracy: 0.8358\n",
      "Epoch 164/210\n",
      "379/379 - 0s - loss: 0.4399 - accuracy: 0.8177 - val_loss: 0.4757 - val_accuracy: 0.8284\n",
      "Epoch 165/210\n",
      "379/379 - 0s - loss: 0.4147 - accuracy: 0.8428 - val_loss: 0.4720 - val_accuracy: 0.8433\n",
      "Epoch 166/210\n",
      "379/379 - 0s - loss: 0.4619 - accuracy: 0.8283 - val_loss: 0.4774 - val_accuracy: 0.8284\n",
      "Epoch 167/210\n",
      "379/379 - 0s - loss: 0.4389 - accuracy: 0.8203 - val_loss: 0.4803 - val_accuracy: 0.8358\n",
      "Epoch 168/210\n",
      "379/379 - 0s - loss: 0.4376 - accuracy: 0.8164 - val_loss: 0.4821 - val_accuracy: 0.8433\n",
      "Epoch 169/210\n",
      "379/379 - 0s - loss: 0.4537 - accuracy: 0.8203 - val_loss: 0.4926 - val_accuracy: 0.8358\n",
      "Epoch 170/210\n",
      "379/379 - 0s - loss: 0.4347 - accuracy: 0.8283 - val_loss: 0.4903 - val_accuracy: 0.8358\n",
      "Epoch 171/210\n",
      "379/379 - 0s - loss: 0.4302 - accuracy: 0.8309 - val_loss: 0.4904 - val_accuracy: 0.8284\n",
      "Epoch 172/210\n",
      "379/379 - 0s - loss: 0.4257 - accuracy: 0.8415 - val_loss: 0.4992 - val_accuracy: 0.8358\n",
      "Epoch 173/210\n",
      "379/379 - 0s - loss: 0.4463 - accuracy: 0.8243 - val_loss: 0.4694 - val_accuracy: 0.8433\n",
      "Epoch 174/210\n",
      "379/379 - 0s - loss: 0.4306 - accuracy: 0.8137 - val_loss: 0.4833 - val_accuracy: 0.8433\n",
      "Epoch 175/210\n",
      "379/379 - 0s - loss: 0.4475 - accuracy: 0.8322 - val_loss: 0.4830 - val_accuracy: 0.8358\n",
      "Epoch 176/210\n",
      "379/379 - 0s - loss: 0.4409 - accuracy: 0.8203 - val_loss: 0.4704 - val_accuracy: 0.8358\n",
      "Epoch 177/210\n",
      "379/379 - 0s - loss: 0.4488 - accuracy: 0.8217 - val_loss: 0.4765 - val_accuracy: 0.8358\n",
      "Epoch 178/210\n",
      "379/379 - 0s - loss: 0.4325 - accuracy: 0.8217 - val_loss: 0.4888 - val_accuracy: 0.8433\n",
      "Epoch 179/210\n",
      "379/379 - 0s - loss: 0.4558 - accuracy: 0.8230 - val_loss: 0.4699 - val_accuracy: 0.8209\n",
      "Epoch 180/210\n",
      "379/379 - 0s - loss: 0.4464 - accuracy: 0.8137 - val_loss: 0.5080 - val_accuracy: 0.8284\n",
      "Epoch 181/210\n",
      "379/379 - 0s - loss: 0.4604 - accuracy: 0.8243 - val_loss: 0.4944 - val_accuracy: 0.8284\n",
      "Epoch 182/210\n",
      "379/379 - 0s - loss: 0.4453 - accuracy: 0.8256 - val_loss: 0.4776 - val_accuracy: 0.8358\n",
      "Epoch 183/210\n",
      "379/379 - 0s - loss: 0.4322 - accuracy: 0.8296 - val_loss: 0.4998 - val_accuracy: 0.8134\n",
      "Epoch 184/210\n",
      "379/379 - 0s - loss: 0.4295 - accuracy: 0.8283 - val_loss: 0.4807 - val_accuracy: 0.8433\n",
      "Epoch 185/210\n",
      "379/379 - 0s - loss: 0.4485 - accuracy: 0.8217 - val_loss: 0.4909 - val_accuracy: 0.8209\n",
      "Epoch 186/210\n",
      "379/379 - 0s - loss: 0.4494 - accuracy: 0.8349 - val_loss: 0.5073 - val_accuracy: 0.8358\n",
      "Epoch 187/210\n",
      "379/379 - 0s - loss: 0.4581 - accuracy: 0.8230 - val_loss: 0.4883 - val_accuracy: 0.8284\n",
      "Epoch 188/210\n",
      "379/379 - 0s - loss: 0.4306 - accuracy: 0.8336 - val_loss: 0.4955 - val_accuracy: 0.8284\n",
      "Epoch 189/210\n",
      "379/379 - 0s - loss: 0.4339 - accuracy: 0.8243 - val_loss: 0.4772 - val_accuracy: 0.8284\n",
      "Epoch 190/210\n",
      "379/379 - 0s - loss: 0.4207 - accuracy: 0.8349 - val_loss: 0.4908 - val_accuracy: 0.8358\n",
      "Epoch 191/210\n",
      "379/379 - 0s - loss: 0.4683 - accuracy: 0.8151 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 192/210\n",
      "379/379 - 0s - loss: 0.4335 - accuracy: 0.8336 - val_loss: 0.5021 - val_accuracy: 0.8134\n",
      "Epoch 193/210\n",
      "379/379 - 0s - loss: 0.4484 - accuracy: 0.8177 - val_loss: 0.4856 - val_accuracy: 0.8284\n",
      "Epoch 194/210\n",
      "379/379 - 0s - loss: 0.4570 - accuracy: 0.8322 - val_loss: 0.4904 - val_accuracy: 0.8209\n",
      "Epoch 195/210\n",
      "379/379 - 0s - loss: 0.4435 - accuracy: 0.8190 - val_loss: 0.5076 - val_accuracy: 0.8433\n",
      "Epoch 196/210\n",
      "379/379 - 0s - loss: 0.4504 - accuracy: 0.8269 - val_loss: 0.4855 - val_accuracy: 0.8209\n",
      "Epoch 197/210\n",
      "379/379 - 0s - loss: 0.4537 - accuracy: 0.8309 - val_loss: 0.4791 - val_accuracy: 0.8358\n",
      "Epoch 198/210\n",
      "379/379 - 0s - loss: 0.4363 - accuracy: 0.8217 - val_loss: 0.4741 - val_accuracy: 0.8284\n",
      "Epoch 199/210\n",
      "379/379 - 0s - loss: 0.4598 - accuracy: 0.8296 - val_loss: 0.4613 - val_accuracy: 0.8358\n",
      "Epoch 200/210\n",
      "379/379 - 0s - loss: 0.4527 - accuracy: 0.8111 - val_loss: 0.4811 - val_accuracy: 0.8284\n",
      "Epoch 201/210\n",
      "379/379 - 0s - loss: 0.4466 - accuracy: 0.8388 - val_loss: 0.5007 - val_accuracy: 0.8433\n",
      "Epoch 202/210\n",
      "379/379 - 0s - loss: 0.4526 - accuracy: 0.8230 - val_loss: 0.4897 - val_accuracy: 0.8358\n",
      "Epoch 203/210\n",
      "379/379 - 0s - loss: 0.4368 - accuracy: 0.8243 - val_loss: 0.5286 - val_accuracy: 0.8209\n",
      "Epoch 204/210\n",
      "379/379 - 0s - loss: 0.4252 - accuracy: 0.8322 - val_loss: 0.4865 - val_accuracy: 0.8358\n",
      "Epoch 205/210\n",
      "379/379 - 0s - loss: 0.4622 - accuracy: 0.8177 - val_loss: 0.4749 - val_accuracy: 0.8209\n",
      "Epoch 206/210\n",
      "379/379 - 0s - loss: 0.4326 - accuracy: 0.8190 - val_loss: 0.4805 - val_accuracy: 0.8433\n",
      "Epoch 207/210\n",
      "379/379 - 0s - loss: 0.4434 - accuracy: 0.8230 - val_loss: 0.4876 - val_accuracy: 0.8433\n",
      "Epoch 208/210\n",
      "379/379 - 0s - loss: 0.4613 - accuracy: 0.8124 - val_loss: 0.4886 - val_accuracy: 0.8433\n",
      "Epoch 209/210\n",
      "379/379 - 0s - loss: 0.4303 - accuracy: 0.8309 - val_loss: 0.4939 - val_accuracy: 0.8284\n",
      "Epoch 210/210\n",
      "379/379 - 0s - loss: 0.4327 - accuracy: 0.8269 - val_loss: 0.5093 - val_accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from keras.layers import Dropout\n",
    "def build_classifier(loss):\n",
    "    ### Initializing the ANN\n",
    "    ann_p = tf.keras.models.Sequential()\n",
    "    ### Adding the input layer and the hidden layers\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ### Compiling the ANN\n",
    "    ann_p.compile(optimizer = 'RMSprop', loss = loss, metrics = ['accuracy'])\n",
    "    return ann_p\n",
    "ann_p = KerasClassifier(build_fn = build_classifier, batch_size = 8, epochs = 200, \n",
    "                        validation_data=(X_test,y_test), verbose=2)\n",
    "parameters = {\"batch_size\": [2,5,10,14,20], \n",
    "             \"epochs\": [80,140,180,210],\n",
    "             \"loss\": [\"mean_squared_error\",\"poisson\", \"binary_crossentropy\"]}\n",
    "grid_search = GridSearchCV(estimator=ann_p, param_grid=parameters, scoring=\"accuracy\", cv=6, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09806261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362392200974877 {'batch_size': 2, 'epochs': 210, 'loss': 'binary_crossentropy'}\n"
     ]
    }
   ],
   "source": [
    "print(best_accuracy, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d77ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from keras.layers import Dropout\n",
    "def build_classifier():\n",
    "    ### Initializing the ANN\n",
    "    ann_p = tf.keras.models.Sequential()\n",
    "    ### Adding the input layer and the hidden layers\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=16, activation='elu'))\n",
    "    ann_p.add(Dropout(0.3))\n",
    "    ann_p.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ### Compiling the ANN\n",
    "    ann_p.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann_p\n",
    "ann_p = KerasClassifier(build_fn = build_classifier, batch_size = 2, epochs = 210, \n",
    "                        validation_data=(X_test,y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35602ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77952754 0.86507934 0.8174603  0.85714287 0.8174603  0.78571427]\n",
      "[0.8203974366188049] [0.0322418544823061]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=ann_p, X=X_train, y=y_train, cv=6, n_jobs=-1, verbose=1)\n",
    "print(accuracies)\n",
    "mean = accuracies.mean()\n",
    "std = accuracies.std()\n",
    "print([mean], [std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdeeeb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210\n",
      "67/67 - 1s - loss: 0.5548 - accuracy: 0.7164 - val_loss: 0.4907 - val_accuracy: 0.8060\n",
      "Epoch 2/210\n",
      "67/67 - 0s - loss: 0.4954 - accuracy: 0.8060 - val_loss: 0.4437 - val_accuracy: 0.7985\n",
      "Epoch 3/210\n",
      "67/67 - 0s - loss: 0.4951 - accuracy: 0.7910 - val_loss: 0.4159 - val_accuracy: 0.7910\n",
      "Epoch 4/210\n",
      "67/67 - 0s - loss: 0.4853 - accuracy: 0.7836 - val_loss: 0.4002 - val_accuracy: 0.7761\n",
      "Epoch 5/210\n",
      "67/67 - 0s - loss: 0.4222 - accuracy: 0.8134 - val_loss: 0.3931 - val_accuracy: 0.8060\n",
      "Epoch 6/210\n",
      "67/67 - 0s - loss: 0.4223 - accuracy: 0.7910 - val_loss: 0.3896 - val_accuracy: 0.8134\n",
      "Epoch 7/210\n",
      "67/67 - 0s - loss: 0.3975 - accuracy: 0.8209 - val_loss: 0.3869 - val_accuracy: 0.8284\n",
      "Epoch 8/210\n",
      "67/67 - 0s - loss: 0.4781 - accuracy: 0.7910 - val_loss: 0.3863 - val_accuracy: 0.8209\n",
      "Epoch 9/210\n",
      "67/67 - 0s - loss: 0.4229 - accuracy: 0.8284 - val_loss: 0.3860 - val_accuracy: 0.8209\n",
      "Epoch 10/210\n",
      "67/67 - 0s - loss: 0.4450 - accuracy: 0.8209 - val_loss: 0.3882 - val_accuracy: 0.8358\n",
      "Epoch 11/210\n",
      "67/67 - 0s - loss: 0.4484 - accuracy: 0.8134 - val_loss: 0.3888 - val_accuracy: 0.8358\n",
      "Epoch 12/210\n",
      "67/67 - 0s - loss: 0.4329 - accuracy: 0.7910 - val_loss: 0.3874 - val_accuracy: 0.8358\n",
      "Epoch 13/210\n",
      "67/67 - 0s - loss: 0.4101 - accuracy: 0.8284 - val_loss: 0.3868 - val_accuracy: 0.8209\n",
      "Epoch 14/210\n",
      "67/67 - 0s - loss: 0.5333 - accuracy: 0.8134 - val_loss: 0.3840 - val_accuracy: 0.8209\n",
      "Epoch 15/210\n",
      "67/67 - 0s - loss: 0.4833 - accuracy: 0.7761 - val_loss: 0.3820 - val_accuracy: 0.8284\n",
      "Epoch 16/210\n",
      "67/67 - 0s - loss: 0.3624 - accuracy: 0.8433 - val_loss: 0.3829 - val_accuracy: 0.8358\n",
      "Epoch 17/210\n",
      "67/67 - 0s - loss: 0.4492 - accuracy: 0.8060 - val_loss: 0.3815 - val_accuracy: 0.8284\n",
      "Epoch 18/210\n",
      "67/67 - 0s - loss: 0.4615 - accuracy: 0.7836 - val_loss: 0.3819 - val_accuracy: 0.8433\n",
      "Epoch 19/210\n",
      "67/67 - 0s - loss: 0.4279 - accuracy: 0.8209 - val_loss: 0.3816 - val_accuracy: 0.8358\n",
      "Epoch 20/210\n",
      "67/67 - 0s - loss: 0.4215 - accuracy: 0.8134 - val_loss: 0.3817 - val_accuracy: 0.8284\n",
      "Epoch 21/210\n",
      "67/67 - 0s - loss: 0.4228 - accuracy: 0.8134 - val_loss: 0.3828 - val_accuracy: 0.8358\n",
      "Epoch 22/210\n",
      "67/67 - 0s - loss: 0.3428 - accuracy: 0.8433 - val_loss: 0.3850 - val_accuracy: 0.8358\n",
      "Epoch 23/210\n",
      "67/67 - 0s - loss: 0.4381 - accuracy: 0.7761 - val_loss: 0.3820 - val_accuracy: 0.8358\n",
      "Epoch 24/210\n",
      "67/67 - 0s - loss: 0.3695 - accuracy: 0.8134 - val_loss: 0.3838 - val_accuracy: 0.8358\n",
      "Epoch 25/210\n",
      "67/67 - 0s - loss: 0.3972 - accuracy: 0.8358 - val_loss: 0.3844 - val_accuracy: 0.8284\n",
      "Epoch 26/210\n",
      "67/67 - 0s - loss: 0.4392 - accuracy: 0.7985 - val_loss: 0.3817 - val_accuracy: 0.8284\n",
      "Epoch 27/210\n",
      "67/67 - 0s - loss: 0.4545 - accuracy: 0.8209 - val_loss: 0.3813 - val_accuracy: 0.8284\n",
      "Epoch 28/210\n",
      "67/67 - 0s - loss: 0.4613 - accuracy: 0.8284 - val_loss: 0.3794 - val_accuracy: 0.8284\n",
      "Epoch 29/210\n",
      "67/67 - 0s - loss: 0.4120 - accuracy: 0.8284 - val_loss: 0.3823 - val_accuracy: 0.8284\n",
      "Epoch 30/210\n",
      "67/67 - 0s - loss: 0.4572 - accuracy: 0.8134 - val_loss: 0.3787 - val_accuracy: 0.8284\n",
      "Epoch 31/210\n",
      "67/67 - 0s - loss: 0.4850 - accuracy: 0.8209 - val_loss: 0.3761 - val_accuracy: 0.8209\n",
      "Epoch 32/210\n",
      "67/67 - 0s - loss: 0.4657 - accuracy: 0.8060 - val_loss: 0.3765 - val_accuracy: 0.8209\n",
      "Epoch 33/210\n",
      "67/67 - 0s - loss: 0.4123 - accuracy: 0.8209 - val_loss: 0.3771 - val_accuracy: 0.8284\n",
      "Epoch 34/210\n",
      "67/67 - 0s - loss: 0.4381 - accuracy: 0.8060 - val_loss: 0.3752 - val_accuracy: 0.8209\n",
      "Epoch 35/210\n",
      "67/67 - 0s - loss: 0.3910 - accuracy: 0.8433 - val_loss: 0.3751 - val_accuracy: 0.8209\n",
      "Epoch 36/210\n",
      "67/67 - 0s - loss: 0.4309 - accuracy: 0.8284 - val_loss: 0.3713 - val_accuracy: 0.8209\n",
      "Epoch 37/210\n",
      "67/67 - 0s - loss: 0.4218 - accuracy: 0.8060 - val_loss: 0.3728 - val_accuracy: 0.8209\n",
      "Epoch 38/210\n",
      "67/67 - 0s - loss: 0.4476 - accuracy: 0.8060 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 39/210\n",
      "67/67 - 0s - loss: 0.4099 - accuracy: 0.8284 - val_loss: 0.3736 - val_accuracy: 0.8209\n",
      "Epoch 40/210\n",
      "67/67 - 0s - loss: 0.4072 - accuracy: 0.8358 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 41/210\n",
      "67/67 - 0s - loss: 0.4277 - accuracy: 0.8433 - val_loss: 0.3739 - val_accuracy: 0.8209\n",
      "Epoch 42/210\n",
      "67/67 - 0s - loss: 0.4184 - accuracy: 0.8358 - val_loss: 0.3729 - val_accuracy: 0.8209\n",
      "Epoch 43/210\n",
      "67/67 - 0s - loss: 0.4684 - accuracy: 0.8060 - val_loss: 0.3696 - val_accuracy: 0.8284\n",
      "Epoch 44/210\n",
      "67/67 - 0s - loss: 0.4412 - accuracy: 0.7985 - val_loss: 0.3690 - val_accuracy: 0.8358\n",
      "Epoch 45/210\n",
      "67/67 - 0s - loss: 0.4210 - accuracy: 0.8209 - val_loss: 0.3689 - val_accuracy: 0.8358\n",
      "Epoch 46/210\n",
      "67/67 - 0s - loss: 0.4239 - accuracy: 0.8358 - val_loss: 0.3688 - val_accuracy: 0.8358\n",
      "Epoch 47/210\n",
      "67/67 - 0s - loss: 0.4177 - accuracy: 0.8507 - val_loss: 0.3705 - val_accuracy: 0.8433\n",
      "Epoch 48/210\n",
      "67/67 - 0s - loss: 0.4255 - accuracy: 0.8284 - val_loss: 0.3699 - val_accuracy: 0.8358\n",
      "Epoch 49/210\n",
      "67/67 - 0s - loss: 0.4460 - accuracy: 0.8433 - val_loss: 0.3681 - val_accuracy: 0.8358\n",
      "Epoch 50/210\n",
      "67/67 - 0s - loss: 0.3738 - accuracy: 0.8433 - val_loss: 0.3685 - val_accuracy: 0.8358\n",
      "Epoch 51/210\n",
      "67/67 - 0s - loss: 0.4304 - accuracy: 0.8284 - val_loss: 0.3683 - val_accuracy: 0.8358\n",
      "Epoch 52/210\n",
      "67/67 - 0s - loss: 0.4219 - accuracy: 0.8358 - val_loss: 0.3706 - val_accuracy: 0.8433\n",
      "Epoch 53/210\n",
      "67/67 - 0s - loss: 0.4347 - accuracy: 0.8134 - val_loss: 0.3718 - val_accuracy: 0.8358\n",
      "Epoch 54/210\n",
      "67/67 - 0s - loss: 0.4226 - accuracy: 0.8209 - val_loss: 0.3703 - val_accuracy: 0.8358\n",
      "Epoch 55/210\n",
      "67/67 - 0s - loss: 0.4042 - accuracy: 0.8507 - val_loss: 0.3698 - val_accuracy: 0.8358\n",
      "Epoch 56/210\n",
      "67/67 - 0s - loss: 0.4405 - accuracy: 0.8060 - val_loss: 0.3687 - val_accuracy: 0.8358\n",
      "Epoch 57/210\n",
      "67/67 - 0s - loss: 0.4004 - accuracy: 0.8209 - val_loss: 0.3698 - val_accuracy: 0.8358\n",
      "Epoch 58/210\n",
      "67/67 - 0s - loss: 0.3783 - accuracy: 0.8060 - val_loss: 0.3707 - val_accuracy: 0.8358\n",
      "Epoch 59/210\n",
      "67/67 - 0s - loss: 0.3948 - accuracy: 0.8358 - val_loss: 0.3706 - val_accuracy: 0.8358\n",
      "Epoch 60/210\n",
      "67/67 - 0s - loss: 0.4275 - accuracy: 0.8284 - val_loss: 0.3705 - val_accuracy: 0.8284\n",
      "Epoch 61/210\n",
      "67/67 - 0s - loss: 0.3470 - accuracy: 0.8507 - val_loss: 0.3699 - val_accuracy: 0.8284\n",
      "Epoch 62/210\n",
      "67/67 - 0s - loss: 0.4129 - accuracy: 0.8433 - val_loss: 0.3680 - val_accuracy: 0.8284\n",
      "Epoch 63/210\n",
      "67/67 - 0s - loss: 0.4483 - accuracy: 0.8209 - val_loss: 0.3675 - val_accuracy: 0.8284\n",
      "Epoch 64/210\n",
      "67/67 - 0s - loss: 0.4037 - accuracy: 0.8134 - val_loss: 0.3669 - val_accuracy: 0.8284\n",
      "Epoch 65/210\n",
      "67/67 - 0s - loss: 0.3743 - accuracy: 0.8731 - val_loss: 0.3667 - val_accuracy: 0.8358\n",
      "Epoch 66/210\n",
      "67/67 - 0s - loss: 0.3909 - accuracy: 0.8209 - val_loss: 0.3691 - val_accuracy: 0.8358\n",
      "Epoch 67/210\n",
      "67/67 - 0s - loss: 0.4464 - accuracy: 0.8134 - val_loss: 0.3676 - val_accuracy: 0.8358\n",
      "Epoch 68/210\n",
      "67/67 - 0s - loss: 0.4553 - accuracy: 0.8134 - val_loss: 0.3663 - val_accuracy: 0.8433\n",
      "Epoch 69/210\n",
      "67/67 - 0s - loss: 0.4390 - accuracy: 0.8507 - val_loss: 0.3633 - val_accuracy: 0.8358\n",
      "Epoch 70/210\n",
      "67/67 - 0s - loss: 0.4180 - accuracy: 0.8209 - val_loss: 0.3620 - val_accuracy: 0.8358\n",
      "Epoch 71/210\n",
      "67/67 - 0s - loss: 0.3669 - accuracy: 0.8433 - val_loss: 0.3650 - val_accuracy: 0.8358\n",
      "Epoch 72/210\n",
      "67/67 - 0s - loss: 0.4522 - accuracy: 0.8134 - val_loss: 0.3624 - val_accuracy: 0.8433\n",
      "Epoch 73/210\n",
      "67/67 - 0s - loss: 0.3445 - accuracy: 0.8507 - val_loss: 0.3670 - val_accuracy: 0.8358\n",
      "Epoch 74/210\n",
      "67/67 - 0s - loss: 0.4028 - accuracy: 0.8134 - val_loss: 0.3675 - val_accuracy: 0.8358\n",
      "Epoch 75/210\n",
      "67/67 - 0s - loss: 0.4135 - accuracy: 0.8358 - val_loss: 0.3677 - val_accuracy: 0.8433\n",
      "Epoch 76/210\n",
      "67/67 - 0s - loss: 0.4110 - accuracy: 0.8358 - val_loss: 0.3692 - val_accuracy: 0.8433\n",
      "Epoch 77/210\n",
      "67/67 - 0s - loss: 0.3966 - accuracy: 0.8507 - val_loss: 0.3672 - val_accuracy: 0.8433\n",
      "Epoch 78/210\n",
      "67/67 - 0s - loss: 0.3855 - accuracy: 0.8582 - val_loss: 0.3658 - val_accuracy: 0.8433\n",
      "Epoch 79/210\n",
      "67/67 - 0s - loss: 0.4296 - accuracy: 0.8284 - val_loss: 0.3628 - val_accuracy: 0.8358\n",
      "Epoch 80/210\n",
      "67/67 - 0s - loss: 0.3934 - accuracy: 0.8433 - val_loss: 0.3616 - val_accuracy: 0.8433\n",
      "Epoch 81/210\n",
      "67/67 - 0s - loss: 0.3528 - accuracy: 0.8284 - val_loss: 0.3634 - val_accuracy: 0.8358\n",
      "Epoch 82/210\n",
      "67/67 - 0s - loss: 0.3491 - accuracy: 0.8731 - val_loss: 0.3639 - val_accuracy: 0.8358\n",
      "Epoch 83/210\n",
      "67/67 - 0s - loss: 0.4120 - accuracy: 0.8358 - val_loss: 0.3660 - val_accuracy: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/210\n",
      "67/67 - 0s - loss: 0.3809 - accuracy: 0.8582 - val_loss: 0.3650 - val_accuracy: 0.8433\n",
      "Epoch 85/210\n",
      "67/67 - 0s - loss: 0.4149 - accuracy: 0.8134 - val_loss: 0.3625 - val_accuracy: 0.8433\n",
      "Epoch 86/210\n",
      "67/67 - 0s - loss: 0.4050 - accuracy: 0.8284 - val_loss: 0.3636 - val_accuracy: 0.8433\n",
      "Epoch 87/210\n",
      "67/67 - 0s - loss: 0.4055 - accuracy: 0.8284 - val_loss: 0.3646 - val_accuracy: 0.8433\n",
      "Epoch 88/210\n",
      "67/67 - 0s - loss: 0.4921 - accuracy: 0.8134 - val_loss: 0.3629 - val_accuracy: 0.8433\n",
      "Epoch 89/210\n",
      "67/67 - 0s - loss: 0.4631 - accuracy: 0.7612 - val_loss: 0.3605 - val_accuracy: 0.8433\n",
      "Epoch 90/210\n",
      "67/67 - 0s - loss: 0.3785 - accuracy: 0.8433 - val_loss: 0.3619 - val_accuracy: 0.8433\n",
      "Epoch 91/210\n",
      "67/67 - 0s - loss: 0.4043 - accuracy: 0.8433 - val_loss: 0.3625 - val_accuracy: 0.8433\n",
      "Epoch 92/210\n",
      "67/67 - 0s - loss: 0.3751 - accuracy: 0.8209 - val_loss: 0.3617 - val_accuracy: 0.8433\n",
      "Epoch 93/210\n",
      "67/67 - 0s - loss: 0.4035 - accuracy: 0.8358 - val_loss: 0.3619 - val_accuracy: 0.8433\n",
      "Epoch 94/210\n",
      "67/67 - 0s - loss: 0.4516 - accuracy: 0.8060 - val_loss: 0.3609 - val_accuracy: 0.8433\n",
      "Epoch 95/210\n",
      "67/67 - 0s - loss: 0.4005 - accuracy: 0.8582 - val_loss: 0.3602 - val_accuracy: 0.8433\n",
      "Epoch 96/210\n",
      "67/67 - 0s - loss: 0.3884 - accuracy: 0.8284 - val_loss: 0.3590 - val_accuracy: 0.8433\n",
      "Epoch 97/210\n",
      "67/67 - 0s - loss: 0.4357 - accuracy: 0.8209 - val_loss: 0.3572 - val_accuracy: 0.8433\n",
      "Epoch 98/210\n",
      "67/67 - 0s - loss: 0.3941 - accuracy: 0.8806 - val_loss: 0.3561 - val_accuracy: 0.8433\n",
      "Epoch 99/210\n",
      "67/67 - 0s - loss: 0.4223 - accuracy: 0.8582 - val_loss: 0.3561 - val_accuracy: 0.8358\n",
      "Epoch 100/210\n",
      "67/67 - 0s - loss: 0.4370 - accuracy: 0.7985 - val_loss: 0.3566 - val_accuracy: 0.8433\n",
      "Epoch 101/210\n",
      "67/67 - 0s - loss: 0.4019 - accuracy: 0.8358 - val_loss: 0.3540 - val_accuracy: 0.8433\n",
      "Epoch 102/210\n",
      "67/67 - 0s - loss: 0.3926 - accuracy: 0.8358 - val_loss: 0.3554 - val_accuracy: 0.8433\n",
      "Epoch 103/210\n",
      "67/67 - 0s - loss: 0.4142 - accuracy: 0.8209 - val_loss: 0.3548 - val_accuracy: 0.8433\n",
      "Epoch 104/210\n",
      "67/67 - 0s - loss: 0.3861 - accuracy: 0.8358 - val_loss: 0.3552 - val_accuracy: 0.8582\n",
      "Epoch 105/210\n",
      "67/67 - 0s - loss: 0.4014 - accuracy: 0.8358 - val_loss: 0.3571 - val_accuracy: 0.8433\n",
      "Epoch 106/210\n",
      "67/67 - 0s - loss: 0.3164 - accuracy: 0.8582 - val_loss: 0.3583 - val_accuracy: 0.8433\n",
      "Epoch 107/210\n",
      "67/67 - 0s - loss: 0.3824 - accuracy: 0.8284 - val_loss: 0.3559 - val_accuracy: 0.8433\n",
      "Epoch 108/210\n",
      "67/67 - 0s - loss: 0.4127 - accuracy: 0.8209 - val_loss: 0.3561 - val_accuracy: 0.8433\n",
      "Epoch 109/210\n",
      "67/67 - 0s - loss: 0.3894 - accuracy: 0.8433 - val_loss: 0.3576 - val_accuracy: 0.8433\n",
      "Epoch 110/210\n",
      "67/67 - 0s - loss: 0.3898 - accuracy: 0.8358 - val_loss: 0.3595 - val_accuracy: 0.8433\n",
      "Epoch 111/210\n",
      "67/67 - 0s - loss: 0.4096 - accuracy: 0.8433 - val_loss: 0.3590 - val_accuracy: 0.8433\n",
      "Epoch 112/210\n",
      "67/67 - 0s - loss: 0.3689 - accuracy: 0.8881 - val_loss: 0.3559 - val_accuracy: 0.8433\n",
      "Epoch 113/210\n",
      "67/67 - 0s - loss: 0.3998 - accuracy: 0.8284 - val_loss: 0.3540 - val_accuracy: 0.8433\n",
      "Epoch 114/210\n",
      "67/67 - 0s - loss: 0.4277 - accuracy: 0.8433 - val_loss: 0.3533 - val_accuracy: 0.8433\n",
      "Epoch 115/210\n",
      "67/67 - 0s - loss: 0.4590 - accuracy: 0.7836 - val_loss: 0.3547 - val_accuracy: 0.8433\n",
      "Epoch 116/210\n",
      "67/67 - 0s - loss: 0.3761 - accuracy: 0.8582 - val_loss: 0.3590 - val_accuracy: 0.8507\n",
      "Epoch 117/210\n",
      "67/67 - 0s - loss: 0.4002 - accuracy: 0.8134 - val_loss: 0.3583 - val_accuracy: 0.8507\n",
      "Epoch 118/210\n",
      "67/67 - 0s - loss: 0.4069 - accuracy: 0.8209 - val_loss: 0.3570 - val_accuracy: 0.8507\n",
      "Epoch 119/210\n",
      "67/67 - 0s - loss: 0.3812 - accuracy: 0.8731 - val_loss: 0.3562 - val_accuracy: 0.8582\n",
      "Epoch 120/210\n",
      "67/67 - 0s - loss: 0.3527 - accuracy: 0.8657 - val_loss: 0.3588 - val_accuracy: 0.8507\n",
      "Epoch 121/210\n",
      "67/67 - 0s - loss: 0.4073 - accuracy: 0.8209 - val_loss: 0.3557 - val_accuracy: 0.8507\n",
      "Epoch 122/210\n",
      "67/67 - 0s - loss: 0.4313 - accuracy: 0.8134 - val_loss: 0.3546 - val_accuracy: 0.8507\n",
      "Epoch 123/210\n",
      "67/67 - 0s - loss: 0.3790 - accuracy: 0.8582 - val_loss: 0.3563 - val_accuracy: 0.8507\n",
      "Epoch 124/210\n",
      "67/67 - 0s - loss: 0.4048 - accuracy: 0.8284 - val_loss: 0.3550 - val_accuracy: 0.8507\n",
      "Epoch 125/210\n",
      "67/67 - 0s - loss: 0.4164 - accuracy: 0.8358 - val_loss: 0.3572 - val_accuracy: 0.8582\n",
      "Epoch 126/210\n",
      "67/67 - 0s - loss: 0.4060 - accuracy: 0.8060 - val_loss: 0.3569 - val_accuracy: 0.8507\n",
      "Epoch 127/210\n",
      "67/67 - 0s - loss: 0.4075 - accuracy: 0.8507 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 128/210\n",
      "67/67 - 0s - loss: 0.3523 - accuracy: 0.8582 - val_loss: 0.3584 - val_accuracy: 0.8582\n",
      "Epoch 129/210\n",
      "67/67 - 0s - loss: 0.4205 - accuracy: 0.8284 - val_loss: 0.3551 - val_accuracy: 0.8582\n",
      "Epoch 130/210\n",
      "67/67 - 0s - loss: 0.3785 - accuracy: 0.8582 - val_loss: 0.3548 - val_accuracy: 0.8582\n",
      "Epoch 131/210\n",
      "67/67 - 0s - loss: 0.3650 - accuracy: 0.8507 - val_loss: 0.3559 - val_accuracy: 0.8582\n",
      "Epoch 132/210\n",
      "67/67 - 0s - loss: 0.3952 - accuracy: 0.8657 - val_loss: 0.3532 - val_accuracy: 0.8507\n",
      "Epoch 133/210\n",
      "67/67 - 0s - loss: 0.4084 - accuracy: 0.8209 - val_loss: 0.3520 - val_accuracy: 0.8507\n",
      "Epoch 134/210\n",
      "67/67 - 0s - loss: 0.4157 - accuracy: 0.8582 - val_loss: 0.3496 - val_accuracy: 0.8507\n",
      "Epoch 135/210\n",
      "67/67 - 0s - loss: 0.3836 - accuracy: 0.8284 - val_loss: 0.3489 - val_accuracy: 0.8507\n",
      "Epoch 136/210\n",
      "67/67 - 0s - loss: 0.4059 - accuracy: 0.8209 - val_loss: 0.3495 - val_accuracy: 0.8507\n",
      "Epoch 137/210\n",
      "67/67 - 0s - loss: 0.3863 - accuracy: 0.8284 - val_loss: 0.3532 - val_accuracy: 0.8582\n",
      "Epoch 138/210\n",
      "67/67 - 0s - loss: 0.3461 - accuracy: 0.8806 - val_loss: 0.3558 - val_accuracy: 0.8507\n",
      "Epoch 139/210\n",
      "67/67 - 0s - loss: 0.3565 - accuracy: 0.8582 - val_loss: 0.3564 - val_accuracy: 0.8507\n",
      "Epoch 140/210\n",
      "67/67 - 0s - loss: 0.4333 - accuracy: 0.8433 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 141/210\n",
      "67/67 - 0s - loss: 0.3952 - accuracy: 0.8507 - val_loss: 0.3544 - val_accuracy: 0.8507\n",
      "Epoch 142/210\n",
      "67/67 - 0s - loss: 0.3896 - accuracy: 0.8582 - val_loss: 0.3558 - val_accuracy: 0.8507\n",
      "Epoch 143/210\n",
      "67/67 - 0s - loss: 0.3537 - accuracy: 0.8582 - val_loss: 0.3547 - val_accuracy: 0.8433\n",
      "Epoch 144/210\n",
      "67/67 - 0s - loss: 0.4372 - accuracy: 0.8284 - val_loss: 0.3543 - val_accuracy: 0.8433\n",
      "Epoch 145/210\n",
      "67/67 - 0s - loss: 0.3861 - accuracy: 0.8433 - val_loss: 0.3527 - val_accuracy: 0.8433\n",
      "Epoch 146/210\n",
      "67/67 - 0s - loss: 0.4051 - accuracy: 0.8433 - val_loss: 0.3510 - val_accuracy: 0.8507\n",
      "Epoch 147/210\n",
      "67/67 - 0s - loss: 0.3876 - accuracy: 0.8731 - val_loss: 0.3511 - val_accuracy: 0.8358\n",
      "Epoch 148/210\n",
      "67/67 - 0s - loss: 0.4506 - accuracy: 0.8433 - val_loss: 0.3500 - val_accuracy: 0.8433\n",
      "Epoch 149/210\n",
      "67/67 - 0s - loss: 0.4489 - accuracy: 0.8358 - val_loss: 0.3499 - val_accuracy: 0.8507\n",
      "Epoch 150/210\n",
      "67/67 - 0s - loss: 0.4183 - accuracy: 0.8358 - val_loss: 0.3507 - val_accuracy: 0.8433\n",
      "Epoch 151/210\n",
      "67/67 - 0s - loss: 0.3518 - accuracy: 0.8507 - val_loss: 0.3553 - val_accuracy: 0.8507\n",
      "Epoch 152/210\n",
      "67/67 - 0s - loss: 0.4264 - accuracy: 0.8433 - val_loss: 0.3559 - val_accuracy: 0.8507\n",
      "Epoch 153/210\n",
      "67/67 - 0s - loss: 0.3865 - accuracy: 0.8358 - val_loss: 0.3546 - val_accuracy: 0.8507\n",
      "Epoch 154/210\n",
      "67/67 - 0s - loss: 0.3904 - accuracy: 0.8433 - val_loss: 0.3556 - val_accuracy: 0.8507\n",
      "Epoch 155/210\n",
      "67/67 - 0s - loss: 0.3938 - accuracy: 0.8507 - val_loss: 0.3534 - val_accuracy: 0.8433\n",
      "Epoch 156/210\n",
      "67/67 - 0s - loss: 0.3875 - accuracy: 0.8657 - val_loss: 0.3506 - val_accuracy: 0.8358\n",
      "Epoch 157/210\n",
      "67/67 - 0s - loss: 0.3701 - accuracy: 0.8582 - val_loss: 0.3504 - val_accuracy: 0.8358\n",
      "Epoch 158/210\n",
      "67/67 - 0s - loss: 0.4598 - accuracy: 0.8060 - val_loss: 0.3483 - val_accuracy: 0.8358\n",
      "Epoch 159/210\n",
      "67/67 - 0s - loss: 0.3694 - accuracy: 0.8582 - val_loss: 0.3477 - val_accuracy: 0.8507\n",
      "Epoch 160/210\n",
      "67/67 - 0s - loss: 0.3594 - accuracy: 0.8507 - val_loss: 0.3484 - val_accuracy: 0.8507\n",
      "Epoch 161/210\n",
      "67/67 - 0s - loss: 0.3925 - accuracy: 0.8433 - val_loss: 0.3486 - val_accuracy: 0.8507\n",
      "Epoch 162/210\n",
      "67/67 - 0s - loss: 0.4264 - accuracy: 0.8582 - val_loss: 0.3491 - val_accuracy: 0.8507\n",
      "Epoch 163/210\n",
      "67/67 - 0s - loss: 0.4537 - accuracy: 0.8358 - val_loss: 0.3493 - val_accuracy: 0.8433\n",
      "Epoch 164/210\n",
      "67/67 - 0s - loss: 0.4784 - accuracy: 0.8358 - val_loss: 0.3498 - val_accuracy: 0.8433\n",
      "Epoch 165/210\n",
      "67/67 - 0s - loss: 0.4022 - accuracy: 0.8433 - val_loss: 0.3493 - val_accuracy: 0.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/210\n",
      "67/67 - 0s - loss: 0.3971 - accuracy: 0.8806 - val_loss: 0.3504 - val_accuracy: 0.8507\n",
      "Epoch 167/210\n",
      "67/67 - 0s - loss: 0.4817 - accuracy: 0.8060 - val_loss: 0.3485 - val_accuracy: 0.8582\n",
      "Epoch 168/210\n",
      "67/67 - 0s - loss: 0.3984 - accuracy: 0.8358 - val_loss: 0.3485 - val_accuracy: 0.8582\n",
      "Epoch 169/210\n",
      "67/67 - 0s - loss: 0.3367 - accuracy: 0.8582 - val_loss: 0.3507 - val_accuracy: 0.8507\n",
      "Epoch 170/210\n",
      "67/67 - 0s - loss: 0.3617 - accuracy: 0.8582 - val_loss: 0.3509 - val_accuracy: 0.8507\n",
      "Epoch 171/210\n",
      "67/67 - 0s - loss: 0.4461 - accuracy: 0.8209 - val_loss: 0.3496 - val_accuracy: 0.8507\n",
      "Epoch 172/210\n",
      "67/67 - 0s - loss: 0.4307 - accuracy: 0.7985 - val_loss: 0.3460 - val_accuracy: 0.8507\n",
      "Epoch 173/210\n",
      "67/67 - 0s - loss: 0.4168 - accuracy: 0.8358 - val_loss: 0.3459 - val_accuracy: 0.8507\n",
      "Epoch 174/210\n",
      "67/67 - 0s - loss: 0.3299 - accuracy: 0.8731 - val_loss: 0.3475 - val_accuracy: 0.8433\n",
      "Epoch 175/210\n",
      "67/67 - 0s - loss: 0.4272 - accuracy: 0.8433 - val_loss: 0.3464 - val_accuracy: 0.8433\n",
      "Epoch 176/210\n",
      "67/67 - 0s - loss: 0.3724 - accuracy: 0.8507 - val_loss: 0.3480 - val_accuracy: 0.8582\n",
      "Epoch 177/210\n",
      "67/67 - 0s - loss: 0.4027 - accuracy: 0.8209 - val_loss: 0.3497 - val_accuracy: 0.8582\n",
      "Epoch 178/210\n",
      "67/67 - 0s - loss: 0.3772 - accuracy: 0.8134 - val_loss: 0.3499 - val_accuracy: 0.8582\n",
      "Epoch 179/210\n",
      "67/67 - 0s - loss: 0.3755 - accuracy: 0.8433 - val_loss: 0.3505 - val_accuracy: 0.8507\n",
      "Epoch 180/210\n",
      "67/67 - 0s - loss: 0.4154 - accuracy: 0.8433 - val_loss: 0.3501 - val_accuracy: 0.8507\n",
      "Epoch 181/210\n",
      "67/67 - 0s - loss: 0.4549 - accuracy: 0.8358 - val_loss: 0.3510 - val_accuracy: 0.8582\n",
      "Epoch 182/210\n",
      "67/67 - 0s - loss: 0.3259 - accuracy: 0.8284 - val_loss: 0.3549 - val_accuracy: 0.8507\n",
      "Epoch 183/210\n",
      "67/67 - 0s - loss: 0.4290 - accuracy: 0.8433 - val_loss: 0.3522 - val_accuracy: 0.8433\n",
      "Epoch 184/210\n",
      "67/67 - 0s - loss: 0.4008 - accuracy: 0.8209 - val_loss: 0.3523 - val_accuracy: 0.8433\n",
      "Epoch 185/210\n",
      "67/67 - 0s - loss: 0.4332 - accuracy: 0.8433 - val_loss: 0.3502 - val_accuracy: 0.8433\n",
      "Epoch 186/210\n",
      "67/67 - 0s - loss: 0.3779 - accuracy: 0.8358 - val_loss: 0.3516 - val_accuracy: 0.8358\n",
      "Epoch 187/210\n",
      "67/67 - 0s - loss: 0.3928 - accuracy: 0.8582 - val_loss: 0.3493 - val_accuracy: 0.8582\n",
      "Epoch 188/210\n",
      "67/67 - 0s - loss: 0.3880 - accuracy: 0.8433 - val_loss: 0.3505 - val_accuracy: 0.8582\n",
      "Epoch 189/210\n",
      "67/67 - 0s - loss: 0.3701 - accuracy: 0.8507 - val_loss: 0.3528 - val_accuracy: 0.8507\n",
      "Epoch 190/210\n",
      "67/67 - 0s - loss: 0.3801 - accuracy: 0.8657 - val_loss: 0.3502 - val_accuracy: 0.8507\n",
      "Epoch 191/210\n",
      "67/67 - 0s - loss: 0.3917 - accuracy: 0.8657 - val_loss: 0.3507 - val_accuracy: 0.8507\n",
      "Epoch 192/210\n",
      "67/67 - 0s - loss: 0.4094 - accuracy: 0.8657 - val_loss: 0.3487 - val_accuracy: 0.8507\n",
      "Epoch 193/210\n",
      "67/67 - 0s - loss: 0.3987 - accuracy: 0.8582 - val_loss: 0.3467 - val_accuracy: 0.8433\n",
      "Epoch 194/210\n",
      "67/67 - 0s - loss: 0.4401 - accuracy: 0.8507 - val_loss: 0.3468 - val_accuracy: 0.8433\n",
      "Epoch 195/210\n",
      "67/67 - 0s - loss: 0.3571 - accuracy: 0.8582 - val_loss: 0.3491 - val_accuracy: 0.8507\n",
      "Epoch 196/210\n",
      "67/67 - 0s - loss: 0.3460 - accuracy: 0.8582 - val_loss: 0.3495 - val_accuracy: 0.8507\n",
      "Epoch 197/210\n",
      "67/67 - 0s - loss: 0.3802 - accuracy: 0.8507 - val_loss: 0.3505 - val_accuracy: 0.8582\n",
      "Epoch 198/210\n",
      "67/67 - 0s - loss: 0.3844 - accuracy: 0.8358 - val_loss: 0.3499 - val_accuracy: 0.8582\n",
      "Epoch 199/210\n",
      "67/67 - 0s - loss: 0.3635 - accuracy: 0.8657 - val_loss: 0.3525 - val_accuracy: 0.8582\n",
      "Epoch 200/210\n",
      "67/67 - 0s - loss: 0.3938 - accuracy: 0.8358 - val_loss: 0.3529 - val_accuracy: 0.8507\n",
      "Epoch 201/210\n",
      "67/67 - 0s - loss: 0.4024 - accuracy: 0.8433 - val_loss: 0.3528 - val_accuracy: 0.8507\n",
      "Epoch 202/210\n",
      "67/67 - 0s - loss: 0.4419 - accuracy: 0.8358 - val_loss: 0.3519 - val_accuracy: 0.8582\n",
      "Epoch 203/210\n",
      "67/67 - 0s - loss: 0.4091 - accuracy: 0.8060 - val_loss: 0.3477 - val_accuracy: 0.8582\n",
      "Epoch 204/210\n",
      "67/67 - 0s - loss: 0.4210 - accuracy: 0.8657 - val_loss: 0.3459 - val_accuracy: 0.8582\n",
      "Epoch 205/210\n",
      "67/67 - 0s - loss: 0.3491 - accuracy: 0.8657 - val_loss: 0.3468 - val_accuracy: 0.8582\n",
      "Epoch 206/210\n",
      "67/67 - 0s - loss: 0.4380 - accuracy: 0.8284 - val_loss: 0.3451 - val_accuracy: 0.8582\n",
      "Epoch 207/210\n",
      "67/67 - 0s - loss: 0.4074 - accuracy: 0.8433 - val_loss: 0.3447 - val_accuracy: 0.8582\n",
      "Epoch 208/210\n",
      "67/67 - 0s - loss: 0.3699 - accuracy: 0.8657 - val_loss: 0.3446 - val_accuracy: 0.8582\n",
      "Epoch 209/210\n",
      "67/67 - 0s - loss: 0.3524 - accuracy: 0.8582 - val_loss: 0.3460 - val_accuracy: 0.8582\n",
      "Epoch 210/210\n",
      "67/67 - 0s - loss: 0.3199 - accuracy: 0.8657 - val_loss: 0.3491 - val_accuracy: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2aef51f10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_p.fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d0e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6220139966875858\n",
      "0.6479923129672116\n",
      "0.7607795429345929\n",
      "0.8266045548654244\n",
      "train accuracy score= 0.771\n",
      "test accuracy score= 0.851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, f1_score, accuracy_score\n",
    "y_pred_train2 = np.round(ann_p.predict(X_train),0)\n",
    "y_pred_test2 = np.round(ann_p.predict(X_test),0)\n",
    "precision_test = average_precision_score(y_test, y_pred_test2)\n",
    "precision_train = average_precision_score(y_train, y_pred_train2)\n",
    "print(precision_train)\n",
    "print(precision_test)\n",
    "f1_train = f1_score(y_train, y_pred_train2, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_test2, average='macro')\n",
    "print(f1_train)\n",
    "print(f1_test)\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_train2)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_test2)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "981dcffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7509293680297398\n",
      "0.7045454545454546\n",
      "train f1 score= 0.766\n",
      "test f1 score= 0.802\n",
      "train accuracy score= 0.781\n",
      "test accuracy score= 0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-9293d8fe7273>:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clfrf.fit(X_train,y_train)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:316: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 1.0 (renaming of 0.25). Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a svm Classifier#\n",
    "clfrf=RandomForestClassifier(n_estimators=12, criterion=\"entropy\", max_depth=None,\n",
    "    min_samples_split=11, min_samples_leaf=1, min_weight_fraction_leaf=0.01, max_features='auto', max_leaf_nodes=10,\n",
    "    min_impurity_decrease=0.035, min_impurity_split=0.1, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "    verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.04, max_samples=70, random_state = 262)\n",
    "\n",
    "# Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clfrf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred_rf_train = clfrf.predict(X_train)\n",
    "y_pred_rf_test = clfrf.predict(X_test)\n",
    "print(metrics.precision_score(y_train, y_pred_rf_train))\n",
    "print(metrics.precision_score(y_test, y_pred_rf_test))\n",
    "f1_train = f1_score(y_train, y_pred_rf_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_rf_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_rf_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_rf_test)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e35b2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894\n",
      "0.828\n",
      "train f1 score= 0.785\n",
      "test f1 score= 0.787\n",
      "train accuracy score= 0.81\n",
      "test accuracy score= 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf', degree =3, gamma = 0.34, C = 100, max_iter=-1, random_state = 262) \n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm_train = clf.predict(X_train)\n",
    "y_pred_svm_test = clf.predict(X_test)\n",
    "print(np.round(metrics.precision_score(y_train, y_pred_svm_train),3))\n",
    "print(np.round(metrics.precision_score(y_test, y_pred_svm_test),3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_svm_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_svm_test)\n",
    "f1_train = f1_score(y_train, y_pred_svm_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_svm_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1baabd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7481481481481481\n",
      "0.7045454545454546\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#Create a svm Classifier\n",
    "clf2 = svm.SVC(kernel='linear', degree = 3, gamma = 0.01, C = 1000, random_state = 262) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_svm2_train = clf2.predict(X_train)\n",
    "y_pred_svm2_test = clf2.predict(X_test)\n",
    "print(metrics.precision_score(y_train, y_pred_svm2_train))\n",
    "print(metrics.precision_score(y_test, y_pred_svm2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "61603581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:21:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"rate_drop\", \"skip_drop\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[00:21:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "train precision= 0.893\n",
      "test precision= 0.828\n",
      "train f1 score= 0.782\n",
      "test f1 score= 0.787\n",
      "train accuracy score= 0.807\n",
      "test accuracy score= 0.836\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "xgb = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='gbtree', eta=0.25, max_depth=5, min_child_weight=10,\n",
    "                    max_delta_step=0.7, subsample=0.7, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"exact\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=0, n_estimators=8, rate_drop=\"0.2\", skip_drop=\"0.2\",\n",
    "                    random_state = 262)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_train = xgb.predict(X_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "print(\"train precision=\", np.round(metrics.precision_score(y_train, y_pred_xgb_train),3))\n",
    "print(\"test precision=\", np.round(metrics.precision_score(y_test, y_pred_xgb_test),3))\n",
    "f1_train = f1_score(y_train, y_pred_xgb_train, average='macro')\n",
    "f1_test = f1_score(y_test, y_pred_xgb_test, average='macro')\n",
    "print(\"train f1 score=\", np.round(f1_train,3))\n",
    "print(\"test f1 score=\", np.round(f1_test,3))\n",
    "Accuracy_train = accuracy_score(y_train, y_pred_xgb_train)\n",
    "Accuracy_test = accuracy_score(y_test, y_pred_xgb_test)\n",
    "print(\"train accuracy score=\", np.round(Accuracy_train,3))\n",
    "print(\"test accuracy score=\", np.round(Accuracy_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "242d619f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11141818 0.08181724 0.06409427 0.7426703 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(xgb.feature_importances_)\n",
    "import matplotlib.pyplot as mp\n",
    "mp.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\n",
    "mp.show()\n",
    "\n",
    "# plot feature importance\n",
    "from xgboost import plot_importance\n",
    "plot_importance(xgb)\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "40f971e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                          Name     Sex  \\\n",
       "PassengerId                                                                 \n",
       "892               3                              Kelly, Mr. James    male   \n",
       "893               3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "894               2                     Myles, Mr. Thomas Francis    male   \n",
       "895               3                              Wirz, Mr. Albert    male   \n",
       "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "...             ...                                           ...     ...   \n",
       "1305              3                            Spector, Mr. Woolf    male   \n",
       "1306              1                  Oliva y Ocana, Dona. Fermina  female   \n",
       "1307              3                  Saether, Mr. Simon Sivertsen    male   \n",
       "1308              3                           Ware, Mr. Frederick    male   \n",
       "1309              3                      Peter, Master. Michael J    male   \n",
       "\n",
       "              Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \n",
       "PassengerId                                                                   \n",
       "892          34.5      0      0              330911    7.8292   NaN        Q  \n",
       "893          47.0      1      0              363272    7.0000   NaN        S  \n",
       "894          62.0      0      0              240276    9.6875   NaN        Q  \n",
       "895          27.0      0      0              315154    8.6625   NaN        S  \n",
       "896          22.0      1      1             3101298   12.2875   NaN        S  \n",
       "...           ...    ...    ...                 ...       ...   ...      ...  \n",
       "1305          NaN      0      0           A.5. 3236    8.0500   NaN        S  \n",
       "1306         39.0      0      0            PC 17758  108.9000  C105        C  \n",
       "1307         38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "1308          NaN      0      0              359309    8.0500   NaN        S  \n",
       "1309          NaN      1      1                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv(\"test.csv\", index_col=0)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "babe5243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                          Name  Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "892               3                              Kelly, Mr. James    1  34.5   \n",
       "893               3              Wilkes, Mrs. James (Ellen Needs)    0  47.0   \n",
       "894               2                     Myles, Mr. Thomas Francis    1  62.0   \n",
       "895               3                              Wirz, Mr. Albert    1  27.0   \n",
       "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)    0  22.0   \n",
       "...             ...                                           ...  ...   ...   \n",
       "1305              3                            Spector, Mr. Woolf    1   NaN   \n",
       "1306              1                  Oliva y Ocana, Dona. Fermina    0  39.0   \n",
       "1307              3                  Saether, Mr. Simon Sivertsen    1  38.5   \n",
       "1308              3                           Ware, Mr. Frederick    1   NaN   \n",
       "1309              3                      Peter, Master. Michael J    1   NaN   \n",
       "\n",
       "             SibSp  Parch              Ticket      Fare Cabin  Embarked  \n",
       "PassengerId                                                              \n",
       "892              0      0              330911    7.8292   NaN         1  \n",
       "893              1      0              363272    7.0000   NaN         2  \n",
       "894              0      0              240276    9.6875   NaN         1  \n",
       "895              0      0              315154    8.6625   NaN         2  \n",
       "896              1      1             3101298   12.2875   NaN         2  \n",
       "...            ...    ...                 ...       ...   ...       ...  \n",
       "1305             0      0           A.5. 3236    8.0500   NaN         2  \n",
       "1306             0      0            PC 17758  108.9000  C105         0  \n",
       "1307             0      0  SOTON/O.Q. 3101262    7.2500   NaN         2  \n",
       "1308             0      0              359309    8.0500   NaN         2  \n",
       "1309             1      1                2668   22.3583   NaN         0  \n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "dft['Embarked'] = le.fit_transform(dft['Embarked'])\n",
    "dft['Sex'] = le.fit_transform(dft['Sex'])\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8fe056c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.2250</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82.2667</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>61.1750</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Name  Sex   Age  SibSp  Parch  Ticket     Fare  Cabin  \\\n",
       "PassengerId                                                                  \n",
       "892               3     1    1  34.5      0      0       1   7.8292      1   \n",
       "893               3     2    0  47.0      1      0       2   7.0000      2   \n",
       "894               2     1    1  62.0      0      0       1   9.6875      1   \n",
       "895               3     2    1  27.0      0      0       2   8.6625      2   \n",
       "896               3     2    0  22.0      1      1       2  12.2875      2   \n",
       "897               3     2    1  14.0      0      0       2   9.2250      2   \n",
       "898               3     1    0  30.0      0      0       1   7.6292      1   \n",
       "899               2     2    1  26.0      1      1       2  29.0000      2   \n",
       "900               3     0    0  18.0      0      0       0   7.2292      0   \n",
       "901               3     2    1  21.0      2      0       2  24.1500      2   \n",
       "902               3     2    1   NaN      0      0       2   7.8958      2   \n",
       "903               1     2    1  46.0      0      0       2  26.0000      2   \n",
       "904               1     2    0  23.0      1      0       2  82.2667      2   \n",
       "905               2     2    1  63.0      1      0       2  26.0000      2   \n",
       "906               1     2    0  47.0      1      0       2  61.1750      2   \n",
       "\n",
       "             Embarked  \n",
       "PassengerId            \n",
       "892                 1  \n",
       "893                 2  \n",
       "894                 1  \n",
       "895                 2  \n",
       "896                 2  \n",
       "897                 2  \n",
       "898                 1  \n",
       "899                 2  \n",
       "900                 0  \n",
       "901                 2  \n",
       "902                 2  \n",
       "903                 2  \n",
       "904                 2  \n",
       "905                 2  \n",
       "906                 2  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = dft.apply(lambda col: LabelEncoder().fit_transform(dft[\"Embarked\"]) if col.dtype == \"object\" else col)\n",
    "dft.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0b84651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 87\n",
      "Missing: 0\n",
      "Mean Accuracy: 1.000 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# iterative imputation transform for the horse colic dataset\n",
    "from numpy import isnan, mean, std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dft1 = dft.drop(columns={\"Name\",\"Cabin\",\"Ticket\"})\n",
    "# split into input and output elements\n",
    "data = dft1.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 6]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = IterativeImputer()\n",
    "#define modeling pipeline\n",
    "model = XGBClassifier(use_label_encoder=False, base_score=0.25, booster='gbtree', eta=0.3, max_depth=4, min_child_weight=20,\n",
    "                    max_delta_step=0.5, subsample=0.6, colsample_bytree=1, colsample_bylevel=0.7, colsample_bynode=1, \n",
    "                    reg_lambda=1, reg_alpha=1, tree_method=\"approx\", sketch_eps=0.1, scale_pos_weight=1.6, \n",
    "                    objective=\"binary:logitraw\", gamma=0, n_estimators=10, rate_drop=\"0.01\", skip_drop=\"0.8\",\n",
    "                    random_state = 262)# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "dft1 = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(dft1).flatten()))\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "793ecd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 87\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KNNImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e744b0b35cb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# define imputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# fit on the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mimputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KNNImputer' is not defined"
     ]
    }
   ],
   "source": [
    "dft1 = dft.drop(columns={\"Name\",\"Cabin\",\"Ticket\"})\n",
    "# split into input and output elements\n",
    "data = dft1.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 9]\n",
    "X, y = data[:, ix], data[:, 6]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "dft1 = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(dft1).flatten()))\n",
    "dft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f91ce20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.224941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.396993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1          2    3    4         5    6\n",
       "0    3.0  1.0  34.500000  0.0  0.0    7.8292  1.0\n",
       "1    3.0  0.0  47.000000  1.0  0.0    7.0000  2.0\n",
       "2    2.0  1.0  62.000000  0.0  0.0    9.6875  1.0\n",
       "3    3.0  1.0  27.000000  0.0  0.0    8.6625  2.0\n",
       "4    3.0  0.0  22.000000  1.0  1.0   12.2875  2.0\n",
       "..   ...  ...        ...  ...  ...       ...  ...\n",
       "413  3.0  1.0  25.224941  0.0  0.0    8.0500  2.0\n",
       "414  1.0  0.0  39.000000  0.0  0.0  108.9000  0.0\n",
       "415  3.0  1.0  38.500000  0.0  0.0    7.2500  2.0\n",
       "416  3.0  1.0  25.224941  0.0  0.0    8.0500  2.0\n",
       "417  3.0  1.0  23.396993  1.0  1.0   22.3583  0.0\n",
       "\n",
       "[418 rows x 7 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft1 = pd.DataFrame(dft1)\n",
    "dft1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "75933b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "Sex         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1.0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3.0</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass      Fare  Embarked  Sex\n",
       "0       3.0    7.8292       1.0  1.0\n",
       "1       3.0    7.0000       2.0  0.0\n",
       "2       2.0    9.6875       1.0  1.0\n",
       "3       3.0    8.6625       2.0  1.0\n",
       "4       3.0   12.2875       2.0  0.0\n",
       "..      ...       ...       ...  ...\n",
       "413     3.0    8.0500       2.0  1.0\n",
       "414     1.0  108.9000       0.0  0.0\n",
       "415     3.0    7.2500       2.0  1.0\n",
       "416     3.0    8.0500       2.0  1.0\n",
       "417     3.0   22.3583       0.0  1.0\n",
       "\n",
       "[418 rows x 4 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft1.columns = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]\n",
    "dftnew = dft1[[\"Pclass\",\"Fare\",\"Embarked\",\"Sex\"]]\n",
    "print(dftnew.isnull().sum())\n",
    "dftnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ab82d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01528158, 0.5       , 1.        ],\n",
       "       [1.        , 0.01366309, 1.        , 0.        ],\n",
       "       [0.5       , 0.01890874, 0.5       , 1.        ],\n",
       "       ...,\n",
       "       [1.        , 0.01415106, 1.        , 1.        ],\n",
       "       [1.        , 0.01571255, 1.        , 1.        ],\n",
       "       [1.        , 0.0436405 , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftnew1 = mmsc.fit_transform(dftnew)\n",
    "dftnew1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6b51e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = pd.DataFrame(np.round(xgb.predict(dftnew1),0))\n",
    "y_pred_xgb.to_csv(\"xgb_xgbimpute3_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6b8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
